{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7901931,"sourceType":"datasetVersion","datasetId":4640874},{"sourceId":7907242,"sourceType":"datasetVersion","datasetId":4644838},{"sourceId":7907320,"sourceType":"datasetVersion","datasetId":4644892}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-13T05:01:32.530824Z","iopub.execute_input":"2024-04-13T05:01:32.531192Z","iopub.status.idle":"2024-04-13T05:01:33.526857Z","shell.execute_reply.started":"2024-04-13T05:01:32.531164Z","shell.execute_reply":"2024-04-13T05:01:33.525844Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/phishingurl/Data_processed.csv\n/kaggle/input/image-2/Screenshot 2024-03-21 at 9.59.01PM.png\n/kaggle/input/image-1/Screenshot 2024-03-21 at 9.51.40PM.png\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **CREATING DATASET**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/phishingurl/Data_processed.csv')\ndf['target'] = np.where(df['status'] == 'phishing', 1, 0)\ndf.drop(columns=['url','status','lexical_features'], inplace=True)\ndf.drop(columns=['submit_email','sfh',], inplace=True)\ny=df['target']\nX = df.drop(columns=['target'])\nX = X[0:5000]\ny = y[0:5000]","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:01:33.529101Z","iopub.execute_input":"2024-04-13T05:01:33.529635Z","iopub.status.idle":"2024-04-13T05:01:33.670147Z","shell.execute_reply.started":"2024-04-13T05:01:33.529598Z","shell.execute_reply":"2024-04-13T05:01:33.669088Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:01:33.671628Z","iopub.execute_input":"2024-04-13T05:01:33.671956Z","iopub.status.idle":"2024-04-13T05:01:33.698095Z","shell.execute_reply.started":"2024-04-13T05:01:33.671928Z","shell.execute_reply":"2024-04-13T05:01:33.697067Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   login_form  links_in_tags  iframe  popup_window  safe_anchor  onmouseover  \\\n0           0      80.000000       0             0          0.0            0   \n1           0     100.000000       0             0        100.0            0   \n2           0     100.000000       0             0        100.0            0   \n3           0     100.000000       0             0         62.5            0   \n4           1      76.470588       0             0          0.0            0   \n\n   right_clic  whois_registered_domain  domain_registration_length  \\\n0           0                        0                          45   \n1           0                        0                          77   \n2           0                        0                          14   \n3           0                        0                          62   \n4           0                        0                         224   \n\n   domain_age  ...  embedded_domain  having_ip_address  no_of_dots  \\\n0          -1  ...               -1                  1           3   \n1        5767  ...                1                  1           1   \n2        4004  ...                1                  1           4   \n3          -1  ...               -1                  1           2   \n4        8175  ...               -1                  1           2   \n\n   no_of_sensitive_words  out_of_position_tld  https_token  url_length  \\\n0                      0                    1           -1          37   \n1                      0                    1           -1          77   \n2                      1                   -1            1         126   \n3                      0                    1           -1          18   \n4                      0                    1           -1          55   \n\n   tinyURL  prefixSuffix  target  \n0       -1            -1       0  \n1       -1            -1       1  \n2       -1             1       1  \n3       -1            -1       0  \n4       -1            -1       0  \n\n[5 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>login_form</th>\n      <th>links_in_tags</th>\n      <th>iframe</th>\n      <th>popup_window</th>\n      <th>safe_anchor</th>\n      <th>onmouseover</th>\n      <th>right_clic</th>\n      <th>whois_registered_domain</th>\n      <th>domain_registration_length</th>\n      <th>domain_age</th>\n      <th>...</th>\n      <th>embedded_domain</th>\n      <th>having_ip_address</th>\n      <th>no_of_dots</th>\n      <th>no_of_sensitive_words</th>\n      <th>out_of_position_tld</th>\n      <th>https_token</th>\n      <th>url_length</th>\n      <th>tinyURL</th>\n      <th>prefixSuffix</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>80.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>45</td>\n      <td>-1</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>37</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>100.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>100.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>77</td>\n      <td>5767</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>77</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>100.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>100.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14</td>\n      <td>4004</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>126</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>100.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>62.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>62</td>\n      <td>-1</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>18</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>76.470588</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>224</td>\n      <td>8175</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>55</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 24 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"discrete = [True, False, True, True, False, True, True, True, False, False, False, True, True, False, True, True, False, False, True, True, False, True, True, False]","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:01:33.700020Z","iopub.execute_input":"2024-04-13T05:01:33.700299Z","iopub.status.idle":"2024-04-13T05:01:33.704988Z","shell.execute_reply.started":"2024-04-13T05:01:33.700272Z","shell.execute_reply":"2024-04-13T05:01:33.704092Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:01:33.706588Z","iopub.execute_input":"2024-04-13T05:01:33.707202Z","iopub.status.idle":"2024-04-13T05:01:34.881729Z","shell.execute_reply.started":"2024-04-13T05:01:33.707163Z","shell.execute_reply":"2024-04-13T05:01:34.880783Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_train.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:01:34.884466Z","iopub.execute_input":"2024-04-13T05:01:34.884865Z","iopub.status.idle":"2024-04-13T05:01:34.891403Z","shell.execute_reply.started":"2024-04-13T05:01:34.884828Z","shell.execute_reply":"2024-04-13T05:01:34.890729Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Index(['login_form', 'links_in_tags', 'iframe', 'popup_window', 'safe_anchor',\n       'onmouseover', 'right_clic', 'whois_registered_domain',\n       'domain_registration_length', 'domain_age', 'web_traffic', 'dns_record',\n       'google_index', 'page_rank', 'embedded_domain', 'having_ip_address',\n       'no_of_dots', 'no_of_sensitive_words', 'out_of_position_tld',\n       'https_token', 'url_length', 'tinyURL', 'prefixSuffix'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"# ID3 ALGORITHM","metadata":{}},{"cell_type":"markdown","source":"#### **The above approach is the first algorithm developed for the decision trees.**\n* The major disadvantage is that it works only on categorical data\n* ID3 uses information gain as splitting criteria.\n* The growing stops when all instances belong to a single value of target feature or when    best information gain is not greater than zero\n* ID3 does not apply any pruning procedures nor does it handle numeric attributes or missing values.\n","metadata":{}},{"cell_type":"code","source":"class Node:\n    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False):\n\n        self.data = data\n        self.children = children\n        self.split_on = split_on\n        self.pred_class = pred_class\n        self.is_leaf = is_leaf","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:38:59.978281Z","iopub.execute_input":"2024-04-07T04:38:59.978732Z","iopub.status.idle":"2024-04-07T04:38:59.985997Z","shell.execute_reply.started":"2024-04-07T04:38:59.978693Z","shell.execute_reply":"2024-04-07T04:38:59.984671Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<h2>Information Gain and Entropy</h2>\n\n<p>Information gain is an impurity-based criterion used in decision trees to measure the reduction in uncertainty or impurity of a dataset after a particular feature is chosen for splitting.</p>\n\n<p>The formula for Information Gain ($\\text{IG}$) is:</p>\n\n$$\n\\text{IG}(a, S) = \\text{Entropy}(y, S) - \\sum_{\\bar{\\sigma}_{a=v_{i,j}}(S)} \\frac{|\\bar{\\sigma}_{a=v_{i,j}}(S)| \\cdot \\text{Entropy}(y, \\bar{\\sigma}_{a=v_{i,j}}(S))}{|S|}\n$$\n\n<p>Where:</p>\n\n<ul>\n  <li>$\\text{IG}(a, S)$: Information gain for attribute $a$ and subset $S$.</li>\n  <li>$\\text{Entropy}(y, S)$: Entropy of target attribute $y$ in subset $S$.</li>\n  <li>$v_{i,j}$: Values of attribute $a$.</li>\n</ul>\n\n<p>Entropy ($\\text{Entropy}$) is a measure of impurity or uncertainty in a dataset. The formula for Entropy is:</p>\n\n$$\n\\text{Entropy}(y, S) = - \\sum_{c_j \\in \\text{dom}(y)} \\frac{|\\bar{\\sigma}_{y=c_j}(S)|}{|S|} \\cdot \\log_2 \\frac{|\\bar{\\sigma}_{y=c_j}(S)|}{|S|}\n$$\n\n<p>Where:</p>\n\n<ul>\n  <li>$\\text{dom}(y)$: Domain of target attribute $y$.</li>\n  <li>$\\bar{\\sigma}_{y=c_j}(S)$: Subset of $S$ where target attribute $y$ takes the value $c_j$.</li>\n</ul>\n","metadata":{}},{"cell_type":"code","source":"class DecisionTreeClassifier:\n    def __init__(self):\n        self.root = Node()\n    @staticmethod\n    def calculate_entropy(Y):\n        _, labels_counts = np.unique(Y, return_counts=True)\n        total_instances = len(Y)\n        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n        return entropy\n    def split_on_feature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n\n        split_nodes = {}\n        weighted_entropy = 0\n        total_instances = len(data)\n\n        for unique_value in unique_values:\n            partition = data[data[:, feat_index] == unique_value, :]\n            node = Node(data=partition)\n            split_nodes[unique_value] = node\n            partition_y = self.get_y(partition)\n            node_entropy = self.calculate_entropy(partition_y)\n            weighted_entropy += (len(partition) / total_instances) * node_entropy\n\n        return split_nodes, weighted_entropy\n    def best_split(self, node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n\n        index_feature_split = -1\n        min_entropy = 1\n\n        for i in range(node.data.shape[1] - 1):\n            split_nodes, weighted_entropy = self.split_on_feature(node.data, i)\n            if weighted_entropy < min_entropy:\n                child_nodes, min_entropy = split_nodes, weighted_entropy\n                index_feature_split = i\n\n        node.children = child_nodes\n        node.split_on = index_feature_split\n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        y = self.get_y(node.data)\n        return True if self.calculate_entropy(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.array([self.traverse_tree(x, self.root) for index, x in X.iterrows()])\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        predicted_class = self.traverse_tree(x, node.children[feat_value])\n        return predicted_class","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:39:01.608868Z","iopub.execute_input":"2024-04-07T04:39:01.609447Z","iopub.status.idle":"2024-04-07T04:39:01.631674Z","shell.execute_reply.started":"2024-04-07T04:39:01.609403Z","shell.execute_reply":"2024-04-07T04:39:01.630329Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:06:06.555171Z","iopub.execute_input":"2024-03-21T17:06:06.555634Z","iopub.status.idle":"2024-03-21T17:06:08.016770Z","shell.execute_reply.started":"2024-03-21T17:06:06.555600Z","shell.execute_reply":"2024-03-21T17:06:08.015072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some **advantages** in using ID3 including easy decomposability, strong intuitive nature etc. And some\n**disadvantages** include the following;\n<br>\nâ€¢ Using information gain for feature selection, the algorithm tends to select attributes with more values, which is\ndue to the fact that the value of the information gain of this kind of attribute will be bigger than others.\n<br>\nâ€¢ In the decision tree building process, it is difficult to control the tree size. However, most researchers have tried\nto improve on this using various pruning methods to avoid the occurrence of over-fitting, which has led to the\ndecision tree building process to be completed in two steps, that is modeling and pruning. Meanwhile, it will save\na lot of time if a concise decision tree is built in onestep.\n<br>\nâ€¢ There are several logarithmic calculations in the attribute selection process, this has made the computation of\ninformation gain time consuming.","metadata":{}},{"cell_type":"markdown","source":"#### *This procedure will not work on our dataset. As some of the features in my datasets are continuos values. (not categorical)*","metadata":{}},{"cell_type":"markdown","source":"# C4.5 Decision Trees","metadata":{}},{"cell_type":"markdown","source":"#### **C4.5 was the next algorithm discovered in this feild**\n* It uses gain ratio as splitting criteria\n* The splitting ceases when the number of instances to be split is below a certain threshold\n* *C4.5 can handle numeric attributes.*","metadata":{}},{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kagglesdsdata/datasets/4644892/7907320/Screenshot%202024-03-21%20at%209.59.01PM.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240321%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240321T163522Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=94e0a88cde06144312f1496f726e8dbe75803b314d79d2cd7e0cbcb76d96ba377e88882dca68b6636a970dfceee0680e80ad0f6f66d38afcbca51d1b2cd7ca34947d21256d6acf9246bc90cefac628205f5105be1219e4a1f54568526d30012f6867719663b5ac3ad6a725179d2797c7e2aea056d3a62e173705262e98e571f97fba87fe3c7e0c92f3fea9025268c4fa66122ec83be45725b42b5219e4c028f732e6fe33ce8ecaaebd1a5c293982c92ae34670d68129a98963286f88bb3ae6df87da095955566299e693ff8bb5457397bf8ff57c71c66c3a7ece72c9816757e00377f790baf65590c315c6f56afbb450735e810b45199ae1631e8cbcfaac9e7e)","metadata":{}},{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kagglesdsdata/datasets/4644838/7907242/Screenshot%202024-03-21%20at%209.51.40PM.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240321%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240321T164759Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=738ad3a70255123689ea37e6f8a326aae6960a0eddcddf490dd086ef39b59a7b4d3563cbf35fd152fadadf22d48d29f37d4ae35079c78d2f91217a8a73a07961e4278a64c33cfba8a4e2c70a5568c254f975dceba97bdd2452960b7dd6d28b3f40f7e60bcd57b3dc85e76c99a1a9a24f3e3490d15984062e7a42dd73c786896031a3ded13265c65cac6a66e4b930898e8268243d38465239bdfac9c9654d716207483e50c078e5af64d14a3e1f9df0a395f935d57333211a588f1a31e419f5bb9f72d7994897bd056e93ddf09f329cf03e61cd4020cb62655d06ad555e426d59ac75ec21625cab448d57484882c89d0749727ad068277eee60702f70c8e56950)","metadata":{}},{"cell_type":"markdown","source":"### I have tried three variants of C4.5 Decision Tree","metadata":{}},{"cell_type":"code","source":"class Node:\n    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False , threshold=None):\n\n        self.data = data\n        self.children = children\n        self.split_on = split_on\n        self.threshold = threshold #Used when splitting using discrete features\n        self.pred_class = pred_class\n        self.is_leaf = is_leaf","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:40:41.744284Z","iopub.execute_input":"2024-03-21T17:40:41.744725Z","iopub.status.idle":"2024-03-21T17:40:41.751264Z","shell.execute_reply.started":"2024-03-21T17:40:41.744694Z","shell.execute_reply":"2024-03-21T17:40:41.749760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In the below algorithm** I have used Information Gain Ratio, for finding the best split threshold and on which feature to split. To find the best split in continuos feature it first sort the dataset and finds cut with maximum Information Gain and finally among all features, feature with maximum Information Gain ratio is selected.","metadata":{}},{"cell_type":"markdown","source":"<h2>Information Gain Ratio</h2>\n\n<p>Information gain ratio is a normalized version of information gain, which takes into account the intrinsic information of a split. It helps in reducing the bias towards attributes with a large number of distinct values.</p>\n\n<p>The formula for Information Gain Ratio ($\\text{IGR}$) is:</p>\n\n$$\n\\text{IGR}(a, S) = \\frac{\\text{InformationGain}(a, S)}{\\text{Entropy}(a, S)}\n$$\n\n<p>Where:</p>\n\n<ul>\n  <li>$\\text{IGR}(a, S)$: Information gain ratio for attribute $a$ and subset $S$.</li>\n  <li>$\\text{InformationGain}(a, S)$: Information gain for attribute $a$ and subset $S$.</li>\n  <li>$\\text{Entropy}(a, S)$: Entropy of attribute $a$ in subset $S$.</li>\n</ul>\n\n<p>Note that this ratio is not defined when the denominator is zero. Also, the ratio may tend to favor attributes for which the denominator is very small. Consequently, it is suggested to first calculate the information gain for all attributes and then consider only attributes that have performed at least as well as the average information gain when selecting the best attribute based on the gain ratio.</p>\n\n<p>It has been shown that the gain ratio tends to outperform simple information gain criteria, both in terms of accuracy and classifier complexity.</p>\n","metadata":{}},{"cell_type":"code","source":"class DecisionTreeClassifierWithIGR:\n    def __init__(self , discrete):\n        self.root = Node()\n        self.discrete = discrete\n    @staticmethod\n    def calculate_entropy(Y):\n        _, labels_counts = np.unique(Y, return_counts=True)\n        total_instances = len(Y)\n        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n        return entropy\n    def split_on_feature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n\n        split_nodes = {}\n        weighted_entropy = 0\n        total_instances = len(data)\n        split_info = 0\n\n        for unique_value in unique_values:\n            partition = data[data[:, feat_index] == unique_value, :]\n            split_info = split_info + (len(partition)/total_instances)*np.log2(len(partition)/total_instances)\n            node = Node(data=partition)\n            split_nodes[unique_value] = node\n            partition_y = self.get_y(partition)\n            node_entropy = self.calculate_entropy(partition_y)\n            weighted_entropy += (len(partition) / total_instances) * node_entropy\n\n        return split_nodes, weighted_entropy, -1*split_info\n    \n    def very_fast_split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        mean = np.mean(feature_values)\n        median = np.median(feature_values)\n        total_entropy = self.calculate_entropy(data[:, -1])\n        total_instances = len(data)\n        weighted_entropy = 0\n        split_nodes = {}\n        \n        left_data1 = data[data[:, feat_index] <= mean]\n        right_data1 = data[data[:, feat_index] > mean]\n        left_node1 = Node(data = left_data1)\n        right_node1 = Node(data = right_data1)\n        left_entropy1 = self.calculate_entropy(left_data1[:, -1])\n        right_entropy1 = self.calculate_entropy(right_data1[:, -1])\n        info_gain1 = total_entropy - (len(left_data1) / total_instances * left_entropy1) - (len(right_data1) / total_instances * right_entropy1)\n        \n        left_data2 = data[data[:, feat_index] <= median]\n        right_data2 = data[data[:, feat_index] > median]\n        left_node2 = Node(data = left_data2)\n        right_node2 = Node(data = right_data2)\n        left_entropy2 = self.calculate_entropy(left_data2[:, -1])\n        right_entropy2 = self.calculate_entropy(right_data2[:, -1])\n        info_gain2 = total_entropy - (len(left_data2) / total_instances * left_entropy2) - (len(right_data2) / total_instances * right_entropy2)\n    \n        if info_gain1 > info_gain2:\n            weighted_entropy = (len(left_data1) / total_instances * left_entropy1) + (len(right_data1) / total_instances * right_entropy1)\n            split_info = (len(left_data1) / total_instances)*np.log2((len(left_data1)+0.001) / total_instances) + (len(right_data1) / total_instances)*np.log2((len(right_data1)+0.001) / total_instances)\n            best_split_point = mean\n            split_nodes[0] = left_node1\n            split_nodes[1] = right_node1\n        else:\n            weighted_entropy = (len(left_data2) / total_instances * left_entropy2) + (len(right_data2) / total_instances * right_entropy2)\n            split_info = (len(left_data2) / total_instances)*np.log2((len(left_data2)+0.001) / total_instances) + (len(right_data2) / total_instances)*np.log2((len(right_data2)+0.001) / total_instances)\n            best_split_point = median\n            split_nodes[0] = left_node2\n            split_nodes[1] = right_node2\n        \n        return best_split_point, split_nodes, weighted_entropy, -1*split_info\n        \n    def split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        feature_values = np.sort(feature_values)\n        total_instances = len(data)\n        best_split_point = None\n        max_info_gain = -float('inf')\n        split_nodes = {}\n        y = self.get_y(data)\n        weighted_entropy = 0\n        split_info = 0\n\n        for i in range(1, len(feature_values)):\n            # Compute split point\n            split_point = (feature_values[i] + feature_values[i-1])/2\n\n            # Split data based on the split point\n            \n            left_data = data[data[:, feat_index] <= split_point]\n            right_data = data[data[:, feat_index] > split_point]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n\n            # Calculate information gain\n            left_entropy = self.calculate_entropy(left_data[:, -1])\n            right_entropy = self.calculate_entropy(right_data[:, -1])\n            total_entropy = self.calculate_entropy(data[:, -1])\n            \n            info_gain = total_entropy - (len(left_data) / total_instances * left_entropy) - (len(right_data) / total_instances * right_entropy)\n            \n\n            # Update best split point if information gain is higher\n            if info_gain > max_info_gain:\n                max_info_gain = info_gain\n                weighted_entropy = (len(left_data) / total_instances * left_entropy) + (len(right_data) / total_instances * right_entropy)\n                split_info = (len(left_data) / total_instances)*np.log2((len(left_data)+0.001) / total_instances) + (len(right_data) / total_instances)*np.log2((len(right_data)+0.001) / total_instances)\n                best_split_point = split_point\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        if len(split_nodes[0].data)==0 or len(split_nodes[1].data)==0: \n            weighted_entropy = self.calculate_entropy(y)\n        return best_split_point, split_nodes, weighted_entropy, -1*split_info\n\n        \n    def best_split(self, node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n        y = self.get_y(node.data)\n        if len(np.unique(y)) == 1:\n            return\n        tot_entropy = self.calculate_entropy(y)\n        index_feature_split = -1\n        min_entropy = 1\n        threshold = -1\n        max_info_gain_ratio = -float('inf')\n        avg_info_gain = 0\n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n                avg_info_gain += tot_entropy - weighted_entropy\n            else:\n                best_split_point, split_nodes, weighted_entropy, split_info = self.very_fast_split_on_contfeature(node.data, i)\n                avg_info_gain += tot_entropy - weighted_entropy\n                                 \n        avg_info_gain = avg_info_gain/(node.data.shape[1] - 1)   \n\n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n                if tot_entropy - weighted_entropy >= avg_info_gain:\n                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n                    #if weighted_entropy < min_entropy:\n                    if info_gain_ratio > max_info_gain_ratio:\n                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n                        index_feature_split = i\n            else:\n                best_split_point, split_nodes, weighted_entropy, split_info = self.split_on_contfeature(node.data, i)\n                if tot_entropy - weighted_entropy >= avg_info_gain:\n                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n                    #if weighted_entropy < min_entropy:\n                    if info_gain_ratio > max_info_gain_ratio:\n                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n                        index_feature_split = i\n                        threshold = best_split_point\n\n        \n        node.children = child_nodes\n        node.split_on = index_feature_split\n        if discrete[index_feature_split] is False:\n            node.threshold = threshold\n            \n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        if len(node.data) < 10:\n            return True\n        y = self.get_y(node.data)\n        return True if self.calculate_entropy(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n        for i in range(len(X)):  # Corrected range syntax\n            prediction = self.traverse_tree(X[i], self.root)\n            predictions[i] = prediction  # Insert prediction into the numpy array\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        if node.threshold is None:\n            predicted_class = self.traverse_tree(x, node.children[feat_value])\n        else:\n            if feat_value >= node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[1])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[0])\n        return predicted_class","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:40:44.723564Z","iopub.execute_input":"2024-03-21T17:40:44.724946Z","iopub.status.idle":"2024-03-21T17:40:44.765409Z","shell.execute_reply.started":"2024-03-21T17:40:44.724899Z","shell.execute_reply":"2024-03-21T17:40:44.763847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below presented algorithm is the variant of above with a change such that on continuous features it finds cut with respect to medain and mean of values in that feature So it works pretty fast but their is some compromise to the accuracy. Choosing mean and median has special effect mentioned in [https://www.tandfonline.com/doi/full/10.1080/08839514.2018.1447479](http://)","metadata":{}},{"cell_type":"markdown","source":"The main advantage of the proposed algorithm is that it avoids the sorting process with complexity of O(mn log n), also for each attribute, there are only two cut points to evaluate. In algorithm 2, we start by computing mean and median for each attribute with a complexity of O(2mn).","metadata":{}},{"cell_type":"code","source":"class DecisionTreeClassifierVFWithIGR:\n    def __init__(self , discrete):\n        self.root = Node()\n        self.discrete = discrete\n    @staticmethod\n    def calculate_entropy(Y):\n        _, labels_counts = np.unique(Y, return_counts=True)\n        total_instances = len(Y)\n        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n        return entropy\n    def split_on_feature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n\n        split_nodes = {}\n        weighted_entropy = 0\n        total_instances = len(data)\n        split_info = 0\n\n        for unique_value in unique_values:\n            partition = data[data[:, feat_index] == unique_value, :]\n            split_info = split_info + (len(partition)/total_instances)*np.log2(len(partition)/total_instances)\n            node = Node(data=partition)\n            split_nodes[unique_value] = node\n            partition_y = self.get_y(partition)\n            node_entropy = self.calculate_entropy(partition_y)\n            weighted_entropy += (len(partition) / total_instances) * node_entropy\n\n        return split_nodes, weighted_entropy, -1*split_info\n    \n    def very_fast_split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        mean = np.mean(feature_values)\n        median = np.median(feature_values)\n        total_entropy = self.calculate_entropy(data[:, -1])\n        total_instances = len(data)\n        weighted_entropy = 0\n        split_nodes = {}\n        \n        left_data1 = data[data[:, feat_index] <= mean]\n        right_data1 = data[data[:, feat_index] > mean]\n        left_node1 = Node(data = left_data1)\n        right_node1 = Node(data = right_data1)\n        left_entropy1 = self.calculate_entropy(left_data1[:, -1])\n        right_entropy1 = self.calculate_entropy(right_data1[:, -1])\n        info_gain1 = total_entropy - (len(left_data1) / total_instances * left_entropy1) - (len(right_data1) / total_instances * right_entropy1)\n        \n        left_data2 = data[data[:, feat_index] <= median]\n        right_data2 = data[data[:, feat_index] > median]\n        left_node2 = Node(data = left_data2)\n        right_node2 = Node(data = right_data2)\n        left_entropy2 = self.calculate_entropy(left_data2[:, -1])\n        right_entropy2 = self.calculate_entropy(right_data2[:, -1])\n        info_gain2 = total_entropy - (len(left_data2) / total_instances * left_entropy2) - (len(right_data2) / total_instances * right_entropy2)\n    \n        if info_gain1 > info_gain2:\n            weighted_entropy = (len(left_data1) / total_instances * left_entropy1) + (len(right_data1) / total_instances * right_entropy1)\n            split_info = (len(left_data1) / total_instances)*np.log2((len(left_data1)+0.001) / total_instances) + (len(right_data1) / total_instances)*np.log2((len(right_data1)+0.001) / total_instances)\n            best_split_point = mean\n            split_nodes[0] = left_node1\n            split_nodes[1] = right_node1\n        else:\n            weighted_entropy = (len(left_data2) / total_instances * left_entropy2) + (len(right_data2) / total_instances * right_entropy2)\n            split_info = (len(left_data2) / total_instances)*np.log2((len(left_data2)+0.001) / total_instances) + (len(right_data2) / total_instances)*np.log2((len(right_data2)+0.001) / total_instances)\n            best_split_point = median\n            split_nodes[0] = left_node2\n            split_nodes[1] = right_node2\n            \n        if len(split_nodes[0].data)==0 or len(split_nodes[1].data)==0: \n            weighted_entropy = self.calculate_entropy(self.get_y(data))\n        return best_split_point, split_nodes, weighted_entropy, -1*split_info\n               \n    def best_split(self, node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n        y = self.get_y(node.data)\n        if len(np.unique(y)) == 1:\n            return\n        tot_entropy = self.calculate_entropy(y)\n        index_feature_split = -1\n        min_entropy = 1\n        threshold = -1\n        max_info_gain_ratio = -float('inf')\n        avg_info_gain = 0\n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n                avg_info_gain += tot_entropy - weighted_entropy\n            else:\n                best_split_point, split_nodes, weighted_entropy, split_info = self.very_fast_split_on_contfeature(node.data, i)\n                avg_info_gain += tot_entropy - weighted_entropy\n                                 \n        avg_info_gain = avg_info_gain/(node.data.shape[1] - 1)                       \n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n                if tot_entropy - weighted_entropy >= avg_info_gain:\n                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n                    #if weighted_entropy < min_entropy:\n                    if info_gain_ratio > max_info_gain_ratio:\n                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n                        index_feature_split = i\n            else:\n                best_split_point, split_nodes, weighted_entropy, split_info = self.very_fast_split_on_contfeature(node.data, i)\n                if tot_entropy - weighted_entropy >= avg_info_gain:\n                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n                    #if weighted_entropy < min_entropy:\n                    if info_gain_ratio > max_info_gain_ratio:\n                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n                        index_feature_split = i\n                        threshold = best_split_point\n\n        \n        node.children = child_nodes\n        node.split_on = index_feature_split\n        if discrete[index_feature_split] is False:\n            node.threshold = threshold\n            \n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        if len(node.data) < 10:\n            return True\n        y = self.get_y(node.data)\n        return True if self.calculate_entropy(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n        for i in range(len(X)):  # Corrected range syntax\n            prediction = self.traverse_tree(X[i], self.root)\n            predictions[i] = prediction  # Insert prediction into the numpy array\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        if node.threshold is None:\n            predicted_class = self.traverse_tree(x, node.children[feat_value])\n        else:\n            if feat_value >= node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[1])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[0])\n        return predicted_class","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:40:46.208778Z","iopub.execute_input":"2024-03-21T17:40:46.209240Z","iopub.status.idle":"2024-03-21T17:40:46.240304Z","shell.execute_reply.started":"2024-03-21T17:40:46.209206Z","shell.execute_reply":"2024-03-21T17:40:46.239001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## OBSERVATION\n<big>I was using directly information gain ratio then the accuracy was coming around 51% - 52%. I noticed that the ratio may tend to favor attributes for which the denominator is very small. Consequently, it is suggested in two stages. First the information gain is calculated for all attributes. As a consequence, taking into consideration only attributes that have performed at least as good as the average information gain, the at- tribute that has obtained the best ratio gain is selected. This improved drastically the accuracy</big>","metadata":{}},{"cell_type":"markdown","source":"*This below is also the variant of first algorithm I provided in C4.5 in this to decide between features I use Information Gain only.*","metadata":{}},{"cell_type":"code","source":"class DecisionTreeClassifierwithIG:\n    def __init__(self , discrete):\n        self.root = Node()\n        self.discrete = discrete\n    @staticmethod\n    def calculate_entropy(Y):\n        _, labels_counts = np.unique(Y, return_counts=True)\n        total_instances = len(Y)\n        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n        return entropy\n    def split_on_feature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n\n        split_nodes = {}\n        weighted_entropy = 0\n        total_instances = len(data)\n\n        for unique_value in unique_values:\n            partition = data[data[:, feat_index] == unique_value, :]\n            node = Node(data=partition)\n            split_nodes[unique_value] = node\n            partition_y = self.get_y(partition)\n            node_entropy = self.calculate_entropy(partition_y)\n            weighted_entropy += (len(partition) / total_instances) * node_entropy\n\n        return split_nodes, weighted_entropy\n    \n    def split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        feature_values = np.sort(feature_values)\n        total_instances = len(data)\n        best_split_point = None\n        max_info_gain = -float('inf')\n        split_nodes = {}\n        weighted_entropy = 0\n\n        for i in range(1, len(feature_values)):\n            # Compute split point\n            split_point = (feature_values[i] + feature_values[i-1])/2\n\n            # Split data based on the split point\n            \n            left_data = data[data[:, feat_index] <= split_point]\n            right_data = data[data[:, feat_index] > split_point]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n\n            # Calculate information gain\n            left_entropy = self.calculate_entropy(left_data[:, -1])\n            right_entropy = self.calculate_entropy(right_data[:, -1])\n            total_entropy = self.calculate_entropy(data[:, -1])\n            \n            info_gain = total_entropy - (len(left_data) / total_instances * left_entropy) - (len(right_data) / total_instances * right_entropy)\n\n            # Update best split point if information gain is higher\n            if info_gain > max_info_gain:\n                max_info_gain = info_gain\n                weighted_entropy = (len(left_data) / total_instances * left_entropy) + (len(right_data) / total_instances * right_entropy)\n                best_split_point = split_point\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        return best_split_point, split_nodes, weighted_entropy\n\n        \n    def best_split(self, node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n\n        index_feature_split = -1\n        min_entropy = 1\n        threshold = -1\n\n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_entropy = self.split_on_feature(node.data, i)\n                if weighted_entropy < min_entropy:\n                    child_nodes, min_entropy = split_nodes, weighted_entropy\n                    index_feature_split = i\n            else:\n                best_split_point, split_nodes, weighted_entropy = self.split_on_contfeature(node.data, i)\n                if weighted_entropy < min_entropy:\n                    child_nodes, min_entropy = split_nodes, weighted_entropy\n                    index_feature_split = i\n                    threshold = best_split_point\n\n        \n        node.children = child_nodes\n        node.split_on = index_feature_split\n        if discrete[index_feature_split] is False:\n            node.threshold = threshold\n            \n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        if len(node.data) < 100:\n            return True\n        y = self.get_y(node.data)\n        return True if self.calculate_entropy(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n        for i in range(len(X)):  # Corrected range syntax\n            prediction = self.traverse_tree(X[i], self.root)\n            predictions[i] = prediction  # Insert prediction into the numpy array\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        if node.threshold is None:\n            predicted_class = self.traverse_tree(x, node.children[feat_value])\n        else:\n            if feat_value >= node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[1])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[0])\n        return predicted_class","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:40:47.727559Z","iopub.execute_input":"2024-03-21T17:40:47.728009Z","iopub.status.idle":"2024-03-21T17:40:47.751894Z","shell.execute_reply.started":"2024-03-21T17:40:47.727978Z","shell.execute_reply":"2024-03-21T17:40:47.750764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Information Gain Ratio vs Information Gain**\nThe IGR is a modification of information gain to reduce feature bias towards attributes that has many\nbranches. The gain ratio is large if the data is spread evenly and the value will be small if all data enters into one\nbranch","metadata":{}},{"cell_type":"code","source":"model1 = DecisionTreeClassifierwithIG(discrete)\nmodel1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:40:48.604642Z","iopub.execute_input":"2024-03-21T17:40:48.605060Z","iopub.status.idle":"2024-03-21T17:45:15.341173Z","shell.execute_reply.started":"2024-03-21T17:40:48.605030Z","shell.execute_reply":"2024-03-21T17:45:15.339649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = DecisionTreeClassifierWithIGR(discrete)\nmodel2.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:45:15.343489Z","iopub.execute_input":"2024-03-21T17:45:15.343914Z","iopub.status.idle":"2024-03-21T17:51:36.705311Z","shell.execute_reply.started":"2024-03-21T17:45:15.343882Z","shell.execute_reply":"2024-03-21T17:51:36.703873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3 = DecisionTreeClassifierVFWithIGR(discrete)\nmodel3.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:51:36.707042Z","iopub.execute_input":"2024-03-21T17:51:36.709077Z","iopub.status.idle":"2024-03-21T17:51:41.040201Z","shell.execute_reply.started":"2024-03-21T17:51:36.709024Z","shell.execute_reply":"2024-03-21T17:51:41.038977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Here You can check the accuracy of all the three models***","metadata":{}},{"cell_type":"code","source":"y_pred3 = model2.predict(X_test.to_numpy())\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred3))","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:51:41.043584Z","iopub.execute_input":"2024-03-21T17:51:41.044374Z","iopub.status.idle":"2024-03-21T17:51:41.065828Z","shell.execute_reply.started":"2024-03-21T17:51:41.044334Z","shell.execute_reply":"2024-03-21T17:51:41.064528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\ny_pred1 = model1.predict(X_test.to_numpy())\ny_pred2 = model2.predict(X_test.to_numpy())\ny_pred3 = model3.predict(X_test.to_numpy())\nprint(\"Model with Information_Gain\")\nprint(metrics.accuracy_score(y_test,y_pred1))\nprint(\"Model with Information_Gain_Ratio\")\nprint(metrics.accuracy_score(y_test,y_pred2))\nprint(\"Model with Information_Gain_Ratio (Very fast C4.5)\")\nprint(metrics.accuracy_score(y_test,y_pred3))","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:51:41.067192Z","iopub.execute_input":"2024-03-21T17:51:41.068022Z","iopub.status.idle":"2024-03-21T17:51:41.115613Z","shell.execute_reply.started":"2024-03-21T17:51:41.067982Z","shell.execute_reply":"2024-03-21T17:51:41.114388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CART ALGORITHM","metadata":{}},{"cell_type":"markdown","source":"#### **Classification And Regression Tree**\n* It is characterized by the fact that it constructs binary trees, namely each in- ternal node has exactly two outgoing edges.\n* CART works on Gini Impurity\n* CART allows for regression also, but I have implemented only the classification part","metadata":{}},{"cell_type":"code","source":"class Node:\n    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False , threshold=None):\n\n        self.data = data\n        self.children = children\n        self.split_on = split_on\n        self.threshold = threshold #Used when splitting using discrete features\n        self.pred_class = pred_class\n        self.is_leaf = is_leaf","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:43:29.924841Z","iopub.execute_input":"2024-04-13T05:43:29.925217Z","iopub.status.idle":"2024-04-13T05:43:29.931898Z","shell.execute_reply.started":"2024-04-13T05:43:29.925189Z","shell.execute_reply":"2024-04-13T05:43:29.930714Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"<h2>Gini Impurity</h2>\n\n<p>Gini impurity is an impurity-based criterion used in decision trees to measure the divergences between the probability distributions of the target attributeâ€™s values.</p>\n\n<p>The Gini impurity formula is defined as:</p>\n\n$$\n\\text{Gini}(y, S) = 1 - \\sum_{c_j \\in \\text{dom}(y)} \\left( \\frac{|\\bar{\\sigma}_{y=c_j}(S)|}{|S|} \\right)^2\n$$\n\n<p>Where:</p>\n\n<ul>\n  <li>$\\text{Gini}(y, S)$: Gini impurity for attribute $y$ and subset $S$.</li>\n  <li>$\\text{dom}(y)$: Domain of attribute $y$.</li>\n  <li>$\\bar{\\sigma}_{y=c_j}(S)$: Subset of $S$ where attribute $y$ takes the value $c_j$.</li>\n</ul>\n\n<p>The evaluation criterion for selecting the attribute $a_i$ is defined as:</p>\n\n$$\n\\text{GiniGain}(a_i, S) = \\text{Gini}(y, S) - \\sum_{\\bar{\\sigma}_{a_i=v_{i,j}}(S)} \\frac{|\\bar{\\sigma}_{a_i=v_{i,j}}(S)| \\cdot \\text{Gini}(y, \\bar{\\sigma}_{a_i=v_{i,j}}(S))}{|S|}\n$$\n\n<p>Where:</p>\n\n<ul>\n  <li>$\\text{GiniGain}(a_i, S)$: Gini gain for attribute $a_i$ and subset $S$.</li>\n  <li>$v_{i,j}$: Values of attribute $a_i$.</li>\n</ul>\n","metadata":{}},{"cell_type":"code","source":"class DecisionTreeClassifierwithGini:\n    def __init__(self , discrete):\n        self.root = Node()\n        self.discrete = discrete\n    @staticmethod\n    def calcGiniImpurity(Y):\n        total_samples = len(Y)\n        if total_samples == 0:\n            return 0.0\n\n        label_counts = {}\n        for label in Y:\n            if label in label_counts:\n                label_counts[label] += 1\n            else:\n                label_counts[label] = 1\n\n        gini_impurity = 1.0\n        for label in label_counts:\n            probability = label_counts[label] / total_samples\n            gini_impurity -= probability ** 2\n\n        return gini_impurity\n    \n    \n    def split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        feature_values = np.sort(feature_values)\n        total_instances = len(data)\n        best_split_point = None\n        mini_gini_impurity = float('inf')\n        split_nodes = {}\n        weighted_gini_impurity = 0\n\n        for i in range(1, len(feature_values)):\n            # Compute split point\n            split_point = (feature_values[i] + feature_values[i-1])/2\n\n            # Split data based on the split point\n            \n            left_data = data[data[:, feat_index] <= split_point]\n            right_data = data[data[:, feat_index] > split_point]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n\n            # Calculate information gain\n            left_gini_impurity = self.calcGiniImpurity(left_data[:, -1])\n            right_gini_impurity = self.calcGiniImpurity(right_data[:, -1])\n    \n        \n            impurity = (len(left_data) / total_instances * left_gini_impurity) + (len(right_data) / total_instances * right_gini_impurity)\n\n            # Update best split point if information gain is higher\n            if impurity < mini_gini_impurity:\n                mini_gini_impurity = impurity\n                weighted_gini_impurity = impurity\n                best_split_point = split_point\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        return split_nodes, weighted_gini_impurity, best_split_point\n\n    def split_on_feature(self , data , feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n        split_nodes = {}\n        total_instances = len(data)\n        best_split_point = None\n        mini_gini_impurity = float('inf')\n        weighted_gini_impurity = 0\n        \n        for unique_value in unique_values:\n            left_data = data[data[:, feat_index] == unique_value, :]\n            right_data = data[data[:, feat_index] != unique_value, :]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n            left_gini_impurity = self.calcGiniImpurity(left_data[:, -1])\n            right_gini_impurity = self.calcGiniImpurity(right_data[:, -1])\n    \n        \n            impurity = (len(left_data) / total_instances * left_gini_impurity) + (len(right_data) / total_instances * right_gini_impurity)\n\n            # Update best split point if information gain is higher\n            if impurity < mini_gini_impurity:\n                mini_gini_impurity = impurity\n                weighted_gini_impurity = impurity\n                best_split_point = unique_value\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        return split_nodes, weighted_gini_impurity, best_split_point\n        \n        \n    def best_split(self , node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n        y = self.get_y(node.data)\n        totalGiniImpurity = self.calcGiniImpurity(y)\n        index_feature_split = -1\n        #take with max ginigain\n        max_ginigain = 0\n        threshold = -1\n        \n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_gini_impurity, split_val = self.split_on_feature(node.data , i)\n                if totalGiniImpurity - weighted_gini_impurity > max_ginigain:\n                    child_nodes , max_ginigain = split_nodes, totalGiniImpurity - weighted_gini_impurity\n                    index_feature_split = i\n                    threshold = split_val\n            else:\n                split_nodes, weighted_gini_impurity, split_val = self.split_on_contfeature(node.data , i)\n                if totalGiniImpurity - weighted_gini_impurity > max_ginigain:\n                    child_nodes , max_ginigain = split_nodes, totalGiniImpurity - weighted_gini_impurity\n                    index_feature_split = i\n                    threshold = split_val\n                    \n        node.children = child_nodes\n        node.split_on = index_feature_split\n        node.threshold = threshold\n\n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        if len(node.data) < 100:\n            return True\n        y = self.get_y(node.data)\n        return True if self.calcGiniImpurity(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n        for i in range(len(X)):  # Corrected range syntax\n            prediction = self.traverse_tree(X[i], self.root)\n            predictions[i] = prediction  # Insert prediction into the numpy array\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        if discrete[node.split_on] is True:\n            if feat_value == node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[0])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[1])\n        else:\n            if feat_value > node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[1])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[0])\n        return predicted_class    ","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:43:32.467707Z","iopub.execute_input":"2024-04-13T05:43:32.468111Z","iopub.status.idle":"2024-04-13T05:43:32.500759Z","shell.execute_reply.started":"2024-04-13T05:43:32.468081Z","shell.execute_reply":"2024-04-13T05:43:32.499762Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifierwithGini(discrete)\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:43:36.193235Z","iopub.execute_input":"2024-04-13T05:43:36.193605Z","iopub.status.idle":"2024-04-13T05:47:52.566622Z","shell.execute_reply.started":"2024-04-13T05:43:36.193578Z","shell.execute_reply":"2024-04-13T05:47:52.565776Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test.to_numpy())\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:47:52.568222Z","iopub.execute_input":"2024-04-13T05:47:52.568752Z","iopub.status.idle":"2024-04-13T05:47:52.584762Z","shell.execute_reply.started":"2024-04-13T05:47:52.568722Z","shell.execute_reply":"2024-04-13T05:47:52.583714Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"0.92\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Making Bagging Classifier****","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\n\nclass BaggingClassifier:\n    def __init__(self, no_of_estimators, max_features, max_samples):\n        self.max_features = max_features\n        self.max_samples = max_samples\n        self.no_of_estimators = no_of_estimators\n        \n    @staticmethod\n    def sample_rows(df, percent):\n        return df.sample(int(percent*df.shape[0]),replace=True)\n    \n    @staticmethod\n    def sample_features(df,percent,discrete):\n        cols = random.sample(df.columns.tolist()[:-1],int(percent*(df.shape[1]-1)))\n        col_indices = [df.columns.tolist().index(col) for col in cols]\n        new_discrete = [discrete[i] for i in col_indices] \n        new_df = df[cols].copy()\n        new_df['target'] = df[23]\n        return new_df , new_discrete\n    \n    def fit(self, X, y, discrete):\n        data = np.column_stack([X, y])\n        data = pd.DataFrame(data)\n        self.estimators_ = []\n        for _ in range(self.no_of_estimators):\n            sample_data = self.sample_rows(data, self.max_samples)\n            nw_data ,new_discrete = self.sample_features(sample_data, self.max_features, discrete)\n            model = DecisionTreeClassifierwithGini(discrete)\n            model.fit(sample_data.iloc[:, :-1], sample_data.iloc[:, -1])\n            self.estimators_.append(model)\n    \n    def predict(self, X):\n        if not hasattr(self, 'estimators_'):\n            raise AttributeError(\"Model has not been trained yet. Call 'fit' with appropriate data first.\")\n        print(len(self.estimators_))\n        predictions = np.array([estimator.predict(X) for estimator in self.estimators_])\n        predictions = predictions.astype(int)\n        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T05:47:52.586396Z","iopub.execute_input":"2024-04-13T05:47:52.586736Z","iopub.status.idle":"2024-04-13T05:47:52.603019Z","shell.execute_reply.started":"2024-04-13T05:47:52.586707Z","shell.execute_reply":"2024-04-13T05:47:52.602244Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"model = BaggingClassifier(10,0.5,0.5)\nmodel.fit(X_train,y_train,discrete)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T06:13:40.107304Z","iopub.execute_input":"2024-04-13T06:13:40.107631Z","iopub.status.idle":"2024-04-13T06:24:39.698101Z","shell.execute_reply.started":"2024-04-13T06:13:40.107605Z","shell.execute_reply":"2024-04-13T06:24:39.696672Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test.to_numpy())\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T06:24:39.700724Z","iopub.execute_input":"2024-04-13T06:24:39.701277Z","iopub.status.idle":"2024-04-13T06:24:39.796971Z","shell.execute_reply.started":"2024-04-13T06:24:39.701242Z","shell.execute_reply":"2024-04-13T06:24:39.795517Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"10\n0.929\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Now Moving toward Random Forest","metadata":{}},{"cell_type":"code","source":"class Node:\n    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False , threshold=None):\n\n        self.data = data\n        self.children = children\n        self.split_on = split_on\n        self.threshold = threshold #Used when splitting using discrete features\n        self.pred_class = pred_class\n        self.is_leaf = is_leaf","metadata":{"execution":{"iopub.status.busy":"2024-04-13T06:24:39.798699Z","iopub.execute_input":"2024-04-13T06:24:39.799097Z","iopub.status.idle":"2024-04-13T06:24:39.807028Z","shell.execute_reply.started":"2024-04-13T06:24:39.799067Z","shell.execute_reply":"2024-04-13T06:24:39.805568Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"class DecisionTreeClassifierwithGiniforRF:\n    def __init__(self , discrete, max_samples, max_features):\n        self.root = Node()\n        self.discrete = discrete\n        self.max_samples = max_samples\n        self.max_features = max_features\n    @staticmethod\n    def calcGiniImpurity(Y):\n        total_samples = len(Y)\n        if total_samples == 0:\n            return 0.0\n\n        label_counts = {}\n        for label in Y:\n            if label in label_counts:\n                label_counts[label] += 1\n            else:\n                label_counts[label] = 1\n\n        gini_impurity = 1.0\n        for label in label_counts:\n            probability = label_counts[label] / total_samples\n            gini_impurity -= probability ** 2\n\n        return gini_impurity\n    \n    def split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        feature_values = np.sort(feature_values)\n        total_instances = len(data)\n        best_split_point = None\n        mini_gini_impurity = float('inf')\n        split_nodes = {}\n        weighted_gini_impurity = 0\n\n        for i in range(1, len(feature_values)):\n            # Compute split point\n            split_point = (feature_values[i] + feature_values[i-1])/2\n\n            # Split data based on the split point\n            \n            left_data = data[data[:, feat_index] <= split_point]\n            right_data = data[data[:, feat_index] > split_point]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n\n            # Calculate information gain\n            left_gini_impurity = self.calcGiniImpurity(left_data[:, -1])\n            right_gini_impurity = self.calcGiniImpurity(right_data[:, -1])\n    \n        \n            impurity = (len(left_data) / total_instances * left_gini_impurity) + (len(right_data) / total_instances * right_gini_impurity)\n\n            # Update best split point if information gain is higher\n            if impurity < mini_gini_impurity:\n                mini_gini_impurity = impurity\n                weighted_gini_impurity = impurity\n                best_split_point = split_point\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        return split_nodes, weighted_gini_impurity, best_split_point\n\n    def split_on_feature(self , data , feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n        split_nodes = {}\n        total_instances = len(data)\n        best_split_point = None\n        mini_gini_impurity = float('inf')\n        weighted_gini_impurity = 0\n        \n        for unique_value in unique_values:\n            left_data = data[data[:, feat_index] == unique_value, :]\n            right_data = data[data[:, feat_index] != unique_value, :]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n            left_gini_impurity = self.calcGiniImpurity(left_data[:, -1])\n            right_gini_impurity = self.calcGiniImpurity(right_data[:, -1])\n    \n        \n            impurity = (len(left_data) / total_instances * left_gini_impurity) + (len(right_data) / total_instances * right_gini_impurity)\n\n            # Update best split point if information gain is higher\n            if impurity < mini_gini_impurity:\n                mini_gini_impurity = impurity\n                weighted_gini_impurity = impurity\n                best_split_point = unique_value\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        return split_nodes, weighted_gini_impurity, best_split_point\n        \n    \n    def sampling(df):\n        percent = self.max_samples\n        data = df.sample(int(percent*df.shape[0]),replace=True)\n        return data\n    \n    def best_split(self , node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n        y = self.get_y(node.data)\n        totalGiniImpurity = self.calcGiniImpurity(y)\n        index_feature_split = -1\n        child_nodes = []\n        #take with max ginigain\n        max_ginigain = 0\n        threshold = -1\n        #\n            #HERE YOU HAVE TO CHANGE DATA TRYING TO DO SAMPLING HERE sampling according to your req.\n        new_data = node.data\n        #now column sampling remains\n        \n        # Column sampling for discrete property\n        total_features = new_data.shape[1] - 1  # Exclude the target column\n        num_features_to_sample = int(self.max_features * total_features)\n\n        # Column sampling for discrete property\n        sampled_columns = np.random.choice(range(total_features), num_features_to_sample, replace=False)\n        #\n        for i in sampled_columns:\n            if discrete[i] is True:\n                split_nodes, weighted_gini_impurity, split_val = self.split_on_feature(new_data , i)\n                if totalGiniImpurity - weighted_gini_impurity > max_ginigain:\n                    child_nodes , max_ginigain = split_nodes, totalGiniImpurity - weighted_gini_impurity\n                    index_feature_split = i\n                    threshold = split_val\n            else:\n                split_nodes, weighted_gini_impurity, split_val = self.split_on_contfeature(new_data , i)\n                if totalGiniImpurity - weighted_gini_impurity > max_ginigain:\n                    child_nodes , max_ginigain = split_nodes, totalGiniImpurity - weighted_gini_impurity\n                    index_feature_split = i\n                    threshold = split_val\n        \n        if len(child_nodes) == 0:\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n        \n        node.children = child_nodes\n        node.split_on = index_feature_split\n        node.threshold = threshold\n\n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        if len(node.data) < 100:\n            return True\n        y = self.get_y(node.data)\n        return True if self.calcGiniImpurity(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        print(data)\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n        for i in range(len(X)):  # Corrected range syntax\n            prediction = self.traverse_tree(X[i], self.root)\n            predictions[i] = prediction  # Insert prediction into the numpy array\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        if discrete[node.split_on] is True:\n            if feat_value == node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[0])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[1])\n        else:\n            if feat_value > node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[1])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[0])\n        return predicted_class    ","metadata":{"execution":{"iopub.status.busy":"2024-04-13T06:24:39.808656Z","iopub.execute_input":"2024-04-13T06:24:39.808966Z","iopub.status.idle":"2024-04-13T06:24:39.849387Z","shell.execute_reply.started":"2024-04-13T06:24:39.808941Z","shell.execute_reply":"2024-04-13T06:24:39.847974Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Taking many Decision Trees with RF. and forming Random forest","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\n\nclass RandomForest:\n    def __init__(self, no_of_estimators, max_samples, max_features):\n        self.max_features = max_features\n        self.max_samples = max_samples\n        self.no_of_estimators = no_of_estimators\n    @staticmethod\n    def sample_rows(df, percent):\n        return df.sample(int(percent*df.shape[0]),replace=True)\n    \n    def fit(self, X, y, discrete):\n        data = np.column_stack([X, y])\n        data = pd.DataFrame(data)\n        self.estimators_ = []\n        for _ in range(self.no_of_estimators):\n            sample_data = self.sample_rows(data, self.max_samples)\n            model = DecisionTreeClassifierwithGiniforRF(discrete,self.max_samples,self.max_features)\n            model.fit(sample_data.iloc[:, :-1], sample_data.iloc[:, -1])\n            self.estimators_.append(model)\n            print(\"model trained\")\n    \n    def predict(self, X):\n        if not hasattr(self, 'estimators_'):\n            raise AttributeError(\"Model has not been trained yet. Call 'fit' with appropriate data first.\")\n        print(len(self.estimators_))\n        predictions = np.array([estimator.predict(X) for estimator in self.estimators_])\n        predictions = predictions.astype(int)\n        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T06:24:39.851621Z","iopub.execute_input":"2024-04-13T06:24:39.851970Z","iopub.status.idle":"2024-04-13T06:24:39.866465Z","shell.execute_reply.started":"2024-04-13T06:24:39.851942Z","shell.execute_reply":"2024-04-13T06:24:39.865121Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"model = RandomForest(10,0.5,0.5)\nmodel.fit(X_train, y_train , discrete)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T06:24:39.868625Z","iopub.execute_input":"2024-04-13T06:24:39.868945Z","iopub.status.idle":"2024-04-13T06:29:52.525554Z","shell.execute_reply.started":"2024-04-13T06:24:39.868918Z","shell.execute_reply":"2024-04-13T06:29:52.523842Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"[[  1.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  0.          40.           0.         ...  -1.          -1.\n    0.        ]\n [  1.          33.33333333   0.         ...  -1.          -1.\n    1.        ]\n ...\n [  0.           0.           0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...   1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.           1.\n    0.        ]]\nmodel trained\n[[  0.          45.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n ...\n [  0.          54.54545455   0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  1.         100.           0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[ 0.         80.          0.         ... -1.         -1.\n   0.        ]\n [ 0.          0.          0.         ... -1.         -1.\n   1.        ]\n [ 0.         40.          0.         ... -1.         -1.\n   0.        ]\n ...\n [ 0.         66.66666667  0.         ... -1.         -1.\n   0.        ]\n [ 0.          0.          0.         ... -1.          1.\n   1.        ]\n [ 0.         88.88888889  0.         ... -1.         -1.\n   0.        ]]\nmodel trained\n[[  0.  70.   0. ...  -1.  -1.   0.]\n [  0. 100.   0. ...  -1.  -1.   0.]\n [  0.   0.   0. ...  -1.  -1.   0.]\n ...\n [  0.   0.   0. ...   1.  -1.   1.]\n [  0.   0.   0. ...  -1.   1.   0.]\n [  0. 100.   0. ...  -1.  -1.   0.]]\nmodel trained\n[[  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.           1.\n    1.        ]\n ...\n [  1.          46.15384615   0.         ...  -1.          -1.\n    1.        ]\n [  0.         100.           0.         ...   1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]]\nmodel trained\n[[  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  1.          63.63636364   0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n ...\n [  0.          90.           0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]]\nmodel trained\n[[  0.          87.5          0.         ...  -1.          -1.\n    1.        ]\n [  0.          22.85714286   0.         ...  -1.          -1.\n    0.        ]\n [  0.          50.           0.         ...  -1.          -1.\n    0.        ]\n ...\n [  0.           0.           0.         ...   1.           1.\n    1.        ]\n [  0.          93.75         0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.           1.\n    1.        ]]\nmodel trained\n[[  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  1.          27.27272727   0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n ...\n [  0.           0.           0.         ...   1.           1.\n    1.        ]\n [  0.          66.66666667   0.         ...  -1.          -1.\n    0.        ]\n [  0.          60.           0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[ 0.         64.70588235  0.         ... -1.         -1.\n   0.        ]\n [ 0.         55.55555556  0.         ... -1.         -1.\n   0.        ]\n [ 0.          0.          0.         ... -1.         -1.\n   0.        ]\n ...\n [ 0.         35.71428571  0.         ...  1.          1.\n   0.        ]\n [ 0.         90.          0.         ... -1.         -1.\n   1.        ]\n [ 1.         94.73684211  0.         ... -1.         -1.\n   0.        ]]\nmodel trained\n[[  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  0.          93.75         0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n ...\n [  1.          85.71428571   0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...   1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.           1.\n    1.        ]]\nmodel trained\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = model.predict(X_test.to_numpy())\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T06:29:52.527419Z","iopub.execute_input":"2024-04-13T06:29:52.527829Z","iopub.status.idle":"2024-04-13T06:29:52.640838Z","shell.execute_reply.started":"2024-04-13T06:29:52.527780Z","shell.execute_reply":"2024-04-13T06:29:52.639267Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"10\n0.928\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Testing with Scikit Learn","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf_gini = DecisionTreeClassifier(criterion='gini', random_state=0)\nclf_gini.fit(X_train,y_train)\ny_pred_gini = clf_gini.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:57:28.818553Z","iopub.execute_input":"2024-03-22T13:57:28.818925Z","iopub.status.idle":"2024-03-22T13:57:28.902489Z","shell.execute_reply.started":"2024-03-22T13:57:28.818896Z","shell.execute_reply":"2024-03-22T13:57:28.901249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred_gini))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:57:29.258349Z","iopub.execute_input":"2024-03-22T13:57:29.258742Z","iopub.status.idle":"2024-03-22T13:57:29.266918Z","shell.execute_reply.started":"2024-03-22T13:57:29.258714Z","shell.execute_reply":"2024-03-22T13:57:29.265742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf_entropy = DecisionTreeClassifier(criterion='entropy', random_state=0)\nclf_entropy.fit(X_train,y_train)\ny_pred_entropy = clf_entropy.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:57:30.995526Z","iopub.execute_input":"2024-03-22T13:57:30.996457Z","iopub.status.idle":"2024-03-22T13:57:31.081098Z","shell.execute_reply.started":"2024-03-22T13:57:30.996412Z","shell.execute_reply":"2024-03-22T13:57:31.079997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred_entropy))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:57:32.602650Z","iopub.execute_input":"2024-03-22T13:57:32.603283Z","iopub.status.idle":"2024-03-22T13:57:32.610869Z","shell.execute_reply.started":"2024-03-22T13:57:32.603251Z","shell.execute_reply":"2024-03-22T13:57:32.609485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}