{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7901931,"sourceType":"datasetVersion","datasetId":4640874},{"sourceId":7907242,"sourceType":"datasetVersion","datasetId":4644838},{"sourceId":7907320,"sourceType":"datasetVersion","datasetId":4644892}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-18T07:25:24.462971Z","iopub.execute_input":"2024-04-18T07:25:24.463504Z","iopub.status.idle":"2024-04-18T07:25:24.483790Z","shell.execute_reply.started":"2024-04-18T07:25:24.463431Z","shell.execute_reply":"2024-04-18T07:25:24.482543Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/phishingurl/Data_processed.csv\n/kaggle/input/image-1/Screenshot 2024-03-21 at 9.51.40PM.png\n/kaggle/input/image-2/Screenshot 2024-03-21 at 9.59.01PM.png\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **CREATING DATASET**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/phishingurl/Data_processed.csv')\ndf['target'] = np.where(df['status'] == 'phishing', 1, 0)\ndf.drop(columns=['url','status','lexical_features'], inplace=True)\ndf.drop(columns=['submit_email','sfh',], inplace=True)\ny=df['target']\nX = df.drop(columns=['target'])\nX = X[0:5000]\ny = y[0:5000]","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:25:29.555661Z","iopub.execute_input":"2024-04-18T07:25:29.556699Z","iopub.status.idle":"2024-04-18T07:25:29.669319Z","shell.execute_reply.started":"2024-04-18T07:25:29.556658Z","shell.execute_reply":"2024-04-18T07:25:29.667504Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:25:30.346754Z","iopub.execute_input":"2024-04-18T07:25:30.347206Z","iopub.status.idle":"2024-04-18T07:25:30.372064Z","shell.execute_reply.started":"2024-04-18T07:25:30.347168Z","shell.execute_reply":"2024-04-18T07:25:30.370673Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   login_form  links_in_tags  iframe  popup_window  safe_anchor  onmouseover  \\\n0           0      80.000000       0             0          0.0            0   \n1           0     100.000000       0             0        100.0            0   \n2           0     100.000000       0             0        100.0            0   \n3           0     100.000000       0             0         62.5            0   \n4           1      76.470588       0             0          0.0            0   \n\n   right_clic  whois_registered_domain  domain_registration_length  \\\n0           0                        0                          45   \n1           0                        0                          77   \n2           0                        0                          14   \n3           0                        0                          62   \n4           0                        0                         224   \n\n   domain_age  ...  embedded_domain  having_ip_address  no_of_dots  \\\n0          -1  ...               -1                  1           3   \n1        5767  ...                1                  1           1   \n2        4004  ...                1                  1           4   \n3          -1  ...               -1                  1           2   \n4        8175  ...               -1                  1           2   \n\n   no_of_sensitive_words  out_of_position_tld  https_token  url_length  \\\n0                      0                    1           -1          37   \n1                      0                    1           -1          77   \n2                      1                   -1            1         126   \n3                      0                    1           -1          18   \n4                      0                    1           -1          55   \n\n   tinyURL  prefixSuffix  target  \n0       -1            -1       0  \n1       -1            -1       1  \n2       -1             1       1  \n3       -1            -1       0  \n4       -1            -1       0  \n\n[5 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>login_form</th>\n      <th>links_in_tags</th>\n      <th>iframe</th>\n      <th>popup_window</th>\n      <th>safe_anchor</th>\n      <th>onmouseover</th>\n      <th>right_clic</th>\n      <th>whois_registered_domain</th>\n      <th>domain_registration_length</th>\n      <th>domain_age</th>\n      <th>...</th>\n      <th>embedded_domain</th>\n      <th>having_ip_address</th>\n      <th>no_of_dots</th>\n      <th>no_of_sensitive_words</th>\n      <th>out_of_position_tld</th>\n      <th>https_token</th>\n      <th>url_length</th>\n      <th>tinyURL</th>\n      <th>prefixSuffix</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>80.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>45</td>\n      <td>-1</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>37</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>100.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>100.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>77</td>\n      <td>5767</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>77</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>100.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>100.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14</td>\n      <td>4004</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>126</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>100.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>62.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>62</td>\n      <td>-1</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>18</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>76.470588</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>224</td>\n      <td>8175</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>55</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"discrete = [True, False, True, True, False, True, True, True, False, False, False, True, True, False, True, True, False, False, True, True, False, True, True, False]","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:25:30.885142Z","iopub.execute_input":"2024-04-18T07:25:30.885630Z","iopub.status.idle":"2024-04-18T07:25:30.892573Z","shell.execute_reply.started":"2024-04-18T07:25:30.885593Z","shell.execute_reply":"2024-04-18T07:25:30.891555Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:25:31.624133Z","iopub.execute_input":"2024-04-18T07:25:31.624743Z","iopub.status.idle":"2024-04-18T07:25:33.431629Z","shell.execute_reply.started":"2024-04-18T07:25:31.624700Z","shell.execute_reply":"2024-04-18T07:25:33.430076Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_train.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:25:33.433865Z","iopub.execute_input":"2024-04-18T07:25:33.434394Z","iopub.status.idle":"2024-04-18T07:25:33.444969Z","shell.execute_reply.started":"2024-04-18T07:25:33.434343Z","shell.execute_reply":"2024-04-18T07:25:33.443337Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Index(['login_form', 'links_in_tags', 'iframe', 'popup_window', 'safe_anchor',\n       'onmouseover', 'right_clic', 'whois_registered_domain',\n       'domain_registration_length', 'domain_age', 'web_traffic', 'dns_record',\n       'google_index', 'page_rank', 'embedded_domain', 'having_ip_address',\n       'no_of_dots', 'no_of_sensitive_words', 'out_of_position_tld',\n       'https_token', 'url_length', 'tinyURL', 'prefixSuffix'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"# ID3 ALGORITHM","metadata":{}},{"cell_type":"markdown","source":"#### **The above approach is the first algorithm developed for the decision trees.**\n* The major disadvantage is that it works only on categorical data\n* ID3 uses information gain as splitting criteria.\n* The growing stops when all instances belong to a single value of target feature or when    best information gain is not greater than zero\n* ID3 does not apply any pruning procedures nor does it handle numeric attributes or missing values.\n","metadata":{}},{"cell_type":"code","source":"class Node:\n    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False):\n\n        self.data = data\n        self.children = children\n        self.split_on = split_on\n        self.pred_class = pred_class\n        self.is_leaf = is_leaf","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:38:59.978281Z","iopub.execute_input":"2024-04-07T04:38:59.978732Z","iopub.status.idle":"2024-04-07T04:38:59.985997Z","shell.execute_reply.started":"2024-04-07T04:38:59.978693Z","shell.execute_reply":"2024-04-07T04:38:59.984671Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<h2>Information Gain and Entropy</h2>\n\n<p>Information gain is an impurity-based criterion used in decision trees to measure the reduction in uncertainty or impurity of a dataset after a particular feature is chosen for splitting.</p>\n\n<p>The formula for Information Gain ($\\text{IG}$) is:</p>\n\n$$\n\\text{IG}(a, S) = \\text{Entropy}(y, S) - \\sum_{\\bar{\\sigma}_{a=v_{i,j}}(S)} \\frac{|\\bar{\\sigma}_{a=v_{i,j}}(S)| \\cdot \\text{Entropy}(y, \\bar{\\sigma}_{a=v_{i,j}}(S))}{|S|}\n$$\n\n<p>Where:</p>\n\n<ul>\n  <li>$\\text{IG}(a, S)$: Information gain for attribute $a$ and subset $S$.</li>\n  <li>$\\text{Entropy}(y, S)$: Entropy of target attribute $y$ in subset $S$.</li>\n  <li>$v_{i,j}$: Values of attribute $a$.</li>\n</ul>\n\n<p>Entropy ($\\text{Entropy}$) is a measure of impurity or uncertainty in a dataset. The formula for Entropy is:</p>\n\n$$\n\\text{Entropy}(y, S) = - \\sum_{c_j \\in \\text{dom}(y)} \\frac{|\\bar{\\sigma}_{y=c_j}(S)|}{|S|} \\cdot \\log_2 \\frac{|\\bar{\\sigma}_{y=c_j}(S)|}{|S|}\n$$\n\n<p>Where:</p>\n\n<ul>\n  <li>$\\text{dom}(y)$: Domain of target attribute $y$.</li>\n  <li>$\\bar{\\sigma}_{y=c_j}(S)$: Subset of $S$ where target attribute $y$ takes the value $c_j$.</li>\n</ul>\n","metadata":{}},{"cell_type":"code","source":"class DecisionTreeClassifier:\n    def __init__(self):\n        self.root = Node()\n    @staticmethod\n    def calculate_entropy(Y):\n        _, labels_counts = np.unique(Y, return_counts=True)\n        total_instances = len(Y)\n        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n        return entropy\n    def split_on_feature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n\n        split_nodes = {}\n        weighted_entropy = 0\n        total_instances = len(data)\n\n        for unique_value in unique_values:\n            partition = data[data[:, feat_index] == unique_value, :]\n            node = Node(data=partition)\n            split_nodes[unique_value] = node\n            partition_y = self.get_y(partition)\n            node_entropy = self.calculate_entropy(partition_y)\n            weighted_entropy += (len(partition) / total_instances) * node_entropy\n\n        return split_nodes, weighted_entropy\n    def best_split(self, node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n\n        index_feature_split = -1\n        min_entropy = 1\n\n        for i in range(node.data.shape[1] - 1):\n            split_nodes, weighted_entropy = self.split_on_feature(node.data, i)\n            if weighted_entropy < min_entropy:\n                child_nodes, min_entropy = split_nodes, weighted_entropy\n                index_feature_split = i\n\n        node.children = child_nodes\n        node.split_on = index_feature_split\n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        y = self.get_y(node.data)\n        return True if self.calculate_entropy(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.array([self.traverse_tree(x, self.root) for index, x in X.iterrows()])\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        predicted_class = self.traverse_tree(x, node.children[feat_value])\n        return predicted_class","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:39:01.608868Z","iopub.execute_input":"2024-04-07T04:39:01.609447Z","iopub.status.idle":"2024-04-07T04:39:01.631674Z","shell.execute_reply.started":"2024-04-07T04:39:01.609403Z","shell.execute_reply":"2024-04-07T04:39:01.630329Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:06:06.555171Z","iopub.execute_input":"2024-03-21T17:06:06.555634Z","iopub.status.idle":"2024-03-21T17:06:08.016770Z","shell.execute_reply.started":"2024-03-21T17:06:06.555600Z","shell.execute_reply":"2024-03-21T17:06:08.015072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some **advantages** in using ID3 including easy decomposability, strong intuitive nature etc. And some\n**disadvantages** include the following;\n<br>\n• Using information gain for feature selection, the algorithm tends to select attributes with more values, which is\ndue to the fact that the value of the information gain of this kind of attribute will be bigger than others.\n<br>\n• In the decision tree building process, it is difficult to control the tree size. However, most researchers have tried\nto improve on this using various pruning methods to avoid the occurrence of over-fitting, which has led to the\ndecision tree building process to be completed in two steps, that is modeling and pruning. Meanwhile, it will save\na lot of time if a concise decision tree is built in onestep.\n<br>\n• There are several logarithmic calculations in the attribute selection process, this has made the computation of\ninformation gain time consuming.","metadata":{}},{"cell_type":"markdown","source":"#### *This procedure will not work on our dataset. As some of the features in my datasets are continuos values. (not categorical)*","metadata":{}},{"cell_type":"markdown","source":"# C4.5 Decision Trees","metadata":{}},{"cell_type":"markdown","source":"#### **C4.5 was the next algorithm discovered in this feild**\n* It uses gain ratio as splitting criteria\n* The splitting ceases when the number of instances to be split is below a certain threshold\n* *C4.5 can handle numeric attributes.*","metadata":{}},{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kagglesdsdata/datasets/4644892/7907320/Screenshot%202024-03-21%20at%209.59.01PM.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240321%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240321T163522Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=94e0a88cde06144312f1496f726e8dbe75803b314d79d2cd7e0cbcb76d96ba377e88882dca68b6636a970dfceee0680e80ad0f6f66d38afcbca51d1b2cd7ca34947d21256d6acf9246bc90cefac628205f5105be1219e4a1f54568526d30012f6867719663b5ac3ad6a725179d2797c7e2aea056d3a62e173705262e98e571f97fba87fe3c7e0c92f3fea9025268c4fa66122ec83be45725b42b5219e4c028f732e6fe33ce8ecaaebd1a5c293982c92ae34670d68129a98963286f88bb3ae6df87da095955566299e693ff8bb5457397bf8ff57c71c66c3a7ece72c9816757e00377f790baf65590c315c6f56afbb450735e810b45199ae1631e8cbcfaac9e7e)","metadata":{}},{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kagglesdsdata/datasets/4644838/7907242/Screenshot%202024-03-21%20at%209.51.40PM.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240321%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240321T164759Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=738ad3a70255123689ea37e6f8a326aae6960a0eddcddf490dd086ef39b59a7b4d3563cbf35fd152fadadf22d48d29f37d4ae35079c78d2f91217a8a73a07961e4278a64c33cfba8a4e2c70a5568c254f975dceba97bdd2452960b7dd6d28b3f40f7e60bcd57b3dc85e76c99a1a9a24f3e3490d15984062e7a42dd73c786896031a3ded13265c65cac6a66e4b930898e8268243d38465239bdfac9c9654d716207483e50c078e5af64d14a3e1f9df0a395f935d57333211a588f1a31e419f5bb9f72d7994897bd056e93ddf09f329cf03e61cd4020cb62655d06ad555e426d59ac75ec21625cab448d57484882c89d0749727ad068277eee60702f70c8e56950)","metadata":{}},{"cell_type":"markdown","source":"### I have tried three variants of C4.5 Decision Tree","metadata":{}},{"cell_type":"code","source":"class Node:\n    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False , threshold=None):\n\n        self.data = data\n        self.children = children\n        self.split_on = split_on\n        self.threshold = threshold #Used when splitting using discrete features\n        self.pred_class = pred_class\n        self.is_leaf = is_leaf","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:40:41.744284Z","iopub.execute_input":"2024-03-21T17:40:41.744725Z","iopub.status.idle":"2024-03-21T17:40:41.751264Z","shell.execute_reply.started":"2024-03-21T17:40:41.744694Z","shell.execute_reply":"2024-03-21T17:40:41.749760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In the below algorithm** I have used Information Gain Ratio, for finding the best split threshold and on which feature to split. To find the best split in continuos feature it first sort the dataset and finds cut with maximum Information Gain and finally among all features, feature with maximum Information Gain ratio is selected.","metadata":{}},{"cell_type":"markdown","source":"<h2>Information Gain Ratio</h2>\n\n<p>Information gain ratio is a normalized version of information gain, which takes into account the intrinsic information of a split. It helps in reducing the bias towards attributes with a large number of distinct values.</p>\n\n<p>The formula for Information Gain Ratio ($\\text{IGR}$) is:</p>\n\n$$\n\\text{IGR}(a, S) = \\frac{\\text{InformationGain}(a, S)}{\\text{Entropy}(a, S)}\n$$\n\n<p>Where:</p>\n\n<ul>\n  <li>$\\text{IGR}(a, S)$: Information gain ratio for attribute $a$ and subset $S$.</li>\n  <li>$\\text{InformationGain}(a, S)$: Information gain for attribute $a$ and subset $S$.</li>\n  <li>$\\text{Entropy}(a, S)$: Entropy of attribute $a$ in subset $S$.</li>\n</ul>\n\n<p>Note that this ratio is not defined when the denominator is zero. Also, the ratio may tend to favor attributes for which the denominator is very small. Consequently, it is suggested to first calculate the information gain for all attributes and then consider only attributes that have performed at least as well as the average information gain when selecting the best attribute based on the gain ratio.</p>\n\n<p>It has been shown that the gain ratio tends to outperform simple information gain criteria, both in terms of accuracy and classifier complexity.</p>\n","metadata":{}},{"cell_type":"code","source":"class DecisionTreeClassifierWithIGR:\n    def __init__(self , discrete):\n        self.root = Node()\n        self.discrete = discrete\n    @staticmethod\n    def calculate_entropy(Y):\n        _, labels_counts = np.unique(Y, return_counts=True)\n        total_instances = len(Y)\n        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n        return entropy\n    def split_on_feature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n\n        split_nodes = {}\n        weighted_entropy = 0\n        total_instances = len(data)\n        split_info = 0\n\n        for unique_value in unique_values:\n            partition = data[data[:, feat_index] == unique_value, :]\n            split_info = split_info + (len(partition)/total_instances)*np.log2(len(partition)/total_instances)\n            node = Node(data=partition)\n            split_nodes[unique_value] = node\n            partition_y = self.get_y(partition)\n            node_entropy = self.calculate_entropy(partition_y)\n            weighted_entropy += (len(partition) / total_instances) * node_entropy\n\n        return split_nodes, weighted_entropy, -1*split_info\n    \n    def very_fast_split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        mean = np.mean(feature_values)\n        median = np.median(feature_values)\n        total_entropy = self.calculate_entropy(data[:, -1])\n        total_instances = len(data)\n        weighted_entropy = 0\n        split_nodes = {}\n        \n        left_data1 = data[data[:, feat_index] <= mean]\n        right_data1 = data[data[:, feat_index] > mean]\n        left_node1 = Node(data = left_data1)\n        right_node1 = Node(data = right_data1)\n        left_entropy1 = self.calculate_entropy(left_data1[:, -1])\n        right_entropy1 = self.calculate_entropy(right_data1[:, -1])\n        info_gain1 = total_entropy - (len(left_data1) / total_instances * left_entropy1) - (len(right_data1) / total_instances * right_entropy1)\n        \n        left_data2 = data[data[:, feat_index] <= median]\n        right_data2 = data[data[:, feat_index] > median]\n        left_node2 = Node(data = left_data2)\n        right_node2 = Node(data = right_data2)\n        left_entropy2 = self.calculate_entropy(left_data2[:, -1])\n        right_entropy2 = self.calculate_entropy(right_data2[:, -1])\n        info_gain2 = total_entropy - (len(left_data2) / total_instances * left_entropy2) - (len(right_data2) / total_instances * right_entropy2)\n    \n        if info_gain1 > info_gain2:\n            weighted_entropy = (len(left_data1) / total_instances * left_entropy1) + (len(right_data1) / total_instances * right_entropy1)\n            split_info = (len(left_data1) / total_instances)*np.log2((len(left_data1)+0.001) / total_instances) + (len(right_data1) / total_instances)*np.log2((len(right_data1)+0.001) / total_instances)\n            best_split_point = mean\n            split_nodes[0] = left_node1\n            split_nodes[1] = right_node1\n        else:\n            weighted_entropy = (len(left_data2) / total_instances * left_entropy2) + (len(right_data2) / total_instances * right_entropy2)\n            split_info = (len(left_data2) / total_instances)*np.log2((len(left_data2)+0.001) / total_instances) + (len(right_data2) / total_instances)*np.log2((len(right_data2)+0.001) / total_instances)\n            best_split_point = median\n            split_nodes[0] = left_node2\n            split_nodes[1] = right_node2\n        \n        return best_split_point, split_nodes, weighted_entropy, -1*split_info\n        \n    def split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        feature_values = np.sort(feature_values)\n        total_instances = len(data)\n        best_split_point = None\n        max_info_gain = -float('inf')\n        split_nodes = {}\n        y = self.get_y(data)\n        weighted_entropy = 0\n        split_info = 0\n\n        for i in range(1, len(feature_values)):\n            # Compute split point\n            split_point = (feature_values[i] + feature_values[i-1])/2\n\n            # Split data based on the split point\n            \n            left_data = data[data[:, feat_index] <= split_point]\n            right_data = data[data[:, feat_index] > split_point]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n\n            # Calculate information gain\n            left_entropy = self.calculate_entropy(left_data[:, -1])\n            right_entropy = self.calculate_entropy(right_data[:, -1])\n            total_entropy = self.calculate_entropy(data[:, -1])\n            \n            info_gain = total_entropy - (len(left_data) / total_instances * left_entropy) - (len(right_data) / total_instances * right_entropy)\n            \n\n            # Update best split point if information gain is higher\n            if info_gain > max_info_gain:\n                max_info_gain = info_gain\n                weighted_entropy = (len(left_data) / total_instances * left_entropy) + (len(right_data) / total_instances * right_entropy)\n                split_info = (len(left_data) / total_instances)*np.log2((len(left_data)+0.001) / total_instances) + (len(right_data) / total_instances)*np.log2((len(right_data)+0.001) / total_instances)\n                best_split_point = split_point\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        if len(split_nodes[0].data)==0 or len(split_nodes[1].data)==0: \n            weighted_entropy = self.calculate_entropy(y)\n        return best_split_point, split_nodes, weighted_entropy, -1*split_info\n\n        \n    def best_split(self, node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n        y = self.get_y(node.data)\n        if len(np.unique(y)) == 1:\n            return\n        tot_entropy = self.calculate_entropy(y)\n        index_feature_split = -1\n        min_entropy = 1\n        threshold = -1\n        max_info_gain_ratio = -float('inf')\n        avg_info_gain = 0\n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n                avg_info_gain += tot_entropy - weighted_entropy\n            else:\n                best_split_point, split_nodes, weighted_entropy, split_info = self.very_fast_split_on_contfeature(node.data, i)\n                avg_info_gain += tot_entropy - weighted_entropy\n                                 \n        avg_info_gain = avg_info_gain/(node.data.shape[1] - 1)   \n\n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n                if tot_entropy - weighted_entropy >= avg_info_gain:\n                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n                    #if weighted_entropy < min_entropy:\n                    if info_gain_ratio > max_info_gain_ratio:\n                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n                        index_feature_split = i\n            else:\n                best_split_point, split_nodes, weighted_entropy, split_info = self.split_on_contfeature(node.data, i)\n                if tot_entropy - weighted_entropy >= avg_info_gain:\n                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n                    #if weighted_entropy < min_entropy:\n                    if info_gain_ratio > max_info_gain_ratio:\n                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n                        index_feature_split = i\n                        threshold = best_split_point\n\n        \n        node.children = child_nodes\n        node.split_on = index_feature_split\n        if discrete[index_feature_split] is False:\n            node.threshold = threshold\n            \n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        if len(node.data) < 10:\n            return True\n        y = self.get_y(node.data)\n        return True if self.calculate_entropy(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n        for i in range(len(X)):  # Corrected range syntax\n            prediction = self.traverse_tree(X[i], self.root)\n            predictions[i] = prediction  # Insert prediction into the numpy array\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        if node.threshold is None:\n            predicted_class = self.traverse_tree(x, node.children[feat_value])\n        else:\n            if feat_value >= node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[1])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[0])\n        return predicted_class","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:40:44.723564Z","iopub.execute_input":"2024-03-21T17:40:44.724946Z","iopub.status.idle":"2024-03-21T17:40:44.765409Z","shell.execute_reply.started":"2024-03-21T17:40:44.724899Z","shell.execute_reply":"2024-03-21T17:40:44.763847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below presented algorithm is the variant of above with a change such that on continuous features it finds cut with respect to medain and mean of values in that feature So it works pretty fast but their is some compromise to the accuracy. Choosing mean and median has special effect mentioned in [https://www.tandfonline.com/doi/full/10.1080/08839514.2018.1447479](http://)","metadata":{}},{"cell_type":"markdown","source":"The main advantage of the proposed algorithm is that it avoids the sorting process with complexity of O(mn log n), also for each attribute, there are only two cut points to evaluate. In algorithm 2, we start by computing mean and median for each attribute with a complexity of O(2mn).","metadata":{}},{"cell_type":"code","source":"class DecisionTreeClassifierVFWithIGR:\n    def __init__(self , discrete):\n        self.root = Node()\n        self.discrete = discrete\n    @staticmethod\n    def calculate_entropy(Y):\n        _, labels_counts = np.unique(Y, return_counts=True)\n        total_instances = len(Y)\n        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n        return entropy\n    def split_on_feature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n\n        split_nodes = {}\n        weighted_entropy = 0\n        total_instances = len(data)\n        split_info = 0\n\n        for unique_value in unique_values:\n            partition = data[data[:, feat_index] == unique_value, :]\n            split_info = split_info + (len(partition)/total_instances)*np.log2(len(partition)/total_instances)\n            node = Node(data=partition)\n            split_nodes[unique_value] = node\n            partition_y = self.get_y(partition)\n            node_entropy = self.calculate_entropy(partition_y)\n            weighted_entropy += (len(partition) / total_instances) * node_entropy\n\n        return split_nodes, weighted_entropy, -1*split_info\n    \n    def very_fast_split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        mean = np.mean(feature_values)\n        median = np.median(feature_values)\n        total_entropy = self.calculate_entropy(data[:, -1])\n        total_instances = len(data)\n        weighted_entropy = 0\n        split_nodes = {}\n        \n        left_data1 = data[data[:, feat_index] <= mean]\n        right_data1 = data[data[:, feat_index] > mean]\n        left_node1 = Node(data = left_data1)\n        right_node1 = Node(data = right_data1)\n        left_entropy1 = self.calculate_entropy(left_data1[:, -1])\n        right_entropy1 = self.calculate_entropy(right_data1[:, -1])\n        info_gain1 = total_entropy - (len(left_data1) / total_instances * left_entropy1) - (len(right_data1) / total_instances * right_entropy1)\n        \n        left_data2 = data[data[:, feat_index] <= median]\n        right_data2 = data[data[:, feat_index] > median]\n        left_node2 = Node(data = left_data2)\n        right_node2 = Node(data = right_data2)\n        left_entropy2 = self.calculate_entropy(left_data2[:, -1])\n        right_entropy2 = self.calculate_entropy(right_data2[:, -1])\n        info_gain2 = total_entropy - (len(left_data2) / total_instances * left_entropy2) - (len(right_data2) / total_instances * right_entropy2)\n    \n        if info_gain1 > info_gain2:\n            weighted_entropy = (len(left_data1) / total_instances * left_entropy1) + (len(right_data1) / total_instances * right_entropy1)\n            split_info = (len(left_data1) / total_instances)*np.log2((len(left_data1)+0.001) / total_instances) + (len(right_data1) / total_instances)*np.log2((len(right_data1)+0.001) / total_instances)\n            best_split_point = mean\n            split_nodes[0] = left_node1\n            split_nodes[1] = right_node1\n        else:\n            weighted_entropy = (len(left_data2) / total_instances * left_entropy2) + (len(right_data2) / total_instances * right_entropy2)\n            split_info = (len(left_data2) / total_instances)*np.log2((len(left_data2)+0.001) / total_instances) + (len(right_data2) / total_instances)*np.log2((len(right_data2)+0.001) / total_instances)\n            best_split_point = median\n            split_nodes[0] = left_node2\n            split_nodes[1] = right_node2\n            \n        if len(split_nodes[0].data)==0 or len(split_nodes[1].data)==0: \n            weighted_entropy = self.calculate_entropy(self.get_y(data))\n        return best_split_point, split_nodes, weighted_entropy, -1*split_info\n               \n    def best_split(self, node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n        y = self.get_y(node.data)\n        if len(np.unique(y)) == 1:\n            return\n        tot_entropy = self.calculate_entropy(y)\n        index_feature_split = -1\n        min_entropy = 1\n        threshold = -1\n        max_info_gain_ratio = -float('inf')\n        avg_info_gain = 0\n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n                avg_info_gain += tot_entropy - weighted_entropy\n            else:\n                best_split_point, split_nodes, weighted_entropy, split_info = self.very_fast_split_on_contfeature(node.data, i)\n                avg_info_gain += tot_entropy - weighted_entropy\n                                 \n        avg_info_gain = avg_info_gain/(node.data.shape[1] - 1)                       \n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n                if tot_entropy - weighted_entropy >= avg_info_gain:\n                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n                    #if weighted_entropy < min_entropy:\n                    if info_gain_ratio > max_info_gain_ratio:\n                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n                        index_feature_split = i\n            else:\n                best_split_point, split_nodes, weighted_entropy, split_info = self.very_fast_split_on_contfeature(node.data, i)\n                if tot_entropy - weighted_entropy >= avg_info_gain:\n                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n                    #if weighted_entropy < min_entropy:\n                    if info_gain_ratio > max_info_gain_ratio:\n                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n                        index_feature_split = i\n                        threshold = best_split_point\n\n        \n        node.children = child_nodes\n        node.split_on = index_feature_split\n        if discrete[index_feature_split] is False:\n            node.threshold = threshold\n            \n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        if len(node.data) < 10:\n            return True\n        y = self.get_y(node.data)\n        return True if self.calculate_entropy(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n        for i in range(len(X)):  # Corrected range syntax\n            prediction = self.traverse_tree(X[i], self.root)\n            predictions[i] = prediction  # Insert prediction into the numpy array\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        if node.threshold is None:\n            predicted_class = self.traverse_tree(x, node.children[feat_value])\n        else:\n            if feat_value >= node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[1])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[0])\n        return predicted_class","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:40:46.208778Z","iopub.execute_input":"2024-03-21T17:40:46.209240Z","iopub.status.idle":"2024-03-21T17:40:46.240304Z","shell.execute_reply.started":"2024-03-21T17:40:46.209206Z","shell.execute_reply":"2024-03-21T17:40:46.239001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## OBSERVATION\n<big>I was using directly information gain ratio then the accuracy was coming around 51% - 52%. I noticed that the ratio may tend to favor attributes for which the denominator is very small. Consequently, it is suggested in two stages. First the information gain is calculated for all attributes. As a consequence, taking into consideration only attributes that have performed at least as good as the average information gain, the at- tribute that has obtained the best ratio gain is selected. This improved drastically the accuracy</big>","metadata":{}},{"cell_type":"markdown","source":"*This below is also the variant of first algorithm I provided in C4.5 in this to decide between features I use Information Gain only.*","metadata":{}},{"cell_type":"code","source":"class DecisionTreeClassifierwithIG:\n    def __init__(self , discrete):\n        self.root = Node()\n        self.discrete = discrete\n    @staticmethod\n    def calculate_entropy(Y):\n        _, labels_counts = np.unique(Y, return_counts=True)\n        total_instances = len(Y)\n        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n        return entropy\n    def split_on_feature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n\n        split_nodes = {}\n        weighted_entropy = 0\n        total_instances = len(data)\n\n        for unique_value in unique_values:\n            partition = data[data[:, feat_index] == unique_value, :]\n            node = Node(data=partition)\n            split_nodes[unique_value] = node\n            partition_y = self.get_y(partition)\n            node_entropy = self.calculate_entropy(partition_y)\n            weighted_entropy += (len(partition) / total_instances) * node_entropy\n\n        return split_nodes, weighted_entropy\n    \n    def split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        feature_values = np.sort(feature_values)\n        total_instances = len(data)\n        best_split_point = None\n        max_info_gain = -float('inf')\n        split_nodes = {}\n        weighted_entropy = 0\n\n        for i in range(1, len(feature_values)):\n            # Compute split point\n            split_point = (feature_values[i] + feature_values[i-1])/2\n\n            # Split data based on the split point\n            \n            left_data = data[data[:, feat_index] <= split_point]\n            right_data = data[data[:, feat_index] > split_point]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n\n            # Calculate information gain\n            left_entropy = self.calculate_entropy(left_data[:, -1])\n            right_entropy = self.calculate_entropy(right_data[:, -1])\n            total_entropy = self.calculate_entropy(data[:, -1])\n            \n            info_gain = total_entropy - (len(left_data) / total_instances * left_entropy) - (len(right_data) / total_instances * right_entropy)\n\n            # Update best split point if information gain is higher\n            if info_gain > max_info_gain:\n                max_info_gain = info_gain\n                weighted_entropy = (len(left_data) / total_instances * left_entropy) + (len(right_data) / total_instances * right_entropy)\n                best_split_point = split_point\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        return best_split_point, split_nodes, weighted_entropy\n\n        \n    def best_split(self, node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n\n        index_feature_split = -1\n        min_entropy = 1\n        threshold = -1\n\n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_entropy = self.split_on_feature(node.data, i)\n                if weighted_entropy < min_entropy:\n                    child_nodes, min_entropy = split_nodes, weighted_entropy\n                    index_feature_split = i\n            else:\n                best_split_point, split_nodes, weighted_entropy = self.split_on_contfeature(node.data, i)\n                if weighted_entropy < min_entropy:\n                    child_nodes, min_entropy = split_nodes, weighted_entropy\n                    index_feature_split = i\n                    threshold = best_split_point\n\n        \n        node.children = child_nodes\n        node.split_on = index_feature_split\n        if discrete[index_feature_split] is False:\n            node.threshold = threshold\n            \n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        if len(node.data) < 100:\n            return True\n        y = self.get_y(node.data)\n        return True if self.calculate_entropy(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n        for i in range(len(X)):  # Corrected range syntax\n            prediction = self.traverse_tree(X[i], self.root)\n            predictions[i] = prediction  # Insert prediction into the numpy array\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        if node.threshold is None:\n            predicted_class = self.traverse_tree(x, node.children[feat_value])\n        else:\n            if feat_value >= node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[1])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[0])\n        return predicted_class","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:40:47.727559Z","iopub.execute_input":"2024-03-21T17:40:47.728009Z","iopub.status.idle":"2024-03-21T17:40:47.751894Z","shell.execute_reply.started":"2024-03-21T17:40:47.727978Z","shell.execute_reply":"2024-03-21T17:40:47.750764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Information Gain Ratio vs Information Gain**\nThe IGR is a modification of information gain to reduce feature bias towards attributes that has many\nbranches. The gain ratio is large if the data is spread evenly and the value will be small if all data enters into one\nbranch","metadata":{}},{"cell_type":"code","source":"model1 = DecisionTreeClassifierwithIG(discrete)\nmodel1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:40:48.604642Z","iopub.execute_input":"2024-03-21T17:40:48.605060Z","iopub.status.idle":"2024-03-21T17:45:15.341173Z","shell.execute_reply.started":"2024-03-21T17:40:48.605030Z","shell.execute_reply":"2024-03-21T17:45:15.339649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = DecisionTreeClassifierWithIGR(discrete)\nmodel2.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:45:15.343489Z","iopub.execute_input":"2024-03-21T17:45:15.343914Z","iopub.status.idle":"2024-03-21T17:51:36.705311Z","shell.execute_reply.started":"2024-03-21T17:45:15.343882Z","shell.execute_reply":"2024-03-21T17:51:36.703873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3 = DecisionTreeClassifierVFWithIGR(discrete)\nmodel3.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:51:36.707042Z","iopub.execute_input":"2024-03-21T17:51:36.709077Z","iopub.status.idle":"2024-03-21T17:51:41.040201Z","shell.execute_reply.started":"2024-03-21T17:51:36.709024Z","shell.execute_reply":"2024-03-21T17:51:41.038977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Here You can check the accuracy of all the three models***","metadata":{}},{"cell_type":"code","source":"y_pred3 = model2.predict(X_test.to_numpy())\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred3))","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:51:41.043584Z","iopub.execute_input":"2024-03-21T17:51:41.044374Z","iopub.status.idle":"2024-03-21T17:51:41.065828Z","shell.execute_reply.started":"2024-03-21T17:51:41.044334Z","shell.execute_reply":"2024-03-21T17:51:41.064528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\ny_pred1 = model1.predict(X_test.to_numpy())\ny_pred2 = model2.predict(X_test.to_numpy())\ny_pred3 = model3.predict(X_test.to_numpy())\nprint(\"Model with Information_Gain\")\nprint(metrics.accuracy_score(y_test,y_pred1))\nprint(\"Model with Information_Gain_Ratio\")\nprint(metrics.accuracy_score(y_test,y_pred2))\nprint(\"Model with Information_Gain_Ratio (Very fast C4.5)\")\nprint(metrics.accuracy_score(y_test,y_pred3))","metadata":{"execution":{"iopub.status.busy":"2024-03-21T17:51:41.067192Z","iopub.execute_input":"2024-03-21T17:51:41.068022Z","iopub.status.idle":"2024-03-21T17:51:41.115613Z","shell.execute_reply.started":"2024-03-21T17:51:41.067982Z","shell.execute_reply":"2024-03-21T17:51:41.114388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CART ALGORITHM","metadata":{}},{"cell_type":"markdown","source":"#### **Classification And Regression Tree**\n* It is characterized by the fact that it constructs binary trees, namely each in- ternal node has exactly two outgoing edges.\n* CART works on Gini Impurity\n* CART allows for regression also, but I have implemented only the classification part","metadata":{}},{"cell_type":"code","source":"class Node:\n    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False , threshold=None):\n\n        self.data = data\n        self.children = children\n        self.split_on = split_on\n        self.threshold = threshold #Used when splitting using discrete features\n        self.pred_class = pred_class\n        self.is_leaf = is_leaf","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:25:45.343892Z","iopub.execute_input":"2024-04-18T07:25:45.344323Z","iopub.status.idle":"2024-04-18T07:25:45.353236Z","shell.execute_reply.started":"2024-04-18T07:25:45.344289Z","shell.execute_reply":"2024-04-18T07:25:45.351435Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"<h2>Gini Impurity</h2>\n\n<p>Gini impurity is an impurity-based criterion used in decision trees to measure the divergences between the probability distributions of the target attribute’s values.</p>\n\n<p>The Gini impurity formula is defined as:</p>\n\n$$\n\\text{Gini}(y, S) = 1 - \\sum_{c_j \\in \\text{dom}(y)} \\left( \\frac{|\\bar{\\sigma}_{y=c_j}(S)|}{|S|} \\right)^2\n$$\n\n<p>Where:</p>\n\n<ul>\n  <li>$\\text{Gini}(y, S)$: Gini impurity for attribute $y$ and subset $S$.</li>\n  <li>$\\text{dom}(y)$: Domain of attribute $y$.</li>\n  <li>$\\bar{\\sigma}_{y=c_j}(S)$: Subset of $S$ where attribute $y$ takes the value $c_j$.</li>\n</ul>\n\n<p>The evaluation criterion for selecting the attribute $a_i$ is defined as:</p>\n\n$$\n\\text{GiniGain}(a_i, S) = \\text{Gini}(y, S) - \\sum_{\\bar{\\sigma}_{a_i=v_{i,j}}(S)} \\frac{|\\bar{\\sigma}_{a_i=v_{i,j}}(S)| \\cdot \\text{Gini}(y, \\bar{\\sigma}_{a_i=v_{i,j}}(S))}{|S|}\n$$\n\n<p>Where:</p>\n\n<ul>\n  <li>$\\text{GiniGain}(a_i, S)$: Gini gain for attribute $a_i$ and subset $S$.</li>\n  <li>$v_{i,j}$: Values of attribute $a_i$.</li>\n</ul>\n","metadata":{}},{"cell_type":"code","source":"class DecisionTreeClassifierwithGini:\n    def __init__(self , discrete):\n        self.root = Node()\n        self.discrete = discrete\n    @staticmethod\n    def calcGiniImpurity(Y):\n        total_samples = len(Y)\n        if total_samples == 0:\n            return 0.0\n\n        label_counts = {}\n        for label in Y:\n            if label in label_counts:\n                label_counts[label] += 1\n            else:\n                label_counts[label] = 1\n\n        gini_impurity = 1.0\n        for label in label_counts:\n            probability = label_counts[label] / total_samples\n            gini_impurity -= probability ** 2\n\n        return gini_impurity\n    \n    \n    def split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        feature_values = np.sort(feature_values)\n        total_instances = len(data)\n        best_split_point = None\n        mini_gini_impurity = float('inf')\n        split_nodes = {}\n        weighted_gini_impurity = 0\n\n        for i in range(1, len(feature_values)):\n            # Compute split point\n            split_point = (feature_values[i] + feature_values[i-1])/2\n\n            # Split data based on the split point\n            \n            left_data = data[data[:, feat_index] <= split_point]\n            right_data = data[data[:, feat_index] > split_point]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n\n            # Calculate information gain\n            left_gini_impurity = self.calcGiniImpurity(left_data[:, -1])\n            right_gini_impurity = self.calcGiniImpurity(right_data[:, -1])\n    \n        \n            impurity = (len(left_data) / total_instances * left_gini_impurity) + (len(right_data) / total_instances * right_gini_impurity)\n\n            # Update best split point if information gain is higher\n            if impurity < mini_gini_impurity:\n                mini_gini_impurity = impurity\n                weighted_gini_impurity = impurity\n                best_split_point = split_point\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        return split_nodes, weighted_gini_impurity, best_split_point\n\n    def split_on_feature(self , data , feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n        split_nodes = {}\n        total_instances = len(data)\n        best_split_point = None\n        mini_gini_impurity = float('inf')\n        weighted_gini_impurity = 0\n        \n        for unique_value in unique_values:\n            left_data = data[data[:, feat_index] == unique_value, :]\n            right_data = data[data[:, feat_index] != unique_value, :]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n            left_gini_impurity = self.calcGiniImpurity(left_data[:, -1])\n            right_gini_impurity = self.calcGiniImpurity(right_data[:, -1])\n    \n        \n            impurity = (len(left_data) / total_instances * left_gini_impurity) + (len(right_data) / total_instances * right_gini_impurity)\n\n            # Update best split point if information gain is higher\n            if impurity < mini_gini_impurity:\n                mini_gini_impurity = impurity\n                weighted_gini_impurity = impurity\n                best_split_point = unique_value\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        return split_nodes, weighted_gini_impurity, best_split_point\n        \n        \n    def best_split(self , node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n        y = self.get_y(node.data)\n        totalGiniImpurity = self.calcGiniImpurity(y)\n        index_feature_split = -1\n        #take with max ginigain\n        max_ginigain = 0\n        threshold = -1\n        \n        for i in range(node.data.shape[1] - 1):\n            if discrete[i] is True:\n                split_nodes, weighted_gini_impurity, split_val = self.split_on_feature(node.data , i)\n                if totalGiniImpurity - weighted_gini_impurity > max_ginigain:\n                    child_nodes , max_ginigain = split_nodes, totalGiniImpurity - weighted_gini_impurity\n                    index_feature_split = i\n                    threshold = split_val\n            else:\n                split_nodes, weighted_gini_impurity, split_val = self.split_on_contfeature(node.data , i)\n                if totalGiniImpurity - weighted_gini_impurity > max_ginigain:\n                    child_nodes , max_ginigain = split_nodes, totalGiniImpurity - weighted_gini_impurity\n                    index_feature_split = i\n                    threshold = split_val\n                    \n        node.children = child_nodes\n        node.split_on = index_feature_split\n        node.threshold = threshold\n\n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        if len(node.data) < 100:\n            return True\n        y = self.get_y(node.data)\n        return True if self.calcGiniImpurity(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n        for i in range(len(X)):  # Corrected range syntax\n            prediction = self.traverse_tree(X[i], self.root)\n            predictions[i] = prediction  # Insert prediction into the numpy array\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        if discrete[node.split_on] is True:\n            if feat_value == node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[0])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[1])\n        else:\n            if feat_value > node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[1])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[0])\n        return predicted_class    ","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:25:48.105888Z","iopub.execute_input":"2024-04-18T07:25:48.106424Z","iopub.status.idle":"2024-04-18T07:25:48.147086Z","shell.execute_reply.started":"2024-04-18T07:25:48.106385Z","shell.execute_reply":"2024-04-18T07:25:48.145444Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifierwithGini(discrete)\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:25:49.760587Z","iopub.execute_input":"2024-04-18T07:25:49.761027Z","iopub.status.idle":"2024-04-18T07:30:42.748778Z","shell.execute_reply.started":"2024-04-18T07:25:49.760990Z","shell.execute_reply":"2024-04-18T07:30:42.747161Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Trying Precision recall and accuracy","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## This is the result using single Decision Tree with Gini Index as Impurity","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(X_test.to_numpy())\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:30:42.751699Z","iopub.execute_input":"2024-04-18T07:30:42.753159Z","iopub.status.idle":"2024-04-18T07:30:42.775595Z","shell.execute_reply.started":"2024-04-18T07:30:42.753107Z","shell.execute_reply":"2024-04-18T07:30:42.774173Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"0.929\n","output_type":"stream"}]},{"cell_type":"code","source":"true_positives = sum((y_test == 1) & (y_pred == 1))\nfalse_positives = sum((y_test == 0) & (y_pred == 1))\nfalse_negatives = sum((y_test == 1) & (y_pred == 0))\ntrue_negatives = sum((y_test == 0) & (y_pred == 0))\n\nprecision = true_positives / (true_positives + false_positives)\nrecall = true_positives / (true_positives + false_negatives)\nf1_score = (2*precision*recall)/(precision + recall)\naccuracy = (true_positives + true_negatives)/(true_positives + true_negatives + false_positives + false_negatives)\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"f1_score:\", f1_score)\nprint(\"accuracy:\", accuracy)\n\n# Construct confusion matrix\nconf_matrix = np.array([[true_negatives, false_positives],\n                        [false_negatives, true_positives]])\n\nimport matplotlib.pyplot as plt\nimport itertools\n\n# Plotting the confusion matrix\nplt.figure(figsize=(4, 4))\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\n\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['Pred Negative', 'Pred Positive'])\nplt.yticks(tick_marks, ['Actual Negative', 'Actual Positive'])\n\nthresh = conf_matrix.max() / 2.\nfor i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n    plt.text(j, i, conf_matrix[i, j], horizontalalignment=\"center\", color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:33:42.301774Z","iopub.execute_input":"2024-04-18T07:33:42.302261Z","iopub.status.idle":"2024-04-18T07:33:42.697169Z","shell.execute_reply.started":"2024-04-18T07:33:42.302221Z","shell.execute_reply":"2024-04-18T07:33:42.695783Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Precision: 0.92\nRecall: 0.9368635437881874\nf1_score: 0.9283551967709384\naccuracy: 0.929\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 400x400 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# ***Making Bagging Classifier***","metadata":{}},{"cell_type":"markdown","source":"![](https://media.geeksforgeeks.org/wp-content/uploads/20230731175958/Bagging-classifier.png)","metadata":{}},{"cell_type":"markdown","source":"# What is Bagging? What is Ensemble Learning?\n> Ensemble learning gives credence to the idea of the “wisdom of crowds,” which suggests that the decision-making of a larger group of people is typically better than that of an individual expert. Similarly, ensemble learning refers to a group (or ensemble) of base learners, or models, which work collectively to achieve a better final prediction.\n> A single model, also known as a base or weak learner, may not perform well individually due to high variance or high bias. However, when weak learners are aggregated, they can form a strong learner, as their combination reduces bias or variance, yielding better model performance.\n\n\n\n> Bagging, also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance within a noisy data set.\n> In bagging, a random sample of data in a training set is selected with replacement—meaning that the individual data points can be chosen more than once. After generating several data samples, these weak models are then trained independently. Depending on the type of task—regression or classification, for example—the average or majority of those predictions yield a more accurate estimate. ","metadata":{}},{"cell_type":"markdown","source":"## How does Bagging works?\n> ***Bootstrapping***:  Bagging leverages a bootstrapping sampling technique to create diverse samples. This resampling method generates different subsets of the training data set. It does so by selecting data points at random and with replacement. This means that each time you select a data point from the training data set, you are able to select the same instance multiple times. As a result, a value or instance repeated twice (or more) in a sample.\n\n> ***Parallel training***: These bootstrap samples are then trained independently and in parallel with each other using weak or base learners.\n\n> ***Aggregation***: Finally, depending on the task (that is, regression or classification), an average or a majority of the predictions are taken to compute a more accurate estimate. In the case of regression, an average is taken of all the outputs predicted by the individual classifiers; this is known as soft voting. For classification problems, the class with the highest majority of votes is accepted; this is known as hard voting or majority voting.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\n\nclass BaggingClassifier:\n    def __init__(self, no_of_estimators, max_features, max_samples):\n        self.max_features = max_features\n        self.max_samples = max_samples\n        self.no_of_estimators = no_of_estimators\n        \n    @staticmethod\n    def sample_rows(df, percent):\n        return df.sample(int(percent*df.shape[0]),replace=True)\n    \n    @staticmethod\n    def sample_features(df,percent,discrete):\n        cols = random.sample(df.columns.tolist()[:-1],int(percent*(df.shape[1]-1)))\n        col_indices = [df.columns.tolist().index(col) for col in cols]\n        new_discrete = [discrete[i] for i in col_indices] \n        new_df = df[cols].copy()\n        new_df['target'] = df[23]\n        return new_df , new_discrete\n    \n    def fit(self, X, y, discrete):\n        data = np.column_stack([X, y])\n        data = pd.DataFrame(data)\n        self.estimators_ = []\n        for _ in range(self.no_of_estimators):\n            sample_data = self.sample_rows(data, self.max_samples)\n            nw_data ,new_discrete = self.sample_features(sample_data, self.max_features, discrete)\n            model = DecisionTreeClassifierwithGini(discrete)\n            model.fit(sample_data.iloc[:, :-1], sample_data.iloc[:, -1])\n            print(\"Model trained\")\n            self.estimators_.append(model)\n    \n    def predict(self, X):\n        if not hasattr(self, 'estimators_'):\n            raise AttributeError(\"Model has not been trained yet. Call 'fit' with appropriate data first.\")\n        print(len(self.estimators_))\n        predictions = np.array([estimator.predict(X) for estimator in self.estimators_])\n        predictions = predictions.astype(int)\n        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:35:06.762766Z","iopub.execute_input":"2024-04-18T07:35:06.763327Z","iopub.status.idle":"2024-04-18T07:35:06.782851Z","shell.execute_reply.started":"2024-04-18T07:35:06.763289Z","shell.execute_reply":"2024-04-18T07:35:06.781218Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = BaggingClassifier(50,0.5,0.2)\nmodel.fit(X_train,y_train,discrete)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:36:07.805889Z","iopub.execute_input":"2024-04-18T07:36:07.806406Z","iopub.status.idle":"2024-04-18T07:44:46.789890Z","shell.execute_reply.started":"2024-04-18T07:36:07.806366Z","shell.execute_reply":"2024-04-18T07:44:46.787996Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Model trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\nModel trained\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = model.predict(X_test.to_numpy())\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:44:46.793269Z","iopub.execute_input":"2024-04-18T07:44:46.793830Z","iopub.status.idle":"2024-04-18T07:44:47.073596Z","shell.execute_reply.started":"2024-04-18T07:44:46.793782Z","shell.execute_reply":"2024-04-18T07:44:47.072390Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"50\n0.907\n","output_type":"stream"}]},{"cell_type":"code","source":"true_positives = sum((y_test == 1) & (y_pred == 1))\nfalse_positives = sum((y_test == 0) & (y_pred == 1))\nfalse_negatives = sum((y_test == 1) & (y_pred == 0))\ntrue_negatives = sum((y_test == 0) & (y_pred == 0))\n\nprecision = true_positives / (true_positives + false_positives)\nrecall = true_positives / (true_positives + false_negatives)\nf1_score = (2*precision*recall)/(precision + recall)\naccuracy = (true_positives + true_negatives)/(true_positives + true_negatives + false_positives + false_negatives)\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"f1_score:\", f1_score)\nprint(\"accuracy:\", accuracy)\n\n# Construct confusion matrix\nconf_matrix = np.array([[true_negatives, false_positives],\n                        [false_negatives, true_positives]])\n\nimport matplotlib.pyplot as plt\nimport itertools\n\n# Plotting the confusion matrix\nplt.figure(figsize=(4, 4))\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\n\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['Pred Negative', 'Pred Positive'])\nplt.yticks(tick_marks, ['Actual Negative', 'Actual Positive'])\n\nthresh = conf_matrix.max() / 2.\nfor i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n    plt.text(j, i, conf_matrix[i, j], horizontalalignment=\"center\", color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:44:47.075155Z","iopub.execute_input":"2024-04-18T07:44:47.076536Z","iopub.status.idle":"2024-04-18T07:44:47.429154Z","shell.execute_reply.started":"2024-04-18T07:44:47.076430Z","shell.execute_reply":"2024-04-18T07:44:47.428063Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Precision: 0.9383259911894273\nRecall: 0.8676171079429735\nf1_score: 0.9015873015873016\naccuracy: 0.907\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 400x400 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# ***Random Forest***","metadata":{}},{"cell_type":"markdown","source":"## Random Forest  Vs  Bagging Classifier\n\n<big>The main difference between bagging and random forest lies in the way they introduce randomness in the dataset. Bagging introduces randomness by sampling with replacement, while random forest introduces randomness by using a subset of features for each tree. Bagging is a simple and effective technique that can improve the accuracy of a model by reducing the variance. However, bagging does not address the problem of highly correlated trees, which can reduce the diversity of the model. Random forest, on the other hand, addresses the problem of correlated trees by using a subset of features at each node. This results in a diverse set of trees that can improve the accuracy of a model by reducing overfitting and increasing the diversity of the model. Another difference between bagging and random forests is the number of trees used in the model. Bagging can use any number of trees, while random forest typically uses a large number of trees (hundreds or thousands) to achieve better accuracy. This is because random forest requires more trees to achieve the same level of accuracy as bagging, due to the feature selection process.</big>","metadata":{}},{"cell_type":"markdown","source":"## Why Random Forest?\n<big>Random forest is a flexible, easy-to-use machine learning algorithm that produces, even without hyper-parameter tuning, a great result most of the time. It is also one of the most-used algorithms, due to its simplicity and diversity (it can be used for both classification and regression tasks).It reduces overfitting problem in decision trees and helps to improve the accuracy. It reduces prediction variance compared to single decision tree. Trees can be created in parallel, since there is no dependence between iterations, which speeds up the training time</big>","metadata":{}},{"cell_type":"code","source":"class Node:\n    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False , threshold=None):\n\n        self.data = data\n        self.children = children\n        self.split_on = split_on\n        self.threshold = threshold #Used when splitting using discrete features\n        self.pred_class = pred_class\n        self.is_leaf = is_leaf","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:47:56.088210Z","iopub.execute_input":"2024-04-18T07:47:56.088802Z","iopub.status.idle":"2024-04-18T07:47:56.097366Z","shell.execute_reply.started":"2024-04-18T07:47:56.088755Z","shell.execute_reply":"2024-04-18T07:47:56.095752Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class DecisionTreeClassifierwithGiniforRF:\n    def __init__(self , discrete, max_samples, max_features):\n        self.root = Node()\n        self.discrete = discrete\n        self.max_samples = max_samples\n        self.max_features = max_features\n    @staticmethod\n    def calcGiniImpurity(Y):\n        total_samples = len(Y)\n        if total_samples == 0:\n            return 0.0\n\n        label_counts = {}\n        for label in Y:\n            if label in label_counts:\n                label_counts[label] += 1\n            else:\n                label_counts[label] = 1\n\n        gini_impurity = 1.0\n        for label in label_counts:\n            probability = label_counts[label] / total_samples\n            gini_impurity -= probability ** 2\n\n        return gini_impurity\n    \n    def split_on_contfeature(self, data, feat_index):\n        feature_values = data[:, feat_index]\n        feature_values = np.sort(feature_values)\n        total_instances = len(data)\n        best_split_point = None\n        mini_gini_impurity = float('inf')\n        split_nodes = {}\n        weighted_gini_impurity = 0\n\n        for i in range(1, len(feature_values)):\n            # Compute split point\n            split_point = (feature_values[i] + feature_values[i-1])/2\n\n            # Split data based on the split point\n            \n            left_data = data[data[:, feat_index] <= split_point]\n            right_data = data[data[:, feat_index] > split_point]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n\n            # Calculate information gain\n            left_gini_impurity = self.calcGiniImpurity(left_data[:, -1])\n            right_gini_impurity = self.calcGiniImpurity(right_data[:, -1])\n    \n        \n            impurity = (len(left_data) / total_instances * left_gini_impurity) + (len(right_data) / total_instances * right_gini_impurity)\n\n            # Update best split point if information gain is higher\n            if impurity < mini_gini_impurity:\n                mini_gini_impurity = impurity\n                weighted_gini_impurity = impurity\n                best_split_point = split_point\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        return split_nodes, weighted_gini_impurity, best_split_point\n\n    def split_on_feature(self , data , feat_index):\n        feature_values = data[:, feat_index]\n        unique_values = np.unique(feature_values)\n        split_nodes = {}\n        total_instances = len(data)\n        best_split_point = None\n        mini_gini_impurity = float('inf')\n        weighted_gini_impurity = 0\n        \n        for unique_value in unique_values:\n            left_data = data[data[:, feat_index] == unique_value, :]\n            right_data = data[data[:, feat_index] != unique_value, :]\n            left_node = Node(data = left_data)\n            right_node = Node(data = right_data)\n            left_gini_impurity = self.calcGiniImpurity(left_data[:, -1])\n            right_gini_impurity = self.calcGiniImpurity(right_data[:, -1])\n    \n        \n            impurity = (len(left_data) / total_instances * left_gini_impurity) + (len(right_data) / total_instances * right_gini_impurity)\n\n            # Update best split point if information gain is higher\n            if impurity < mini_gini_impurity:\n                mini_gini_impurity = impurity\n                weighted_gini_impurity = impurity\n                best_split_point = unique_value\n                split_nodes[0] = left_node\n                split_nodes[1] = right_node\n        \n        return split_nodes, weighted_gini_impurity, best_split_point\n        \n    \n    def sampling(df):\n        percent = self.max_samples\n        data = df.sample(int(percent*df.shape[0]),replace=True)\n        return data\n    \n    def best_split(self , node):\n        if self.meet_criteria(node):\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n        y = self.get_y(node.data)\n        totalGiniImpurity = self.calcGiniImpurity(y)\n        index_feature_split = -1\n        child_nodes = []\n        #take with max ginigain\n        max_ginigain = 0\n        threshold = -1\n        #\n            #HERE YOU HAVE TO CHANGE DATA TRYING TO DO SAMPLING HERE sampling according to your req.\n        new_data = node.data\n        #now column sampling remains\n        \n        # Column sampling for discrete property\n        total_features = new_data.shape[1] - 1  # Exclude the target column\n        num_features_to_sample = int(self.max_features * total_features)\n\n        # Column sampling for discrete property\n        sampled_columns = np.random.choice(range(total_features), num_features_to_sample, replace=False)\n        #\n        for i in sampled_columns:\n            if discrete[i] is True:\n                split_nodes, weighted_gini_impurity, split_val = self.split_on_feature(new_data , i)\n                if totalGiniImpurity - weighted_gini_impurity > max_ginigain:\n                    child_nodes , max_ginigain = split_nodes, totalGiniImpurity - weighted_gini_impurity\n                    index_feature_split = i\n                    threshold = split_val\n            else:\n                split_nodes, weighted_gini_impurity, split_val = self.split_on_contfeature(new_data , i)\n                if totalGiniImpurity - weighted_gini_impurity > max_ginigain:\n                    child_nodes , max_ginigain = split_nodes, totalGiniImpurity - weighted_gini_impurity\n                    index_feature_split = i\n                    threshold = split_val\n        \n        if len(child_nodes) == 0:\n            node.is_leaf = True\n            y = self.get_y(node.data)\n            node.pred_class = self.get_pred_class(y)\n            return\n        \n        node.children = child_nodes\n        node.split_on = index_feature_split\n        node.threshold = threshold\n\n        for child_node in child_nodes.values():\n            self.best_split(child_node)\n    def meet_criteria(self, node):\n        if len(node.data) < 100:\n            return True\n        y = self.get_y(node.data)\n        return True if self.calcGiniImpurity(y) == 0 else False\n    @staticmethod\n    def get_y(data):\n        y = data[:, -1]\n        return y\n    @staticmethod\n    def get_pred_class(Y):\n        labels, labels_counts = np.unique(Y, return_counts=True)\n        index = np.argmax(labels_counts)\n        return labels[index]\n    def fit(self, X, Y):\n        data = np.column_stack([X, Y])\n        print(data)\n        self.root.data = data\n        self.best_split(self.root)\n    def predict(self, X):\n        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n        for i in range(len(X)):  # Corrected range syntax\n            prediction = self.traverse_tree(X[i], self.root)\n            predictions[i] = prediction  # Insert prediction into the numpy array\n        return predictions\n    def traverse_tree(self, x, node):\n        if node.is_leaf:\n            return node.pred_class\n        feat_value = x[node.split_on]\n        if discrete[node.split_on] is True:\n            if feat_value == node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[0])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[1])\n        else:\n            if feat_value > node.threshold:\n                predicted_class = self.traverse_tree(x, node.children[1])\n            else:\n                predicted_class = self.traverse_tree(x, node.children[0])\n        return predicted_class    ","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:47:56.623665Z","iopub.execute_input":"2024-04-18T07:47:56.624089Z","iopub.status.idle":"2024-04-18T07:47:56.666911Z","shell.execute_reply.started":"2024-04-18T07:47:56.624057Z","shell.execute_reply":"2024-04-18T07:47:56.665426Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Taking many Decision Trees with RF. and forming Random forest","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\n\nclass RandomForest:\n    def __init__(self, no_of_estimators, max_samples, max_features):\n        self.max_features = max_features\n        self.max_samples = max_samples\n        self.no_of_estimators = no_of_estimators\n    @staticmethod\n    def sample_rows(df, percent):\n        return df.sample(int(percent*df.shape[0]),replace=True)\n    \n    def fit(self, X, y, discrete):\n        data = np.column_stack([X, y])\n        data = pd.DataFrame(data)\n        self.estimators_ = []\n        for _ in range(self.no_of_estimators):\n            sample_data = self.sample_rows(data, self.max_samples)\n            model = DecisionTreeClassifierwithGiniforRF(discrete,self.max_samples,self.max_features)\n            model.fit(sample_data.iloc[:, :-1], sample_data.iloc[:, -1])\n            self.estimators_.append(model)\n            print(\"model trained\")\n    \n    def predict(self, X):\n        if not hasattr(self, 'estimators_'):\n            raise AttributeError(\"Model has not been trained yet. Call 'fit' with appropriate data first.\")\n        print(len(self.estimators_))\n        predictions = np.array([estimator.predict(X) for estimator in self.estimators_])\n        predictions = predictions.astype(int)\n        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:47:59.443238Z","iopub.execute_input":"2024-04-18T07:47:59.443709Z","iopub.status.idle":"2024-04-18T07:47:59.457349Z","shell.execute_reply.started":"2024-04-18T07:47:59.443676Z","shell.execute_reply":"2024-04-18T07:47:59.455849Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model = RandomForest(100,1,0.2)\nmodel.fit(X_train, y_train , discrete)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T07:48:28.847441Z","iopub.execute_input":"2024-04-18T07:48:28.847933Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[[  0.           0.           0.         ...  -1.          -1.\n    0.        ]\n [  0.          64.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n ...\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  0.          75.           0.         ...  -1.          -1.\n    1.        ]\n [  0.          61.53846154   0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[  0. 100.   0. ...  -1.  -1.   0.]\n [  0. 100.   0. ...  -1.  -1.   1.]\n [  0.  80.   0. ...  -1.  -1.   1.]\n ...\n [  0.   0.   0. ...   1.  -1.   1.]\n [  0. 100.   0. ...  -1.  -1.   1.]\n [  0.  20.   0. ...   1.  -1.   0.]]\nmodel trained\n[[  0.          86.36363636   0.         ...  -1.          -1.\n    0.        ]\n [  0.          97.14285714   0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...   1.           1.\n    1.        ]\n ...\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.          94.59459459   0.         ...  -1.           1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  0.          91.66666667   0.         ...  -1.          -1.\n    0.        ]\n [  0.          50.           0.         ...  -1.          -1.\n    1.        ]\n ...\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]]\nmodel trained\n[[  0.          50.           0.         ...  -1.          -1.\n    0.        ]\n [  0.          94.44444444   0.         ...  -1.          -1.\n    1.        ]\n [  1.          44.06779661   0.         ...  -1.           1.\n    1.        ]\n ...\n [  0.          71.42857143   0.         ...  -1.          -1.\n    1.        ]\n [  0.         100.           0.         ...  -1.           1.\n    1.        ]\n [  1.          84.61538462   0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[  0.          72.22222222   0.         ...  -1.          -1.\n    1.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    0.        ]\n ...\n [  0.           0.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  1.          66.66666667   0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n ...\n [  0.          53.84615385   0.         ...  -1.          -1.\n    0.        ]\n [  0.          66.66666667   0.         ...  -1.          -1.\n    0.        ]\n [  0.          67.39130435   0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[  0.           0.           0.         ...  -1.           1.\n    1.        ]\n [  0.          55.55555556   0.         ...  -1.           1.\n    1.        ]\n [  0.          91.66666667   0.         ...  -1.          -1.\n    0.        ]\n ...\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  0.          65.2173913    0.         ...  -1.          -1.\n    0.        ]\n [  0.          78.72340426   0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[  0.          53.84615385   0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...   1.          -1.\n    1.        ]\n [  0.          88.88888889   0.         ...  -1.          -1.\n    0.        ]\n ...\n [  0.           0.           0.         ...   1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]]\nmodel trained\n[[  0.          33.33333333   0.         ...  -1.          -1.\n    0.        ]\n [  0.          66.66666667   0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...   1.           1.\n    1.        ]\n ...\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[ 0.          7.69230769  0.         ...  1.          1.\n   1.        ]\n [ 0.         12.5         0.         ... -1.         -1.\n   0.        ]\n [ 0.         56.60377358  0.         ... -1.         -1.\n   0.        ]\n ...\n [ 0.         21.73913043  0.         ... -1.         -1.\n   0.        ]\n [ 0.         40.          0.         ...  1.         -1.\n   0.        ]\n [ 0.         75.          0.         ... -1.          1.\n   0.        ]]\nmodel trained\n[[ 0.          0.          0.         ... -1.         -1.\n   0.        ]\n [ 0.         50.          0.         ... -1.         -1.\n   1.        ]\n [ 0.         50.          0.         ... -1.         -1.\n   1.        ]\n ...\n [ 0.         87.5         0.         ... -1.         -1.\n   1.        ]\n [ 0.         33.33333333  0.         ... -1.         -1.\n   1.        ]\n [ 0.          0.          0.         ... -1.         -1.\n   0.        ]]\nmodel trained\n[[  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    0.        ]\n [  0.          33.33333333   0.         ...  -1.          -1.\n    1.        ]\n ...\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  0.          87.5          0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...   1.          -1.\n    1.        ]]\nmodel trained\n[[  0.          80.           0.         ...  -1.          -1.\n    1.        ]\n [  0.          47.22222222   0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n ...\n [  0.           0.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.           1.\n    0.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[  0.   0.   0. ...   1.  -1.   1.]\n [  0.  95.   0. ...  -1.  -1.   0.]\n [  0.   0.   0. ...  -1.  -1.   0.]\n ...\n [  0. 100.   0. ...  -1.  -1.   1.]\n [  0.   0.   0. ...  -1.  -1.   1.]\n [  0.   0.   0. ...  -1.  -1.   1.]]\nmodel trained\n[[  0.          57.14285714   0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  0.          75.           0.         ...  -1.          -1.\n    0.        ]\n ...\n [  1.          62.5          0.         ...  -1.          -1.\n    1.        ]\n [  0.         100.           0.         ...   1.          -1.\n    0.        ]\n [  0.          80.           0.         ...  -1.          -1.\n    1.        ]]\nmodel trained\n[[  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.          66.66666667   0.         ...   1.           1.\n    0.        ]\n [  0.           8.33333333   0.         ...  -1.          -1.\n    1.        ]\n ...\n [  0.          75.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[  0.          22.22222222   0.         ...  -1.          -1.\n    0.        ]\n [  0.          85.96491228   0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n ...\n [  0.          50.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  1.          27.5          0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[ 0.         40.          0.         ... -1.         -1.\n   0.        ]\n [ 0.          0.          0.         ...  1.          1.\n   1.        ]\n [ 0.         46.26865672  0.         ... -1.         -1.\n   0.        ]\n ...\n [ 0.         44.44444444  0.         ... -1.         -1.\n   0.        ]\n [ 0.          7.69230769  0.         ...  1.          1.\n   1.        ]\n [ 0.          9.09090909  0.         ... -1.          1.\n   1.        ]]\nmodel trained\n[[ 0.          0.          0.         ... -1.          1.\n   1.        ]\n [ 0.         90.          0.         ... -1.         -1.\n   1.        ]\n [ 0.         93.47826087  0.         ... -1.         -1.\n   0.        ]\n ...\n [ 0.         30.          0.         ... -1.         -1.\n   0.        ]\n [ 0.          0.          0.         ... -1.         -1.\n   1.        ]\n [ 0.         15.78947368  0.         ... -1.         -1.\n   1.        ]]\nmodel trained\n[[  0.          28.57142857   0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...   1.          -1.\n    1.        ]\n [  0.           0.           0.         ...   1.          -1.\n    1.        ]\n ...\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  0.          66.66666667   0.         ...  -1.           1.\n    1.        ]\n [  0.          28.57142857   0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[ 0.         50.          0.         ... -1.         -1.\n   0.        ]\n [ 0.         95.23809524  0.         ... -1.         -1.\n   0.        ]\n [ 0.         75.          0.         ... -1.         -1.\n   0.        ]\n ...\n [ 0.         72.          0.         ...  1.          1.\n   1.        ]\n [ 0.          0.          0.         ... -1.         -1.\n   1.        ]\n [ 0.         91.66666667  0.         ... -1.         -1.\n   0.        ]]\nmodel trained\n[[  0.          92.30769231   0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...   1.           1.\n    1.        ]\n [  0.         100.           0.         ...  -1.           1.\n    1.        ]\n ...\n [  0.          12.5          0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]]\nmodel trained\n[[  0.          40.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  1.          66.66666667   0.         ...  -1.          -1.\n    1.        ]\n ...\n [  1.          13.33333333   0.         ...   1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...   1.          -1.\n    1.        ]]\nmodel trained\n[[  0.           0.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n ...\n [  0.           7.69230769   0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.           1.\n    1.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[  0.         100.           0.         ...  -1.           1.\n    1.        ]\n [  0.          38.88888889   0.         ...  -1.          -1.\n    1.        ]\n [  1.          18.18181818   0.         ...  -1.          -1.\n    1.        ]\n ...\n [  0.          83.33333333   0.         ...   1.          -1.\n    0.        ]\n [  0.          50.           0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]]\nmodel trained\n[[  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  0.           1.96078431   0.         ...  -1.          -1.\n    1.        ]\n [  0.         100.           0.         ...  -1.           1.\n    1.        ]\n ...\n [  0.          50.           0.         ...  -1.           1.\n    1.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    1.        ]\n [  0.          12.90322581   0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[ 0.          0.          0.         ...  1.         -1.\n   1.        ]\n [ 0.          0.          0.         ... -1.         -1.\n   0.        ]\n [ 0.         47.36842105  0.         ... -1.          1.\n   0.        ]\n ...\n [ 0.          0.          0.         ... -1.          1.\n   1.        ]\n [ 0.          0.          0.         ...  1.          1.\n   1.        ]\n [ 0.         66.66666667  0.         ...  1.         -1.\n   0.        ]]\nmodel trained\n[[ 0.         22.72727273  0.         ... -1.         -1.\n   0.        ]\n [ 0.         56.          0.         ... -1.         -1.\n   0.        ]\n [ 0.         60.          0.         ... -1.         -1.\n   0.        ]\n ...\n [ 0.          7.69230769  0.         ...  1.          1.\n   1.        ]\n [ 0.         62.74509804  0.         ... -1.         -1.\n   0.        ]\n [ 0.          0.          0.         ...  1.          1.\n   1.        ]]\nmodel trained\n[[  0.          33.33333333   0.         ...  -1.          -1.\n    1.        ]\n [  0.          94.73684211   0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...   1.          -1.\n    1.        ]\n ...\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]]\nmodel trained\n[[  0.   87.5   0.  ...  -1.   -1.    0. ]\n [  0.   50.    0.  ...  -1.   -1.    1. ]\n [  0.   50.    0.  ...  -1.   -1.    0. ]\n ...\n [  0.    0.    0.  ...  -1.   -1.    0. ]\n [  0.  100.    0.  ...  -1.   -1.    1. ]\n [  0.    0.    0.  ...  -1.   -1.    0. ]]\nmodel trained\n[[  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n ...\n [  0.          61.53846154   0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]]\nmodel trained\n[[  0.         100.           0.         ...  -1.          -1.\n    0.        ]\n [  0.          57.14285714   0.         ...  -1.          -1.\n    0.        ]\n [  0.          97.5          0.         ...  -1.          -1.\n    0.        ]\n ...\n [  0.          92.30769231   0.         ...  -1.           1.\n    0.        ]\n [  0.          66.66666667   0.         ...  -1.          -1.\n    1.        ]\n [  0.           0.           0.         ...   1.           1.\n    1.        ]]\nmodel trained\n[[  1.          63.63636364   0.         ...  -1.          -1.\n    0.        ]\n [  0.          42.85714286   0.         ...  -1.           1.\n    0.        ]\n [  0.           0.           0.         ...  -1.           1.\n    1.        ]\n ...\n [  0.          86.66666667   0.         ...  -1.          -1.\n    0.        ]\n [  0.         100.           0.         ...  -1.           1.\n    1.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]]\nmodel trained\n[[  0.           0.           0.         ...   1.          -1.\n    1.        ]\n [  0.           9.09090909   0.         ...  -1.          -1.\n    0.        ]\n [  0.           0.           0.         ...   1.          -1.\n    1.        ]\n ...\n [  0.         100.           0.         ...   1.          -1.\n    0.        ]\n [  0.           0.           0.         ...  -1.          -1.\n    1.        ]\n [  0.          81.81818182   0.         ...  -1.           1.\n    1.        ]]\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = model.predict(X_test.to_numpy())\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T09:35:35.108782Z","iopub.execute_input":"2024-04-18T09:35:35.109754Z","iopub.status.idle":"2024-04-18T09:35:36.051005Z","shell.execute_reply.started":"2024-04-18T09:35:35.109716Z","shell.execute_reply":"2024-04-18T09:35:36.049525Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"100\n0.939\n","output_type":"stream"}]},{"cell_type":"code","source":"true_positives = sum((y_test == 1) & (y_pred == 1))\nfalse_positives = sum((y_test == 0) & (y_pred == 1))\nfalse_negatives = sum((y_test == 1) & (y_pred == 0))\ntrue_negatives = sum((y_test == 0) & (y_pred == 0))\n\nprecision = true_positives / (true_positives + false_positives)\nrecall = true_positives / (true_positives + false_negatives)\nf1_score = (2*precision*recall)/(precision + recall)\naccuracy = (true_positives + true_negatives)/(true_positives + true_negatives + false_positives + false_negatives)\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"f1_score:\", f1_score)\nprint(\"accuracy:\", accuracy)\n\n# Construct confusion matrix\nconf_matrix = np.array([[true_negatives, false_positives],\n                        [false_negatives, true_positives]])\n\nimport matplotlib.pyplot as plt\nimport itertools\n\n# Plotting the confusion matrix\nplt.figure(figsize=(4, 4))\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\n\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['Pred Negative', 'Pred Positive'])\nplt.yticks(tick_marks, ['Actual Negative', 'Actual Positive'])\n\nthresh = conf_matrix.max() / 2.\nfor i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n    plt.text(j, i, conf_matrix[i, j], horizontalalignment=\"center\", color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T09:35:36.053062Z","iopub.execute_input":"2024-04-18T09:35:36.053435Z","iopub.status.idle":"2024-04-18T09:35:36.429726Z","shell.execute_reply.started":"2024-04-18T09:35:36.053402Z","shell.execute_reply":"2024-04-18T09:35:36.427740Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Precision: 0.9317269076305221\nRecall: 0.945010183299389\nf1_score: 0.9383215369059658\naccuracy: 0.939\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 400x400 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"### ","metadata":{}},{"cell_type":"markdown","source":"# Testing with Scikit Learn","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf_gini = DecisionTreeClassifier(criterion='gini', random_state=0)\nclf_gini.fit(X_train,y_train)\ny_pred_gini = clf_gini.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:57:28.818553Z","iopub.execute_input":"2024-03-22T13:57:28.818925Z","iopub.status.idle":"2024-03-22T13:57:28.902489Z","shell.execute_reply.started":"2024-03-22T13:57:28.818896Z","shell.execute_reply":"2024-03-22T13:57:28.901249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred_gini))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:57:29.258349Z","iopub.execute_input":"2024-03-22T13:57:29.258742Z","iopub.status.idle":"2024-03-22T13:57:29.266918Z","shell.execute_reply.started":"2024-03-22T13:57:29.258714Z","shell.execute_reply":"2024-03-22T13:57:29.265742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf_entropy = DecisionTreeClassifier(criterion='entropy', random_state=0)\nclf_entropy.fit(X_train,y_train)\ny_pred_entropy = clf_entropy.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:57:30.995526Z","iopub.execute_input":"2024-03-22T13:57:30.996457Z","iopub.status.idle":"2024-03-22T13:57:31.081098Z","shell.execute_reply.started":"2024-03-22T13:57:30.996412Z","shell.execute_reply":"2024-03-22T13:57:31.079997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.accuracy_score(y_test,y_pred_entropy))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:57:32.602650Z","iopub.execute_input":"2024-03-22T13:57:32.603283Z","iopub.status.idle":"2024-03-22T13:57:32.610869Z","shell.execute_reply.started":"2024-03-22T13:57:32.603251Z","shell.execute_reply":"2024-03-22T13:57:32.609485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing\n\nnum_cores = multiprocessing.cpu_count()\nprint(\"Number of CPU cores:\", num_cores)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T17:19:07.988364Z","iopub.execute_input":"2024-04-15T17:19:07.989124Z","iopub.status.idle":"2024-04-15T17:19:07.996184Z","shell.execute_reply.started":"2024-04-15T17:19:07.989082Z","shell.execute_reply":"2024-04-15T17:19:07.994830Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Number of CPU cores: 4\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}