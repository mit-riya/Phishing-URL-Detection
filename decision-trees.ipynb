{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd947765",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:52.484684Z",
     "iopub.status.busy": "2024-03-22T14:05:52.483950Z",
     "iopub.status.idle": "2024-03-22T14:05:53.356406Z",
     "shell.execute_reply": "2024-03-22T14:05:53.355224Z"
    },
    "papermill": {
     "duration": 0.895959,
     "end_time": "2024-03-22T14:05:53.359727",
     "exception": false,
     "start_time": "2024-03-22T14:05:52.463768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/image-1/Screenshot 2024-03-21 at 9.51.40PM.png\n",
      "/kaggle/input/phishingurl/Data_processed.csv\n",
      "/kaggle/input/image-2/Screenshot 2024-03-21 at 9.59.01PM.png\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ae5f4",
   "metadata": {
    "papermill": {
     "duration": 0.014458,
     "end_time": "2024-03-22T14:05:53.389961",
     "exception": false,
     "start_time": "2024-03-22T14:05:53.375503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **CREATING DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b1ac7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:53.421070Z",
     "iopub.status.busy": "2024-03-22T14:05:53.420489Z",
     "iopub.status.idle": "2024-03-22T14:05:53.556321Z",
     "shell.execute_reply": "2024-03-22T14:05:53.555163Z"
    },
    "papermill": {
     "duration": 0.154709,
     "end_time": "2024-03-22T14:05:53.559183",
     "exception": false,
     "start_time": "2024-03-22T14:05:53.404474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/phishingurl/Data_processed.csv')\n",
    "df['target'] = np.where(df['status'] == 'phishing', 1, 0)\n",
    "df.drop(columns=['url','status','lexical_features'], inplace=True)\n",
    "df.drop(columns=['submit_email','sfh',], inplace=True)\n",
    "y=df['target']\n",
    "X = df.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd07824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:53.591914Z",
     "iopub.status.busy": "2024-03-22T14:05:53.591489Z",
     "iopub.status.idle": "2024-03-22T14:05:53.596629Z",
     "shell.execute_reply": "2024-03-22T14:05:53.595875Z"
    },
    "papermill": {
     "duration": 0.02433,
     "end_time": "2024-03-22T14:05:53.598686",
     "exception": false,
     "start_time": "2024-03-22T14:05:53.574356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "discrete = [True, False, True, True, False, True, True, True, False, False, False, True, True, False, True, True, False, False, True, True, False, True, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ddd2e78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:53.630700Z",
     "iopub.status.busy": "2024-03-22T14:05:53.630115Z",
     "iopub.status.idle": "2024-03-22T14:05:54.923867Z",
     "shell.execute_reply": "2024-03-22T14:05:54.922683Z"
    },
    "papermill": {
     "duration": 1.312485,
     "end_time": "2024-03-22T14:05:54.926600",
     "exception": false,
     "start_time": "2024-03-22T14:05:53.614115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "713592da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:54.958068Z",
     "iopub.status.busy": "2024-03-22T14:05:54.957645Z",
     "iopub.status.idle": "2024-03-22T14:05:54.965899Z",
     "shell.execute_reply": "2024-03-22T14:05:54.964739Z"
    },
    "papermill": {
     "duration": 0.02678,
     "end_time": "2024-03-22T14:05:54.968304",
     "exception": false,
     "start_time": "2024-03-22T14:05:54.941524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['login_form', 'links_in_tags', 'iframe', 'popup_window', 'safe_anchor',\n",
       "       'onmouseover', 'right_clic', 'whois_registered_domain',\n",
       "       'domain_registration_length', 'domain_age', 'web_traffic', 'dns_record',\n",
       "       'google_index', 'page_rank', 'embedded_domain', 'having_ip_address',\n",
       "       'no_of_dots', 'no_of_sensitive_words', 'out_of_position_tld',\n",
       "       'https_token', 'url_length', 'tinyURL', 'prefixSuffix'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b77a8",
   "metadata": {
    "papermill": {
     "duration": 0.014642,
     "end_time": "2024-03-22T14:05:54.998165",
     "exception": false,
     "start_time": "2024-03-22T14:05:54.983523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ID3 ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d7236",
   "metadata": {
    "papermill": {
     "duration": 0.014991,
     "end_time": "2024-03-22T14:05:55.028727",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.013736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### **The above approach is the first algorithm developed for the decision trees.**\n",
    "* The major disadvantage is that it works only on categorical data\n",
    "* ID3 uses information gain as splitting criteria.\n",
    "* The growing stops when all instances belong to a single value of target feature or when    best information gain is not greater than zero\n",
    "* ID3 does not apply any pruning procedures nor does it handle numeric attributes or missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d65117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:55.061001Z",
     "iopub.status.busy": "2024-03-22T14:05:55.060308Z",
     "iopub.status.idle": "2024-03-22T14:05:55.066122Z",
     "shell.execute_reply": "2024-03-22T14:05:55.065167Z"
    },
    "papermill": {
     "duration": 0.024807,
     "end_time": "2024-03-22T14:05:55.068540",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.043733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False):\n",
    "\n",
    "        self.data = data\n",
    "        self.children = children\n",
    "        self.split_on = split_on\n",
    "        self.pred_class = pred_class\n",
    "        self.is_leaf = is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c658cb",
   "metadata": {
    "papermill": {
     "duration": 0.0147,
     "end_time": "2024-03-22T14:05:55.098401",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.083701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2>Information Gain and Entropy</h2>\n",
    "\n",
    "<p>Information gain is an impurity-based criterion used in decision trees to measure the reduction in uncertainty or impurity of a dataset after a particular feature is chosen for splitting.</p>\n",
    "\n",
    "<p>The formula for Information Gain ($\\text{IG}$) is:</p>\n",
    "\n",
    "$$\n",
    "\\text{IG}(a, S) = \\text{Entropy}(y, S) - \\sum_{\\bar{\\sigma}_{a=v_{i,j}}(S)} \\frac{|\\bar{\\sigma}_{a=v_{i,j}}(S)| \\cdot \\text{Entropy}(y, \\bar{\\sigma}_{a=v_{i,j}}(S))}{|S|}\n",
    "$$\n",
    "\n",
    "<p>Where:</p>\n",
    "\n",
    "<ul>\n",
    "  <li>$\\text{IG}(a, S)$: Information gain for attribute $a$ and subset $S$.</li>\n",
    "  <li>$\\text{Entropy}(y, S)$: Entropy of target attribute $y$ in subset $S$.</li>\n",
    "  <li>$v_{i,j}$: Values of attribute $a$.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Entropy ($\\text{Entropy}$) is a measure of impurity or uncertainty in a dataset. The formula for Entropy is:</p>\n",
    "\n",
    "$$\n",
    "\\text{Entropy}(y, S) = - \\sum_{c_j \\in \\text{dom}(y)} \\frac{|\\bar{\\sigma}_{y=c_j}(S)|}{|S|} \\cdot \\log_2 \\frac{|\\bar{\\sigma}_{y=c_j}(S)|}{|S|}\n",
    "$$\n",
    "\n",
    "<p>Where:</p>\n",
    "\n",
    "<ul>\n",
    "  <li>$\\text{dom}(y)$: Domain of target attribute $y$.</li>\n",
    "  <li>$\\bar{\\sigma}_{y=c_j}(S)$: Subset of $S$ where target attribute $y$ takes the value $c_j$.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a03c2a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:55.131736Z",
     "iopub.status.busy": "2024-03-22T14:05:55.131321Z",
     "iopub.status.idle": "2024-03-22T14:05:55.151558Z",
     "shell.execute_reply": "2024-03-22T14:05:55.150437Z"
    },
    "papermill": {
     "duration": 0.04081,
     "end_time": "2024-03-22T14:05:55.154291",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.113481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "    @staticmethod\n",
    "    def calculate_entropy(Y):\n",
    "        _, labels_counts = np.unique(Y, return_counts=True)\n",
    "        total_instances = len(Y)\n",
    "        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n",
    "        return entropy\n",
    "    def split_on_feature(self, data, feat_index):\n",
    "        feature_values = data[:, feat_index]\n",
    "        unique_values = np.unique(feature_values)\n",
    "\n",
    "        split_nodes = {}\n",
    "        weighted_entropy = 0\n",
    "        total_instances = len(data)\n",
    "\n",
    "        for unique_value in unique_values:\n",
    "            partition = data[data[:, feat_index] == unique_value, :]\n",
    "            node = Node(data=partition)\n",
    "            split_nodes[unique_value] = node\n",
    "            partition_y = self.get_y(partition)\n",
    "            node_entropy = self.calculate_entropy(partition_y)\n",
    "            weighted_entropy += (len(partition) / total_instances) * node_entropy\n",
    "\n",
    "        return split_nodes, weighted_entropy\n",
    "    def best_split(self, node):\n",
    "        if self.meet_criteria(node):\n",
    "            node.is_leaf = True\n",
    "            y = self.get_y(node.data)\n",
    "            node.pred_class = self.get_pred_class(y)\n",
    "            return\n",
    "\n",
    "        index_feature_split = -1\n",
    "        min_entropy = 1\n",
    "\n",
    "        for i in range(node.data.shape[1] - 1):\n",
    "            split_nodes, weighted_entropy = self.split_on_feature(node.data, i)\n",
    "            if weighted_entropy < min_entropy:\n",
    "                child_nodes, min_entropy = split_nodes, weighted_entropy\n",
    "                index_feature_split = i\n",
    "\n",
    "        node.children = child_nodes\n",
    "        node.split_on = index_feature_split\n",
    "        for child_node in child_nodes.values():\n",
    "            self.best_split(child_node)\n",
    "    def meet_criteria(self, node):\n",
    "        y = self.get_y(node.data)\n",
    "        return True if self.calculate_entropy(y) == 0 else False\n",
    "    @staticmethod\n",
    "    def get_y(data):\n",
    "        y = data[:, -1]\n",
    "        return y\n",
    "    @staticmethod\n",
    "    def get_pred_class(Y):\n",
    "        labels, labels_counts = np.unique(Y, return_counts=True)\n",
    "        index = np.argmax(labels_counts)\n",
    "        return labels[index]\n",
    "    def fit(self, X, Y):\n",
    "        data = np.column_stack([X, Y])\n",
    "        self.root.data = data\n",
    "        self.best_split(self.root)\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([self.traverse_tree(x, self.root) for index, x in X.iterrows()])\n",
    "        return predictions\n",
    "    def traverse_tree(self, x, node):\n",
    "        if node.is_leaf:\n",
    "            return node.pred_class\n",
    "        feat_value = x[node.split_on]\n",
    "        predicted_class = self.traverse_tree(x, node.children[feat_value])\n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d61910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T17:06:06.555634Z",
     "iopub.status.busy": "2024-03-21T17:06:06.555171Z",
     "iopub.status.idle": "2024-03-21T17:06:08.016770Z",
     "shell.execute_reply": "2024-03-21T17:06:08.015072Z",
     "shell.execute_reply.started": "2024-03-21T17:06:06.555600Z"
    },
    "papermill": {
     "duration": 0.015882,
     "end_time": "2024-03-22T14:05:55.185370",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.169488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b31a498b",
   "metadata": {
    "papermill": {
     "duration": 0.014503,
     "end_time": "2024-03-22T14:05:55.215599",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.201096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are some **advantages** in using ID3 including easy decomposability, strong intuitive nature etc. And some\n",
    "**disadvantages** include the following;\n",
    "<br>\n",
    "• Using information gain for feature selection, the algorithm tends to select attributes with more values, which is\n",
    "due to the fact that the value of the information gain of this kind of attribute will be bigger than others.\n",
    "<br>\n",
    "• In the decision tree building process, it is difficult to control the tree size. However, most researchers have tried\n",
    "to improve on this using various pruning methods to avoid the occurrence of over-fitting, which has led to the\n",
    "decision tree building process to be completed in two steps, that is modeling and pruning. Meanwhile, it will save\n",
    "a lot of time if a concise decision tree is built in onestep.\n",
    "<br>\n",
    "• There are several logarithmic calculations in the attribute selection process, this has made the computation of\n",
    "information gain time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d50e12",
   "metadata": {
    "papermill": {
     "duration": 0.01459,
     "end_time": "2024-03-22T14:05:55.245263",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.230673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### *This procedure will not work on our dataset. As some of the features in my datasets are continuos values. (not categorical)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c195f70",
   "metadata": {
    "papermill": {
     "duration": 0.014494,
     "end_time": "2024-03-22T14:05:55.274733",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.260239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# C4.5 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd5b253",
   "metadata": {
    "papermill": {
     "duration": 0.014288,
     "end_time": "2024-03-22T14:05:55.303881",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.289593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### **C4.5 was the next algorithm discovered in this feild**\n",
    "* It uses gain ratio as splitting criteria\n",
    "* The splitting ceases when the number of instances to be split is below a certain threshold\n",
    "* *C4.5 can handle numeric attributes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a12a6",
   "metadata": {
    "papermill": {
     "duration": 0.014437,
     "end_time": "2024-03-22T14:05:55.333612",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.319175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://storage.googleapis.com/kagglesdsdata/datasets/4644892/7907320/Screenshot%202024-03-21%20at%209.59.01PM.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240321%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240321T163522Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=94e0a88cde06144312f1496f726e8dbe75803b314d79d2cd7e0cbcb76d96ba377e88882dca68b6636a970dfceee0680e80ad0f6f66d38afcbca51d1b2cd7ca34947d21256d6acf9246bc90cefac628205f5105be1219e4a1f54568526d30012f6867719663b5ac3ad6a725179d2797c7e2aea056d3a62e173705262e98e571f97fba87fe3c7e0c92f3fea9025268c4fa66122ec83be45725b42b5219e4c028f732e6fe33ce8ecaaebd1a5c293982c92ae34670d68129a98963286f88bb3ae6df87da095955566299e693ff8bb5457397bf8ff57c71c66c3a7ece72c9816757e00377f790baf65590c315c6f56afbb450735e810b45199ae1631e8cbcfaac9e7e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff1df3",
   "metadata": {
    "papermill": {
     "duration": 0.014551,
     "end_time": "2024-03-22T14:05:55.363287",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.348736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://storage.googleapis.com/kagglesdsdata/datasets/4644838/7907242/Screenshot%202024-03-21%20at%209.51.40PM.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240321%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240321T164759Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=738ad3a70255123689ea37e6f8a326aae6960a0eddcddf490dd086ef39b59a7b4d3563cbf35fd152fadadf22d48d29f37d4ae35079c78d2f91217a8a73a07961e4278a64c33cfba8a4e2c70a5568c254f975dceba97bdd2452960b7dd6d28b3f40f7e60bcd57b3dc85e76c99a1a9a24f3e3490d15984062e7a42dd73c786896031a3ded13265c65cac6a66e4b930898e8268243d38465239bdfac9c9654d716207483e50c078e5af64d14a3e1f9df0a395f935d57333211a588f1a31e419f5bb9f72d7994897bd056e93ddf09f329cf03e61cd4020cb62655d06ad555e426d59ac75ec21625cab448d57484882c89d0749727ad068277eee60702f70c8e56950)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c18f09",
   "metadata": {
    "papermill": {
     "duration": 0.014547,
     "end_time": "2024-03-22T14:05:55.392680",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.378133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### I have tried three variants of C4.5 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2eba0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:55.424797Z",
     "iopub.status.busy": "2024-03-22T14:05:55.424383Z",
     "iopub.status.idle": "2024-03-22T14:05:55.430487Z",
     "shell.execute_reply": "2024-03-22T14:05:55.429402Z"
    },
    "papermill": {
     "duration": 0.024758,
     "end_time": "2024-03-22T14:05:55.432678",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.407920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False , threshold=None):\n",
    "\n",
    "        self.data = data\n",
    "        self.children = children\n",
    "        self.split_on = split_on\n",
    "        self.threshold = threshold #Used when splitting using discrete features\n",
    "        self.pred_class = pred_class\n",
    "        self.is_leaf = is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc56de8a",
   "metadata": {
    "papermill": {
     "duration": 0.014552,
     "end_time": "2024-03-22T14:05:55.462225",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.447673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**In the below algorithm** I have used Information Gain Ratio, for finding the best split threshold and on which feature to split. To find the best split in continuos feature it first sort the dataset and finds cut with maximum Information Gain and finally among all features, feature with maximum Information Gain ratio is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c1d579",
   "metadata": {
    "papermill": {
     "duration": 0.014737,
     "end_time": "2024-03-22T14:05:55.491941",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.477204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2>Information Gain Ratio</h2>\n",
    "\n",
    "<p>Information gain ratio is a normalized version of information gain, which takes into account the intrinsic information of a split. It helps in reducing the bias towards attributes with a large number of distinct values.</p>\n",
    "\n",
    "<p>The formula for Information Gain Ratio ($\\text{IGR}$) is:</p>\n",
    "\n",
    "$$\n",
    "\\text{IGR}(a, S) = \\frac{\\text{InformationGain}(a, S)}{\\text{Entropy}(a, S)}\n",
    "$$\n",
    "\n",
    "<p>Where:</p>\n",
    "\n",
    "<ul>\n",
    "  <li>$\\text{IGR}(a, S)$: Information gain ratio for attribute $a$ and subset $S$.</li>\n",
    "  <li>$\\text{InformationGain}(a, S)$: Information gain for attribute $a$ and subset $S$.</li>\n",
    "  <li>$\\text{Entropy}(a, S)$: Entropy of attribute $a$ in subset $S$.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Note that this ratio is not defined when the denominator is zero. Also, the ratio may tend to favor attributes for which the denominator is very small. Consequently, it is suggested to first calculate the information gain for all attributes and then consider only attributes that have performed at least as well as the average information gain when selecting the best attribute based on the gain ratio.</p>\n",
    "\n",
    "<p>It has been shown that the gain ratio tends to outperform simple information gain criteria, both in terms of accuracy and classifier complexity.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3202fb33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:55.523597Z",
     "iopub.status.busy": "2024-03-22T14:05:55.523199Z",
     "iopub.status.idle": "2024-03-22T14:05:55.652639Z",
     "shell.execute_reply": "2024-03-22T14:05:55.651515Z"
    },
    "papermill": {
     "duration": 0.148631,
     "end_time": "2024-03-22T14:05:55.655382",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.506751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifierWithIGR:\n",
    "    def __init__(self , discrete):\n",
    "        self.root = Node()\n",
    "        self.discrete = discrete\n",
    "    @staticmethod\n",
    "    def calculate_entropy(Y):\n",
    "        _, labels_counts = np.unique(Y, return_counts=True)\n",
    "        total_instances = len(Y)\n",
    "        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n",
    "        return entropy\n",
    "    def split_on_feature(self, data, feat_index):\n",
    "        feature_values = data[:, feat_index]\n",
    "        unique_values = np.unique(feature_values)\n",
    "\n",
    "        split_nodes = {}\n",
    "        weighted_entropy = 0\n",
    "        total_instances = len(data)\n",
    "        split_info = 0\n",
    "\n",
    "        for unique_value in unique_values:\n",
    "            partition = data[data[:, feat_index] == unique_value, :]\n",
    "            split_info = split_info + (len(partition)/total_instances)*np.log2(len(partition)/total_instances)\n",
    "            node = Node(data=partition)\n",
    "            split_nodes[unique_value] = node\n",
    "            partition_y = self.get_y(partition)\n",
    "            node_entropy = self.calculate_entropy(partition_y)\n",
    "            weighted_entropy += (len(partition) / total_instances) * node_entropy\n",
    "\n",
    "        return split_nodes, weighted_entropy, -1*split_info\n",
    "    \n",
    "    def very_fast_split_on_contfeature(self, data, feat_index):\n",
    "        feature_values = data[:, feat_index]\n",
    "        mean = np.mean(feature_values)\n",
    "        median = np.median(feature_values)\n",
    "        total_entropy = self.calculate_entropy(data[:, -1])\n",
    "        total_instances = len(data)\n",
    "        weighted_entropy = 0\n",
    "        split_nodes = {}\n",
    "        \n",
    "        left_data1 = data[data[:, feat_index] <= mean]\n",
    "        right_data1 = data[data[:, feat_index] > mean]\n",
    "        left_node1 = Node(data = left_data1)\n",
    "        right_node1 = Node(data = right_data1)\n",
    "        left_entropy1 = self.calculate_entropy(left_data1[:, -1])\n",
    "        right_entropy1 = self.calculate_entropy(right_data1[:, -1])\n",
    "        info_gain1 = total_entropy - (len(left_data1) / total_instances * left_entropy1) - (len(right_data1) / total_instances * right_entropy1)\n",
    "        \n",
    "        left_data2 = data[data[:, feat_index] <= median]\n",
    "        right_data2 = data[data[:, feat_index] > median]\n",
    "        left_node2 = Node(data = left_data2)\n",
    "        right_node2 = Node(data = right_data2)\n",
    "        left_entropy2 = self.calculate_entropy(left_data2[:, -1])\n",
    "        right_entropy2 = self.calculate_entropy(right_data2[:, -1])\n",
    "        info_gain2 = total_entropy - (len(left_data2) / total_instances * left_entropy2) - (len(right_data2) / total_instances * right_entropy2)\n",
    "    \n",
    "        if info_gain1 > info_gain2:\n",
    "            weighted_entropy = (len(left_data1) / total_instances * left_entropy1) + (len(right_data1) / total_instances * right_entropy1)\n",
    "            split_info = (len(left_data1) / total_instances)*np.log2((len(left_data1)+0.001) / total_instances) + (len(right_data1) / total_instances)*np.log2((len(right_data1)+0.001) / total_instances)\n",
    "            best_split_point = mean\n",
    "            split_nodes[0] = left_node1\n",
    "            split_nodes[1] = right_node1\n",
    "        else:\n",
    "            weighted_entropy = (len(left_data2) / total_instances * left_entropy2) + (len(right_data2) / total_instances * right_entropy2)\n",
    "            split_info = (len(left_data2) / total_instances)*np.log2((len(left_data2)+0.001) / total_instances) + (len(right_data2) / total_instances)*np.log2((len(right_data2)+0.001) / total_instances)\n",
    "            best_split_point = median\n",
    "            split_nodes[0] = left_node2\n",
    "            split_nodes[1] = right_node2\n",
    "        \n",
    "        return best_split_point, split_nodes, weighted_entropy, -1*split_info\n",
    "        \n",
    "    def split_on_contfeature(self, data, feat_index):\n",
    "        feature_values = data[:, feat_index]\n",
    "        feature_values = np.sort(feature_values)\n",
    "        total_instances = len(data)\n",
    "        best_split_point = None\n",
    "        max_info_gain = -float('inf')\n",
    "        split_nodes = {}\n",
    "        y = self.get_y(data)\n",
    "        weighted_entropy = 0\n",
    "        split_info = 0\n",
    "\n",
    "        for i in range(1, len(feature_values)):\n",
    "            # Compute split point\n",
    "            split_point = (feature_values[i] + feature_values[i-1])/2\n",
    "\n",
    "            # Split data based on the split point\n",
    "            \n",
    "            left_data = data[data[:, feat_index] <= split_point]\n",
    "            right_data = data[data[:, feat_index] > split_point]\n",
    "            left_node = Node(data = left_data)\n",
    "            right_node = Node(data = right_data)\n",
    "\n",
    "            # Calculate information gain\n",
    "            left_entropy = self.calculate_entropy(left_data[:, -1])\n",
    "            right_entropy = self.calculate_entropy(right_data[:, -1])\n",
    "            total_entropy = self.calculate_entropy(data[:, -1])\n",
    "            \n",
    "            info_gain = total_entropy - (len(left_data) / total_instances * left_entropy) - (len(right_data) / total_instances * right_entropy)\n",
    "            \n",
    "\n",
    "            # Update best split point if information gain is higher\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                weighted_entropy = (len(left_data) / total_instances * left_entropy) + (len(right_data) / total_instances * right_entropy)\n",
    "                split_info = (len(left_data) / total_instances)*np.log2((len(left_data)+0.001) / total_instances) + (len(right_data) / total_instances)*np.log2((len(right_data)+0.001) / total_instances)\n",
    "                best_split_point = split_point\n",
    "                split_nodes[0] = left_node\n",
    "                split_nodes[1] = right_node\n",
    "        \n",
    "        if len(split_nodes[0].data)==0 or len(split_nodes[1].data)==0: \n",
    "            weighted_entropy = self.calculate_entropy(y)\n",
    "        return best_split_point, split_nodes, weighted_entropy, -1*split_info\n",
    "\n",
    "        \n",
    "    def best_split(self, node):\n",
    "        if self.meet_criteria(node):\n",
    "            node.is_leaf = True\n",
    "            y = self.get_y(node.data)\n",
    "            node.pred_class = self.get_pred_class(y)\n",
    "            return\n",
    "        y = self.get_y(node.data)\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return\n",
    "        tot_entropy = self.calculate_entropy(y)\n",
    "        index_feature_split = -1\n",
    "        min_entropy = 1\n",
    "        threshold = -1\n",
    "        max_info_gain_ratio = -float('inf')\n",
    "        avg_info_gain = 0\n",
    "        for i in range(node.data.shape[1] - 1):\n",
    "            if discrete[i] is True:\n",
    "                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n",
    "                avg_info_gain += tot_entropy - weighted_entropy\n",
    "            else:\n",
    "                best_split_point, split_nodes, weighted_entropy, split_info = self.very_fast_split_on_contfeature(node.data, i)\n",
    "                avg_info_gain += tot_entropy - weighted_entropy\n",
    "                                 \n",
    "        avg_info_gain = avg_info_gain/(node.data.shape[1] - 1)   \n",
    "\n",
    "        for i in range(node.data.shape[1] - 1):\n",
    "            if discrete[i] is True:\n",
    "                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n",
    "                if tot_entropy - weighted_entropy >= avg_info_gain:\n",
    "                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n",
    "                    #if weighted_entropy < min_entropy:\n",
    "                    if info_gain_ratio > max_info_gain_ratio:\n",
    "                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n",
    "                        index_feature_split = i\n",
    "            else:\n",
    "                best_split_point, split_nodes, weighted_entropy, split_info = self.split_on_contfeature(node.data, i)\n",
    "                if tot_entropy - weighted_entropy >= avg_info_gain:\n",
    "                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n",
    "                    #if weighted_entropy < min_entropy:\n",
    "                    if info_gain_ratio > max_info_gain_ratio:\n",
    "                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n",
    "                        index_feature_split = i\n",
    "                        threshold = best_split_point\n",
    "\n",
    "        \n",
    "        node.children = child_nodes\n",
    "        node.split_on = index_feature_split\n",
    "        if discrete[index_feature_split] is False:\n",
    "            node.threshold = threshold\n",
    "            \n",
    "        for child_node in child_nodes.values():\n",
    "            self.best_split(child_node)\n",
    "    def meet_criteria(self, node):\n",
    "        if len(node.data) < 10:\n",
    "            return True\n",
    "        y = self.get_y(node.data)\n",
    "        return True if self.calculate_entropy(y) == 0 else False\n",
    "    @staticmethod\n",
    "    def get_y(data):\n",
    "        y = data[:, -1]\n",
    "        return y\n",
    "    @staticmethod\n",
    "    def get_pred_class(Y):\n",
    "        labels, labels_counts = np.unique(Y, return_counts=True)\n",
    "        index = np.argmax(labels_counts)\n",
    "        return labels[index]\n",
    "    def fit(self, X, Y):\n",
    "        data = np.column_stack([X, Y])\n",
    "        self.root.data = data\n",
    "        self.best_split(self.root)\n",
    "    def predict(self, X):\n",
    "        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n",
    "        for i in range(len(X)):  # Corrected range syntax\n",
    "            prediction = self.traverse_tree(X[i], self.root)\n",
    "            predictions[i] = prediction  # Insert prediction into the numpy array\n",
    "        return predictions\n",
    "    def traverse_tree(self, x, node):\n",
    "        if node.is_leaf:\n",
    "            return node.pred_class\n",
    "        feat_value = x[node.split_on]\n",
    "        if node.threshold is None:\n",
    "            predicted_class = self.traverse_tree(x, node.children[feat_value])\n",
    "        else:\n",
    "            if feat_value >= node.threshold:\n",
    "                predicted_class = self.traverse_tree(x, node.children[1])\n",
    "            else:\n",
    "                predicted_class = self.traverse_tree(x, node.children[0])\n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2861b45",
   "metadata": {
    "papermill": {
     "duration": 0.015338,
     "end_time": "2024-03-22T14:05:55.686483",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.671145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The below presented algorithm is the variant of above with a change such that on continuous features it finds cut with respect to medain and mean of values in that feature So it works pretty fast but their is some compromise to the accuracy. Choosing mean and median has special effect mentioned in [https://www.tandfonline.com/doi/full/10.1080/08839514.2018.1447479](http://)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf16b88",
   "metadata": {
    "papermill": {
     "duration": 0.014756,
     "end_time": "2024-03-22T14:05:55.716297",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.701541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The main advantage of the proposed algorithm is that it avoids the sorting process with complexity of O(mn log n), also for each attribute, there are only two cut points to evaluate. In algorithm 2, we start by computing mean and median for each attribute with a complexity of O(2mn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86ec67cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:55.748936Z",
     "iopub.status.busy": "2024-03-22T14:05:55.748525Z",
     "iopub.status.idle": "2024-03-22T14:05:55.789585Z",
     "shell.execute_reply": "2024-03-22T14:05:55.788404Z"
    },
    "papermill": {
     "duration": 0.060525,
     "end_time": "2024-03-22T14:05:55.792342",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.731817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifierVFWithIGR:\n",
    "    def __init__(self , discrete):\n",
    "        self.root = Node()\n",
    "        self.discrete = discrete\n",
    "    @staticmethod\n",
    "    def calculate_entropy(Y):\n",
    "        _, labels_counts = np.unique(Y, return_counts=True)\n",
    "        total_instances = len(Y)\n",
    "        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n",
    "        return entropy\n",
    "    def split_on_feature(self, data, feat_index):\n",
    "        feature_values = data[:, feat_index]\n",
    "        unique_values = np.unique(feature_values)\n",
    "\n",
    "        split_nodes = {}\n",
    "        weighted_entropy = 0\n",
    "        total_instances = len(data)\n",
    "        split_info = 0\n",
    "\n",
    "        for unique_value in unique_values:\n",
    "            partition = data[data[:, feat_index] == unique_value, :]\n",
    "            split_info = split_info + (len(partition)/total_instances)*np.log2(len(partition)/total_instances)\n",
    "            node = Node(data=partition)\n",
    "            split_nodes[unique_value] = node\n",
    "            partition_y = self.get_y(partition)\n",
    "            node_entropy = self.calculate_entropy(partition_y)\n",
    "            weighted_entropy += (len(partition) / total_instances) * node_entropy\n",
    "\n",
    "        return split_nodes, weighted_entropy, -1*split_info\n",
    "    \n",
    "    def very_fast_split_on_contfeature(self, data, feat_index):\n",
    "        feature_values = data[:, feat_index]\n",
    "        mean = np.mean(feature_values)\n",
    "        median = np.median(feature_values)\n",
    "        total_entropy = self.calculate_entropy(data[:, -1])\n",
    "        total_instances = len(data)\n",
    "        weighted_entropy = 0\n",
    "        split_nodes = {}\n",
    "        \n",
    "        left_data1 = data[data[:, feat_index] <= mean]\n",
    "        right_data1 = data[data[:, feat_index] > mean]\n",
    "        left_node1 = Node(data = left_data1)\n",
    "        right_node1 = Node(data = right_data1)\n",
    "        left_entropy1 = self.calculate_entropy(left_data1[:, -1])\n",
    "        right_entropy1 = self.calculate_entropy(right_data1[:, -1])\n",
    "        info_gain1 = total_entropy - (len(left_data1) / total_instances * left_entropy1) - (len(right_data1) / total_instances * right_entropy1)\n",
    "        \n",
    "        left_data2 = data[data[:, feat_index] <= median]\n",
    "        right_data2 = data[data[:, feat_index] > median]\n",
    "        left_node2 = Node(data = left_data2)\n",
    "        right_node2 = Node(data = right_data2)\n",
    "        left_entropy2 = self.calculate_entropy(left_data2[:, -1])\n",
    "        right_entropy2 = self.calculate_entropy(right_data2[:, -1])\n",
    "        info_gain2 = total_entropy - (len(left_data2) / total_instances * left_entropy2) - (len(right_data2) / total_instances * right_entropy2)\n",
    "    \n",
    "        if info_gain1 > info_gain2:\n",
    "            weighted_entropy = (len(left_data1) / total_instances * left_entropy1) + (len(right_data1) / total_instances * right_entropy1)\n",
    "            split_info = (len(left_data1) / total_instances)*np.log2((len(left_data1)+0.001) / total_instances) + (len(right_data1) / total_instances)*np.log2((len(right_data1)+0.001) / total_instances)\n",
    "            best_split_point = mean\n",
    "            split_nodes[0] = left_node1\n",
    "            split_nodes[1] = right_node1\n",
    "        else:\n",
    "            weighted_entropy = (len(left_data2) / total_instances * left_entropy2) + (len(right_data2) / total_instances * right_entropy2)\n",
    "            split_info = (len(left_data2) / total_instances)*np.log2((len(left_data2)+0.001) / total_instances) + (len(right_data2) / total_instances)*np.log2((len(right_data2)+0.001) / total_instances)\n",
    "            best_split_point = median\n",
    "            split_nodes[0] = left_node2\n",
    "            split_nodes[1] = right_node2\n",
    "            \n",
    "        if len(split_nodes[0].data)==0 or len(split_nodes[1].data)==0: \n",
    "            weighted_entropy = self.calculate_entropy(self.get_y(data))\n",
    "        return best_split_point, split_nodes, weighted_entropy, -1*split_info\n",
    "               \n",
    "    def best_split(self, node):\n",
    "        if self.meet_criteria(node):\n",
    "            node.is_leaf = True\n",
    "            y = self.get_y(node.data)\n",
    "            node.pred_class = self.get_pred_class(y)\n",
    "            return\n",
    "        y = self.get_y(node.data)\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return\n",
    "        tot_entropy = self.calculate_entropy(y)\n",
    "        index_feature_split = -1\n",
    "        min_entropy = 1\n",
    "        threshold = -1\n",
    "        max_info_gain_ratio = -float('inf')\n",
    "        avg_info_gain = 0\n",
    "        for i in range(node.data.shape[1] - 1):\n",
    "            if discrete[i] is True:\n",
    "                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n",
    "                avg_info_gain += tot_entropy - weighted_entropy\n",
    "            else:\n",
    "                best_split_point, split_nodes, weighted_entropy, split_info = self.very_fast_split_on_contfeature(node.data, i)\n",
    "                avg_info_gain += tot_entropy - weighted_entropy\n",
    "                                 \n",
    "        avg_info_gain = avg_info_gain/(node.data.shape[1] - 1)                       \n",
    "        for i in range(node.data.shape[1] - 1):\n",
    "            if discrete[i] is True:\n",
    "                split_nodes, weighted_entropy, split_info = self.split_on_feature(node.data, i)\n",
    "                if tot_entropy - weighted_entropy >= avg_info_gain:\n",
    "                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n",
    "                    #if weighted_entropy < min_entropy:\n",
    "                    if info_gain_ratio > max_info_gain_ratio:\n",
    "                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n",
    "                        index_feature_split = i\n",
    "            else:\n",
    "                best_split_point, split_nodes, weighted_entropy, split_info = self.very_fast_split_on_contfeature(node.data, i)\n",
    "                if tot_entropy - weighted_entropy >= avg_info_gain:\n",
    "                    info_gain_ratio = (tot_entropy - weighted_entropy)/split_info\n",
    "                    #if weighted_entropy < min_entropy:\n",
    "                    if info_gain_ratio > max_info_gain_ratio:\n",
    "                        child_nodes, min_entropy, max_info_gain_ratio = split_nodes, weighted_entropy, info_gain_ratio\n",
    "                        index_feature_split = i\n",
    "                        threshold = best_split_point\n",
    "\n",
    "        \n",
    "        node.children = child_nodes\n",
    "        node.split_on = index_feature_split\n",
    "        if discrete[index_feature_split] is False:\n",
    "            node.threshold = threshold\n",
    "            \n",
    "        for child_node in child_nodes.values():\n",
    "            self.best_split(child_node)\n",
    "    def meet_criteria(self, node):\n",
    "        if len(node.data) < 10:\n",
    "            return True\n",
    "        y = self.get_y(node.data)\n",
    "        return True if self.calculate_entropy(y) == 0 else False\n",
    "    @staticmethod\n",
    "    def get_y(data):\n",
    "        y = data[:, -1]\n",
    "        return y\n",
    "    @staticmethod\n",
    "    def get_pred_class(Y):\n",
    "        labels, labels_counts = np.unique(Y, return_counts=True)\n",
    "        index = np.argmax(labels_counts)\n",
    "        return labels[index]\n",
    "    def fit(self, X, Y):\n",
    "        data = np.column_stack([X, Y])\n",
    "        self.root.data = data\n",
    "        self.best_split(self.root)\n",
    "    def predict(self, X):\n",
    "        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n",
    "        for i in range(len(X)):  # Corrected range syntax\n",
    "            prediction = self.traverse_tree(X[i], self.root)\n",
    "            predictions[i] = prediction  # Insert prediction into the numpy array\n",
    "        return predictions\n",
    "    def traverse_tree(self, x, node):\n",
    "        if node.is_leaf:\n",
    "            return node.pred_class\n",
    "        feat_value = x[node.split_on]\n",
    "        if node.threshold is None:\n",
    "            predicted_class = self.traverse_tree(x, node.children[feat_value])\n",
    "        else:\n",
    "            if feat_value >= node.threshold:\n",
    "                predicted_class = self.traverse_tree(x, node.children[1])\n",
    "            else:\n",
    "                predicted_class = self.traverse_tree(x, node.children[0])\n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b31fa",
   "metadata": {
    "papermill": {
     "duration": 0.015189,
     "end_time": "2024-03-22T14:05:55.822697",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.807508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## OBSERVATION\n",
    "<big>I was using directly information gain ratio then the accuracy was coming around 51% - 52%. I noticed that the ratio may tend to favor attributes for which the denominator is very small. Consequently, it is suggested in two stages. First the information gain is calculated for all attributes. As a consequence, taking into consideration only attributes that have performed at least as good as the average information gain, the at- tribute that has obtained the best ratio gain is selected. This improved drastically the accuracy</big>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00991cea",
   "metadata": {
    "papermill": {
     "duration": 0.015061,
     "end_time": "2024-03-22T14:05:55.853242",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.838181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "*This below is also the variant of first algorithm I provided in C4.5 in this to decide between features I use Information Gain only.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eff4508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:55.885531Z",
     "iopub.status.busy": "2024-03-22T14:05:55.885099Z",
     "iopub.status.idle": "2024-03-22T14:05:55.915851Z",
     "shell.execute_reply": "2024-03-22T14:05:55.914703Z"
    },
    "papermill": {
     "duration": 0.050503,
     "end_time": "2024-03-22T14:05:55.918775",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.868272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifierwithIG:\n",
    "    def __init__(self , discrete):\n",
    "        self.root = Node()\n",
    "        self.discrete = discrete\n",
    "    @staticmethod\n",
    "    def calculate_entropy(Y):\n",
    "        _, labels_counts = np.unique(Y, return_counts=True)\n",
    "        total_instances = len(Y)\n",
    "        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n",
    "        return entropy\n",
    "    def split_on_feature(self, data, feat_index):\n",
    "        feature_values = data[:, feat_index]\n",
    "        unique_values = np.unique(feature_values)\n",
    "\n",
    "        split_nodes = {}\n",
    "        weighted_entropy = 0\n",
    "        total_instances = len(data)\n",
    "\n",
    "        for unique_value in unique_values:\n",
    "            partition = data[data[:, feat_index] == unique_value, :]\n",
    "            node = Node(data=partition)\n",
    "            split_nodes[unique_value] = node\n",
    "            partition_y = self.get_y(partition)\n",
    "            node_entropy = self.calculate_entropy(partition_y)\n",
    "            weighted_entropy += (len(partition) / total_instances) * node_entropy\n",
    "\n",
    "        return split_nodes, weighted_entropy\n",
    "    \n",
    "    def split_on_contfeature(self, data, feat_index):\n",
    "        feature_values = data[:, feat_index]\n",
    "        feature_values = np.sort(feature_values)\n",
    "        total_instances = len(data)\n",
    "        best_split_point = None\n",
    "        max_info_gain = -float('inf')\n",
    "        split_nodes = {}\n",
    "        weighted_entropy = 0\n",
    "\n",
    "        for i in range(1, len(feature_values)):\n",
    "            # Compute split point\n",
    "            split_point = (feature_values[i] + feature_values[i-1])/2\n",
    "\n",
    "            # Split data based on the split point\n",
    "            \n",
    "            left_data = data[data[:, feat_index] <= split_point]\n",
    "            right_data = data[data[:, feat_index] > split_point]\n",
    "            left_node = Node(data = left_data)\n",
    "            right_node = Node(data = right_data)\n",
    "\n",
    "            # Calculate information gain\n",
    "            left_entropy = self.calculate_entropy(left_data[:, -1])\n",
    "            right_entropy = self.calculate_entropy(right_data[:, -1])\n",
    "            total_entropy = self.calculate_entropy(data[:, -1])\n",
    "            \n",
    "            info_gain = total_entropy - (len(left_data) / total_instances * left_entropy) - (len(right_data) / total_instances * right_entropy)\n",
    "\n",
    "            # Update best split point if information gain is higher\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                weighted_entropy = (len(left_data) / total_instances * left_entropy) + (len(right_data) / total_instances * right_entropy)\n",
    "                best_split_point = split_point\n",
    "                split_nodes[0] = left_node\n",
    "                split_nodes[1] = right_node\n",
    "        \n",
    "        return best_split_point, split_nodes, weighted_entropy\n",
    "\n",
    "        \n",
    "    def best_split(self, node):\n",
    "        if self.meet_criteria(node):\n",
    "            node.is_leaf = True\n",
    "            y = self.get_y(node.data)\n",
    "            node.pred_class = self.get_pred_class(y)\n",
    "            return\n",
    "\n",
    "        index_feature_split = -1\n",
    "        min_entropy = 1\n",
    "        threshold = -1\n",
    "\n",
    "        for i in range(node.data.shape[1] - 1):\n",
    "            if discrete[i] is True:\n",
    "                split_nodes, weighted_entropy = self.split_on_feature(node.data, i)\n",
    "                if weighted_entropy < min_entropy:\n",
    "                    child_nodes, min_entropy = split_nodes, weighted_entropy\n",
    "                    index_feature_split = i\n",
    "            else:\n",
    "                best_split_point, split_nodes, weighted_entropy = self.split_on_contfeature(node.data, i)\n",
    "                if weighted_entropy < min_entropy:\n",
    "                    child_nodes, min_entropy = split_nodes, weighted_entropy\n",
    "                    index_feature_split = i\n",
    "                    threshold = best_split_point\n",
    "\n",
    "        \n",
    "        node.children = child_nodes\n",
    "        node.split_on = index_feature_split\n",
    "        if discrete[index_feature_split] is False:\n",
    "            node.threshold = threshold\n",
    "            \n",
    "        for child_node in child_nodes.values():\n",
    "            self.best_split(child_node)\n",
    "    def meet_criteria(self, node):\n",
    "        if len(node.data) < 100:\n",
    "            return True\n",
    "        y = self.get_y(node.data)\n",
    "        return True if self.calculate_entropy(y) == 0 else False\n",
    "    @staticmethod\n",
    "    def get_y(data):\n",
    "        y = data[:, -1]\n",
    "        return y\n",
    "    @staticmethod\n",
    "    def get_pred_class(Y):\n",
    "        labels, labels_counts = np.unique(Y, return_counts=True)\n",
    "        index = np.argmax(labels_counts)\n",
    "        return labels[index]\n",
    "    def fit(self, X, Y):\n",
    "        data = np.column_stack([X, Y])\n",
    "        self.root.data = data\n",
    "        self.best_split(self.root)\n",
    "    def predict(self, X):\n",
    "        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n",
    "        for i in range(len(X)):  # Corrected range syntax\n",
    "            prediction = self.traverse_tree(X[i], self.root)\n",
    "            predictions[i] = prediction  # Insert prediction into the numpy array\n",
    "        return predictions\n",
    "    def traverse_tree(self, x, node):\n",
    "        if node.is_leaf:\n",
    "            return node.pred_class\n",
    "        feat_value = x[node.split_on]\n",
    "        if node.threshold is None:\n",
    "            predicted_class = self.traverse_tree(x, node.children[feat_value])\n",
    "        else:\n",
    "            if feat_value >= node.threshold:\n",
    "                predicted_class = self.traverse_tree(x, node.children[1])\n",
    "            else:\n",
    "                predicted_class = self.traverse_tree(x, node.children[0])\n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c2a11",
   "metadata": {
    "papermill": {
     "duration": 0.014486,
     "end_time": "2024-03-22T14:05:55.948606",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.934120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Information Gain Ratio vs Information Gain**\n",
    "The IGR is a modification of information gain to reduce feature bias towards attributes that has many\n",
    "branches. The gain ratio is large if the data is spread evenly and the value will be small if all data enters into one\n",
    "branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a068194d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:05:55.980234Z",
     "iopub.status.busy": "2024-03-22T14:05:55.979851Z",
     "iopub.status.idle": "2024-03-22T14:10:40.908132Z",
     "shell.execute_reply": "2024-03-22T14:10:40.907155Z"
    },
    "papermill": {
     "duration": 284.947451,
     "end_time": "2024-03-22T14:10:40.910783",
     "exception": false,
     "start_time": "2024-03-22T14:05:55.963332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = DecisionTreeClassifierwithIG(discrete)\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e771a06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:10:40.941546Z",
     "iopub.status.busy": "2024-03-22T14:10:40.941150Z",
     "iopub.status.idle": "2024-03-22T14:17:53.567921Z",
     "shell.execute_reply": "2024-03-22T14:17:53.566746Z"
    },
    "papermill": {
     "duration": 432.645328,
     "end_time": "2024-03-22T14:17:53.570896",
     "exception": false,
     "start_time": "2024-03-22T14:10:40.925568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = DecisionTreeClassifierWithIGR(discrete)\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27e61bf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:17:53.652966Z",
     "iopub.status.busy": "2024-03-22T14:17:53.652613Z",
     "iopub.status.idle": "2024-03-22T14:17:58.518172Z",
     "shell.execute_reply": "2024-03-22T14:17:58.517216Z"
    },
    "papermill": {
     "duration": 4.883628,
     "end_time": "2024-03-22T14:17:58.520908",
     "exception": false,
     "start_time": "2024-03-22T14:17:53.637280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3 = DecisionTreeClassifierVFWithIGR(discrete)\n",
    "model3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb3f7a",
   "metadata": {
    "papermill": {
     "duration": 0.01401,
     "end_time": "2024-03-22T14:17:58.549886",
     "exception": false,
     "start_time": "2024-03-22T14:17:58.535876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***Here You can check the accuracy of all the three models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9aff3e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:17:58.580267Z",
     "iopub.status.busy": "2024-03-22T14:17:58.579611Z",
     "iopub.status.idle": "2024-03-22T14:17:58.605695Z",
     "shell.execute_reply": "2024-03-22T14:17:58.604415Z"
    },
    "papermill": {
     "duration": 0.044053,
     "end_time": "2024-03-22T14:17:58.608312",
     "exception": false,
     "start_time": "2024-03-22T14:17:58.564259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8219597550306211\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = model2.predict(X_test.to_numpy())\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20d1ea16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:17:58.638978Z",
     "iopub.status.busy": "2024-03-22T14:17:58.638667Z",
     "iopub.status.idle": "2024-03-22T14:17:58.697751Z",
     "shell.execute_reply": "2024-03-22T14:17:58.696598Z"
    },
    "papermill": {
     "duration": 0.077091,
     "end_time": "2024-03-22T14:17:58.699974",
     "exception": false,
     "start_time": "2024-03-22T14:17:58.622883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with Information_Gain\n",
      "0.9041994750656168\n",
      "Model with Information_Gain_Ratio\n",
      "0.8219597550306211\n",
      "Model with Information_Gain_Ratio (Very fast C4.5)\n",
      "0.8810148731408574\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_pred1 = model1.predict(X_test.to_numpy())\n",
    "y_pred2 = model2.predict(X_test.to_numpy())\n",
    "y_pred3 = model3.predict(X_test.to_numpy())\n",
    "print(\"Model with Information_Gain\")\n",
    "print(metrics.accuracy_score(y_test,y_pred1))\n",
    "print(\"Model with Information_Gain_Ratio\")\n",
    "print(metrics.accuracy_score(y_test,y_pred2))\n",
    "print(\"Model with Information_Gain_Ratio (Very fast C4.5)\")\n",
    "print(metrics.accuracy_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad21698",
   "metadata": {
    "papermill": {
     "duration": 0.014139,
     "end_time": "2024-03-22T14:17:58.728904",
     "exception": false,
     "start_time": "2024-03-22T14:17:58.714765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CART ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f1468",
   "metadata": {
    "papermill": {
     "duration": 0.014147,
     "end_time": "2024-03-22T14:17:58.756404",
     "exception": false,
     "start_time": "2024-03-22T14:17:58.742257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### **Classification And Regression Tree**\n",
    "* It is characterized by the fact that it constructs binary trees, namely each in- ternal node has exactly two outgoing edges.\n",
    "* CART works on Gini Impurity\n",
    "* CART allows for regression also, but I have implemented only the classification part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8938f603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:17:58.786950Z",
     "iopub.status.busy": "2024-03-22T14:17:58.786173Z",
     "iopub.status.idle": "2024-03-22T14:17:58.791520Z",
     "shell.execute_reply": "2024-03-22T14:17:58.790921Z"
    },
    "papermill": {
     "duration": 0.022538,
     "end_time": "2024-03-22T14:17:58.793273",
     "exception": false,
     "start_time": "2024-03-22T14:17:58.770735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data=None, children=None, split_on = None, pred_class=None, is_leaf=False , threshold=None):\n",
    "\n",
    "        self.data = data\n",
    "        self.children = children\n",
    "        self.split_on = split_on\n",
    "        self.threshold = threshold #Used when splitting using discrete features\n",
    "        self.pred_class = pred_class\n",
    "        self.is_leaf = is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf755dcd",
   "metadata": {
    "papermill": {
     "duration": 0.013451,
     "end_time": "2024-03-22T14:17:58.820574",
     "exception": false,
     "start_time": "2024-03-22T14:17:58.807123",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5082901",
   "metadata": {
    "papermill": {
     "duration": 0.013928,
     "end_time": "2024-03-22T14:17:58.848178",
     "exception": false,
     "start_time": "2024-03-22T14:17:58.834250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2>Gini Impurity</h2>\n",
    "\n",
    "<p>Gini impurity is an impurity-based criterion used in decision trees to measure the divergences between the probability distributions of the target attribute’s values.</p>\n",
    "\n",
    "<p>The Gini impurity formula is defined as:</p>\n",
    "\n",
    "$$\n",
    "\\text{Gini}(y, S) = 1 - \\sum_{c_j \\in \\text{dom}(y)} \\left( \\frac{|\\bar{\\sigma}_{y=c_j}(S)|}{|S|} \\right)^2\n",
    "$$\n",
    "\n",
    "<p>Where:</p>\n",
    "\n",
    "<ul>\n",
    "  <li>$\\text{Gini}(y, S)$: Gini impurity for attribute $y$ and subset $S$.</li>\n",
    "  <li>$\\text{dom}(y)$: Domain of attribute $y$.</li>\n",
    "  <li>$\\bar{\\sigma}_{y=c_j}(S)$: Subset of $S$ where attribute $y$ takes the value $c_j$.</li>\n",
    "</ul>\n",
    "\n",
    "<p>The evaluation criterion for selecting the attribute $a_i$ is defined as:</p>\n",
    "\n",
    "$$\n",
    "\\text{GiniGain}(a_i, S) = \\text{Gini}(y, S) - \\sum_{\\bar{\\sigma}_{a_i=v_{i,j}}(S)} \\frac{|\\bar{\\sigma}_{a_i=v_{i,j}}(S)| \\cdot \\text{Gini}(y, \\bar{\\sigma}_{a_i=v_{i,j}}(S))}{|S|}\n",
    "$$\n",
    "\n",
    "<p>Where:</p>\n",
    "\n",
    "<ul>\n",
    "  <li>$\\text{GiniGain}(a_i, S)$: Gini gain for attribute $a_i$ and subset $S$.</li>\n",
    "  <li>$v_{i,j}$: Values of attribute $a_i$.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7ae267b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:17:58.878009Z",
     "iopub.status.busy": "2024-03-22T14:17:58.877658Z",
     "iopub.status.idle": "2024-03-22T14:17:58.911352Z",
     "shell.execute_reply": "2024-03-22T14:17:58.910449Z"
    },
    "papermill": {
     "duration": 0.051546,
     "end_time": "2024-03-22T14:17:58.913683",
     "exception": false,
     "start_time": "2024-03-22T14:17:58.862137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifierwithGini:\n",
    "    def __init__(self , discrete):\n",
    "        self.root = Node()\n",
    "        self.discrete = discrete\n",
    "    @staticmethod\n",
    "    def calcGiniImpurity(Y):\n",
    "        total_samples = len(Y)\n",
    "        if total_samples == 0:\n",
    "            return 0.0\n",
    "\n",
    "        label_counts = {}\n",
    "        for label in Y:\n",
    "            if label in label_counts:\n",
    "                label_counts[label] += 1\n",
    "            else:\n",
    "                label_counts[label] = 1\n",
    "\n",
    "        gini_impurity = 1.0\n",
    "        for label in label_counts:\n",
    "            probability = label_counts[label] / total_samples\n",
    "            gini_impurity -= probability ** 2\n",
    "\n",
    "        return gini_impurity\n",
    "    \n",
    "    \n",
    "    def split_on_contfeature(self, data, feat_index):\n",
    "        feature_values = data[:, feat_index]\n",
    "        feature_values = np.sort(feature_values)\n",
    "        total_instances = len(data)\n",
    "        best_split_point = None\n",
    "        mini_gini_impurity = float('inf')\n",
    "        split_nodes = {}\n",
    "        weighted_gini_impurity = 0\n",
    "\n",
    "        for i in range(1, len(feature_values)):\n",
    "            # Compute split point\n",
    "            split_point = (feature_values[i] + feature_values[i-1])/2\n",
    "\n",
    "            # Split data based on the split point\n",
    "            \n",
    "            left_data = data[data[:, feat_index] <= split_point]\n",
    "            right_data = data[data[:, feat_index] > split_point]\n",
    "            left_node = Node(data = left_data)\n",
    "            right_node = Node(data = right_data)\n",
    "\n",
    "            # Calculate information gain\n",
    "            left_gini_impurity = self.calcGiniImpurity(left_data[:, -1])\n",
    "            right_gini_impurity = self.calcGiniImpurity(right_data[:, -1])\n",
    "    \n",
    "        \n",
    "            impurity = (len(left_data) / total_instances * left_gini_impurity) + (len(right_data) / total_instances * right_gini_impurity)\n",
    "\n",
    "            # Update best split point if information gain is higher\n",
    "            if impurity < mini_gini_impurity:\n",
    "                mini_gini_impurity = impurity\n",
    "                weighted_gini_impurity = impurity\n",
    "                best_split_point = split_point\n",
    "                split_nodes[0] = left_node\n",
    "                split_nodes[1] = right_node\n",
    "        \n",
    "        return split_nodes, weighted_gini_impurity, best_split_point\n",
    "\n",
    "    def split_on_feature(self , data , feat_index):\n",
    "        feature_values = data[:, feat_index]\n",
    "        unique_values = np.unique(feature_values)\n",
    "        split_nodes = {}\n",
    "        total_instances = len(data)\n",
    "        best_split_point = None\n",
    "        mini_gini_impurity = float('inf')\n",
    "        weighted_gini_impurity = 0\n",
    "        \n",
    "        for unique_value in unique_values:\n",
    "            left_data = data[data[:, feat_index] == unique_value, :]\n",
    "            right_data = data[data[:, feat_index] != unique_value, :]\n",
    "            left_node = Node(data = left_data)\n",
    "            right_node = Node(data = right_data)\n",
    "            left_gini_impurity = self.calcGiniImpurity(left_data[:, -1])\n",
    "            right_gini_impurity = self.calcGiniImpurity(right_data[:, -1])\n",
    "    \n",
    "        \n",
    "            impurity = (len(left_data) / total_instances * left_gini_impurity) + (len(right_data) / total_instances * right_gini_impurity)\n",
    "\n",
    "            # Update best split point if information gain is higher\n",
    "            if impurity < mini_gini_impurity:\n",
    "                mini_gini_impurity = impurity\n",
    "                weighted_gini_impurity = impurity\n",
    "                best_split_point = unique_value\n",
    "                split_nodes[0] = left_node\n",
    "                split_nodes[1] = right_node\n",
    "        \n",
    "        return split_nodes, weighted_gini_impurity, best_split_point\n",
    "        \n",
    "        \n",
    "    def best_split(self , node):\n",
    "        if self.meet_criteria(node):\n",
    "            node.is_leaf = True\n",
    "            y = self.get_y(node.data)\n",
    "            node.pred_class = self.get_pred_class(y)\n",
    "            return\n",
    "        y = self.get_y(node.data)\n",
    "        totalGiniImpurity = self.calcGiniImpurity(y)\n",
    "        index_feature_split = -1\n",
    "        #take with max ginigain\n",
    "        max_ginigain = 0\n",
    "        threshold = -1\n",
    "        \n",
    "        for i in range(node.data.shape[1] - 1):\n",
    "            if discrete[i] is True:\n",
    "                split_nodes, weighted_gini_impurity, split_val = self.split_on_feature(node.data , i)\n",
    "                if totalGiniImpurity - weighted_gini_impurity > max_ginigain:\n",
    "                    child_nodes , max_ginigain = split_nodes, totalGiniImpurity - weighted_gini_impurity\n",
    "                    index_feature_split = i\n",
    "                    threshold = split_val\n",
    "            else:\n",
    "                split_nodes, weighted_gini_impurity, split_val = self.split_on_contfeature(node.data , i)\n",
    "                if totalGiniImpurity - weighted_gini_impurity > max_ginigain:\n",
    "                    child_nodes , max_ginigain = split_nodes, totalGiniImpurity - weighted_gini_impurity\n",
    "                    index_feature_split = i\n",
    "                    threshold = split_val\n",
    "                    \n",
    "        node.children = child_nodes\n",
    "        node.split_on = index_feature_split\n",
    "        node.threshold = threshold\n",
    "\n",
    "        for child_node in child_nodes.values():\n",
    "            self.best_split(child_node)\n",
    "    def meet_criteria(self, node):\n",
    "        if len(node.data) < 100:\n",
    "            return True\n",
    "        y = self.get_y(node.data)\n",
    "        return True if self.calcGiniImpurity(y) == 0 else False\n",
    "    @staticmethod\n",
    "    def get_y(data):\n",
    "        y = data[:, -1]\n",
    "        return y\n",
    "    @staticmethod\n",
    "    def get_pred_class(Y):\n",
    "        labels, labels_counts = np.unique(Y, return_counts=True)\n",
    "        index = np.argmax(labels_counts)\n",
    "        return labels[index]\n",
    "    def fit(self, X, Y):\n",
    "        data = np.column_stack([X, Y])\n",
    "        self.root.data = data\n",
    "        self.best_split(self.root)\n",
    "    def predict(self, X):\n",
    "        predictions = np.empty(len(X))  # Create an empty numpy array to store predictions\n",
    "        for i in range(len(X)):  # Corrected range syntax\n",
    "            prediction = self.traverse_tree(X[i], self.root)\n",
    "            predictions[i] = prediction  # Insert prediction into the numpy array\n",
    "        return predictions\n",
    "    def traverse_tree(self, x, node):\n",
    "        if node.is_leaf:\n",
    "            return node.pred_class\n",
    "        feat_value = x[node.split_on]\n",
    "        if discrete[node.split_on] is True:\n",
    "            if feat_value == node.threshold:\n",
    "                predicted_class = self.traverse_tree(x, node.children[0])\n",
    "            else:\n",
    "                predicted_class = self.traverse_tree(x, node.children[1])\n",
    "        else:\n",
    "            if feat_value > node.threshold:\n",
    "                predicted_class = self.traverse_tree(x, node.children[1])\n",
    "            else:\n",
    "                predicted_class = self.traverse_tree(x, node.children[0])\n",
    "        return predicted_class    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "793586cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:17:58.943498Z",
     "iopub.status.busy": "2024-03-22T14:17:58.942680Z",
     "iopub.status.idle": "2024-03-22T14:41:38.667228Z",
     "shell.execute_reply": "2024-03-22T14:41:38.666015Z"
    },
    "papermill": {
     "duration": 1419.741551,
     "end_time": "2024-03-22T14:41:38.670018",
     "exception": false,
     "start_time": "2024-03-22T14:17:58.928467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifierwithGini(discrete)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae2fd11c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:41:38.703432Z",
     "iopub.status.busy": "2024-03-22T14:41:38.702573Z",
     "iopub.status.idle": "2024-03-22T14:41:38.736436Z",
     "shell.execute_reply": "2024-03-22T14:41:38.734368Z"
    },
    "papermill": {
     "duration": 0.053343,
     "end_time": "2024-03-22T14:41:38.738887",
     "exception": false,
     "start_time": "2024-03-22T14:41:38.685544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9234470691163604\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test.to_numpy())\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c67e2",
   "metadata": {
    "papermill": {
     "duration": 0.015201,
     "end_time": "2024-03-22T14:41:38.769365",
     "exception": false,
     "start_time": "2024-03-22T14:41:38.754164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing with Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e1fa84d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:41:38.802098Z",
     "iopub.status.busy": "2024-03-22T14:41:38.801710Z",
     "iopub.status.idle": "2024-03-22T14:41:39.073871Z",
     "shell.execute_reply": "2024-03-22T14:41:39.072906Z"
    },
    "papermill": {
     "duration": 0.291875,
     "end_time": "2024-03-22T14:41:39.076710",
     "exception": false,
     "start_time": "2024-03-22T14:41:38.784835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_gini = DecisionTreeClassifier(criterion='gini', random_state=0)\n",
    "clf_gini.fit(X_train,y_train)\n",
    "y_pred_gini = clf_gini.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b4e8a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:41:39.109763Z",
     "iopub.status.busy": "2024-03-22T14:41:39.109385Z",
     "iopub.status.idle": "2024-03-22T14:41:39.116836Z",
     "shell.execute_reply": "2024-03-22T14:41:39.115690Z"
    },
    "papermill": {
     "duration": 0.027213,
     "end_time": "2024-03-22T14:41:39.119360",
     "exception": false,
     "start_time": "2024-03-22T14:41:39.092147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9212598425196851\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test,y_pred_gini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f1e3bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:41:39.152362Z",
     "iopub.status.busy": "2024-03-22T14:41:39.151932Z",
     "iopub.status.idle": "2024-03-22T14:41:39.225822Z",
     "shell.execute_reply": "2024-03-22T14:41:39.224825Z"
    },
    "papermill": {
     "duration": 0.093344,
     "end_time": "2024-03-22T14:41:39.228527",
     "exception": false,
     "start_time": "2024-03-22T14:41:39.135183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_entropy = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "clf_entropy.fit(X_train,y_train)\n",
    "y_pred_entropy = clf_entropy.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b30ad27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T14:41:39.261247Z",
     "iopub.status.busy": "2024-03-22T14:41:39.260851Z",
     "iopub.status.idle": "2024-03-22T14:41:39.267933Z",
     "shell.execute_reply": "2024-03-22T14:41:39.266909Z"
    },
    "papermill": {
     "duration": 0.02646,
     "end_time": "2024-03-22T14:41:39.270377",
     "exception": false,
     "start_time": "2024-03-22T14:41:39.243917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9186351706036745\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test,y_pred_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaaa7f3",
   "metadata": {
    "papermill": {
     "duration": 0.015313,
     "end_time": "2024-03-22T14:41:39.301791",
     "exception": false,
     "start_time": "2024-03-22T14:41:39.286478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4640874,
     "sourceId": 7901931,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4644838,
     "sourceId": 7907242,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4644892,
     "sourceId": 7907320,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2150.39657,
   "end_time": "2024-03-22T14:41:39.943307",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-22T14:05:49.546737",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
