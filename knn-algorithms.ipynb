{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8108499,"sourceType":"datasetVersion","datasetId":4789542}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":1054.175618,"end_time":"2024-04-19T13:52:51.597156","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-19T13:35:17.421538","version":"2.5.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shag2003/knn-algorithms?scriptVersionId=172889273\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/shag2003/knn-algorithms?scriptVersionId=172867452\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"markdown","source":"## Importing all the libraries","metadata":{"papermill":{"duration":0.014813,"end_time":"2024-04-19T13:35:20.596524","exception":false,"start_time":"2024-04-19T13:35:20.581711","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport itertools\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom math import sqrt","metadata":{"papermill":{"duration":2.276907,"end_time":"2024-04-19T13:35:22.88788","exception":false,"start_time":"2024-04-19T13:35:20.610973","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/phishing-urls/Preprocessed_data.csv')\ndataset.head()","metadata":{"papermill":{"duration":0.130039,"end_time":"2024-04-19T13:35:23.031473","exception":false,"start_time":"2024-04-19T13:35:22.901434","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","metadata":{"papermill":{"duration":0.029596,"end_time":"2024-04-19T13:35:23.07552","exception":false,"start_time":"2024-04-19T13:35:23.045924","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = np.where(y == \"phishing\", 1, -1)\ny","metadata":{"papermill":{"duration":0.027686,"end_time":"2024-04-19T13:35:23.117996","exception":false,"start_time":"2024-04-19T13:35:23.09031","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the dataset into the Training set and Test set","metadata":{"papermill":{"duration":0.014611,"end_time":"2024-04-19T13:35:23.147097","exception":false,"start_time":"2024-04-19T13:35:23.132486","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40, random_state = 0)","metadata":{"papermill":{"duration":0.029997,"end_time":"2024-04-19T13:35:23.191033","exception":false,"start_time":"2024-04-19T13:35:23.161036","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train)","metadata":{"papermill":{"duration":0.024412,"end_time":"2024-04-19T13:35:23.229737","exception":false,"start_time":"2024-04-19T13:35:23.205325","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_train)","metadata":{"papermill":{"duration":0.024447,"end_time":"2024-04-19T13:35:23.268808","exception":false,"start_time":"2024-04-19T13:35:23.244361","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_test)","metadata":{"papermill":{"duration":0.026812,"end_time":"2024-04-19T13:35:23.31023","exception":false,"start_time":"2024-04-19T13:35:23.283418","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_test)","metadata":{"papermill":{"duration":0.02574,"end_time":"2024-04-19T13:35:23.350554","exception":false,"start_time":"2024-04-19T13:35:23.324814","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sc = StandardScaler()\n# X_train = sc.fit_transform(X_train)\n# X_test = sc.transform(X_test) #avoid data leakage","metadata":{"papermill":{"duration":0.022842,"end_time":"2024-04-19T13:35:23.38787","exception":false,"start_time":"2024-04-19T13:35:23.365028","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Function to generate metrics for Algorithm (Precision, Recall, F1_score, Accuracy)","metadata":{"papermill":{"duration":0.014238,"end_time":"2024-04-19T13:35:23.416706","exception":false,"start_time":"2024-04-19T13:35:23.402468","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def calculate_metrics(actual, predicted):\n    if not isinstance(predicted, np.ndarray):\n        predicted = np.array(predicted)\n    matrix = np.zeros((2, 2))\n    n_samples = actual.shape[0]\n    for i in range(n_samples):\n        if actual[i] == predicted[i] and actual[i] == 1:\n            matrix[0, 0] += 1  # True Positive\n        elif actual[i] == predicted[i]:\n            matrix[1, 1] += 1  # True Negative\n        elif predicted[i] == 1:\n            matrix[0, 1] += 1  # False Positive\n        else:\n            matrix[1, 0] += 1  # False Negative\n    \n    # Calculate precision, recall, and F1 score\n    precision = matrix[0,0] / (matrix[0,0] + matrix[0,1]) if (matrix[0,0] + matrix[0,1]) > 0 else 0\n    recall = matrix[0,0] / (matrix[0,0] + matrix[1,0]) if (matrix[0,0] + matrix[1,0]) > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    accuracy = np.sum(np.diag(matrix)) / np.sum(matrix) \n\n    labels = ['Actual Positive', 'Actual Negative']\n    print(\"\\t\\tPredicted Positive\\tPredicted Negative\")\n    for i, row_label in enumerate(labels):\n        print(f\"{row_label}\\t\\t{int(matrix[i,0])}\\t\\t\\t{int(matrix[i,1])}\")\n\n    # Print precision, recall, and F1 score\n    print(\"\\nPrecision:\", precision)\n    print(\"Recall:\", recall)\n    print(\"F1 Score:\", f1_score)\n    print(\"Accuracy:\", accuracy)\n\n    # Plotting the confusion matrix\n    plt.figure(figsize=(4, 4))\n    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n\n    tick_marks = np.arange(2)\n    plt.xticks(tick_marks, ['Actual Pos', 'Actual Neg'])\n    plt.yticks(tick_marks, ['Pred Positive', 'Pred Negative'])\n\n    thresh = matrix.max() / 2.\n    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n        plt.text(j, i, matrix[i, j], horizontalalignment=\"center\", color=\"white\" if matrix[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","metadata":{"papermill":{"duration":0.038363,"end_time":"2024-04-19T13:35:23.469978","exception":false,"start_time":"2024-04-19T13:35:23.431615","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# K-Nearest Neighbors (KNN) Algorithm\n\nK-nearest neighbors (KNN) is a simple and versatile classification algorithm. It is a non-parametric method used for both classification and regression tasks. In this algorithm, the classification of a data point is determined by the majority class among its k nearest neighbors in the feature space.\n\n## Algorithm Overview\n\n1. **Initialize**: Choose the value of $k$, the number of nearest neighbors to consider.\n2. **Train**: Store all the training data.\n3. **Predict**:\n   - For each new data point:\n     - Compute the distance between the new data point and all points in the training set.\n     - Identify the $k$ nearest neighbors based on the computed distances.\n     - Assign the class label by majority vote among the $k$ neighbors (for classification) or compute the average (for regression).\n\n## Distance Metric\n\nThe choice of distance metric is crucial in KNN. Common distance metrics include:\n\n- **Euclidean Distance**:\n  $$\n  \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\n  $$\n- **Manhattan Distance**:\n  $$\n  \\sum_{i=1}^{n}|x_i - y_i|\n  $$\n- **Minkowski Distance**:\n  $$\n  \\left(\\sum_{i=1}^{n}|x_i - y_i|^p\\right)^{\\frac{1}{p}}\n  $$\n  \nWe have used the Euclidean distance in our classic kNN model. \n\n## Hyperparameters\n\n- **$k$**: Number of nearest neighbors to consider.\n- **Distance Metric**: Choice of distance metric used to measure similarity.\n\n## Mathematical Notation\n\n- $X$: Training data with features $X_i$ and labels $y_i$, where $i = 1, 2, ..., n$.\n- $x$: New data point to be classified.\n- $k$: Number of nearest neighbors.\n- $d(x, X_i)$: Distance metric between data point $x$ and $X_i$.\n- $N_k(x)$: Set of $k$ nearest neighbors of $x$.\n\n### Prediction for Classification\n\n$$\n\\hat{y}(x) = \\text{argmax}_c \\sum_{i \\in N_k(x)} I(y_i = c)\n$$\n\nwhere $I(\\cdot)$ is the indicator function.\n\n### Prediction for Regression\n\n$$\n\\hat{y}(x) = \\frac{1}{k} \\sum_{i \\in N_k(x)} y_i\n$$","metadata":{}},{"cell_type":"markdown","source":"## Training the classic kNN model on the Training set","metadata":{"papermill":{"duration":0.013939,"end_time":"2024-04-19T13:35:23.498587","exception":false,"start_time":"2024-04-19T13:35:23.484648","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class ClassicKNN():\n  def __init__(self,k):\n    self.k=k\n    print(self.k)\n  def fit(self,X_train,y_train):\n    self.x_train=X_train\n    self.y_train=y_train\n  def calculate_euclidean(self,sample1,sample2):\n    distance=0.0\n    for i in range(len(sample1)):\n      distance+=(sample1[i]-sample2[i])**2 #Euclidean Distance = sqrt(sum i to N (x1_i – x2_i)^2)\n    return sqrt(distance)\n  def nearest_neighbors(self,test_sample):\n    distances=[]#calculate distances from a test sample to every sample in a training set\n    for i in range(len(self.x_train)):\n      distances.append((self.y_train[i],self.calculate_euclidean(self.x_train[i],test_sample)))\n    distances.sort(key=lambda x:x[1])#sort in ascending order, based on a distance value\n    neighbors=[]\n    for i in range(self.k): #get first k samples\n      neighbors.append(distances[i][0])\n    return neighbors\n  def predict(self,test_set):\n    predictions=[]\n    for test_sample in test_set:\n      neighbors=self.nearest_neighbors(test_sample)\n      labels=[sample for sample in neighbors]\n      prediction=max(labels,key=labels.count)\n      predictions.append(prediction)\n    return predictions","metadata":{"papermill":{"duration":0.029907,"end_time":"2024-04-19T13:35:23.542716","exception":false,"start_time":"2024-04-19T13:35:23.512809","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classicModel=ClassicKNN(4) #our model\nclassicModel.fit(X_train,y_train)\nclassicKNN_pred=classicModel.predict(X_test) # our model's predictions","metadata":{"papermill":{"duration":727.9705,"end_time":"2024-04-19T13:47:31.527954","exception":false,"start_time":"2024-04-19T13:35:23.557454","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_metrics(y_test,classicKNN_pred)","metadata":{"papermill":{"duration":0.406306,"end_time":"2024-04-19T13:47:31.949639","exception":false,"start_time":"2024-04-19T13:47:31.543333","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We will explore different variants of KNN to improve the metrics","metadata":{"papermill":{"duration":0.015332,"end_time":"2024-04-19T13:47:31.980279","exception":false,"start_time":"2024-04-19T13:47:31.964947","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Fuzzy k-Nearest Neighbors (Fuzzy KNN) Algorithm\n\nFuzzy k-nearest neighbors (Fuzzy KNN) is an extension of the traditional KNN algorithm, which incorporates fuzzy membership functions to assign weights to each neighbor based on its distance from the query point. This algorithm is particularly useful when dealing with noisy or ambiguous data, as it allows for a more flexible classification approach.\n\n## Algorithm Overview\n\n1. **Initialize**: Choose the value of $k$, the number of nearest neighbors to consider, and the fuzzifier parameter $\\lambda$.\n2. **Train**: Store all the training data.\n3. **Predict**:\n   - For each new data point:\n     - Compute the fuzzy membership values for each training data point based on its distance from the query point.\n     - Calculate the weighted class labels using the fuzzy membership values and the class labels of the training data points.\n     - Assign the class label by aggregating the weighted class labels.\n\n## Fuzzy Membership Function\n\nThe fuzzy membership function assigns a degree of membership to each data point based on its distance from the query point. Common fuzzy membership functions include Gaussian, triangular, and trapezoidal functions.\n\n## Hyperparameters\n\n- **$k$**: Number of nearest neighbors to consider.\n\n## Mathematical Notation\n\n- $X$: Training data with features $X_i$ and labels $y_i$, where $i = 1, 2, ..., n$.\n- $x$: New data point to be classified.\n- $k$: Number of nearest neighbors.\n- $d(x, X_i)$: Distance metric between data point $x$ and $X_i$.\n- $\\mu_i$: Fuzzy membership value of data point $X_i$.\n\n### Fuzzy Membership Calculation\n\nThe fuzzy membership value $\\mu_i$ of a data point $X_i$ is calculated using a fuzzy membership function $f$:\n\n$$\n\\mu_i = f(d(x, X_i))\n$$\n\n### Weighted Class Label Calculation\n\nThe weighted class label $\\hat{y}(x)$ for a query point $x$ is calculated by combining the class labels of the $k$ nearest neighbors with their corresponding fuzzy membership values:\n\n$$\n\\hat{y}(x) = \\frac{\\sum_{i=1}^{k} \\mu_i \\cdot y_i}{\\sum_{i=1}^{k} \\mu_i}\n$$\n\n## Pros and Cons\n\n**Pros**:\n- Robust to noisy or ambiguous data.\n- Allows for a more flexible classification approach compared to traditional KNN.\n- Can handle data with varying degrees of relevance.\n\n**Cons**:\n- Requires careful selection of the fuzzifier parameter $\\lambda$.\n- Computationally more expensive compared to traditional KNN, especially with large datasets.\n- Performance highly depends on the choice of fuzzy membership function and distance metric.\n","metadata":{"papermill":{"duration":0.014748,"end_time":"2024-04-19T13:47:32.010229","exception":false,"start_time":"2024-04-19T13:47:31.995481","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class FuzzyKNN:\n    def __init__(self, k):\n        self.k = k  # Number of nearest neighbors to consider\n    \n    def fit(self, X_train, y_train):\n        \"\"\"\n        Fit the Fuzzy KNN model to the training data.\n\n        Parameters:\n        - X_train (array-like): Training data features.\n        - y_train (array-like): Training data labels.\n        \"\"\"\n        self.X_train = X_train  # Training data features\n        self.y_train = y_train  # Training data labels\n    \n    def predict(self, X_test):\n        \"\"\"\n        Predict the labels for the test data.\n\n        Parameters:\n        - X_test (array-like): Test data features.\n\n        Returns:\n        - y_pred (list): Predicted labels for the test data.\n        \"\"\"\n        y_pred = []  # Initialize list to store predicted labels for test data\n        for x in X_test:  # Iterate over each test sample\n            # Calculate distances from the test sample to all training samples\n            distances = np.linalg.norm(self.X_train - x, axis=1)\n            # Find indices of k nearest neighbors\n            nearest_neighbor_indices = np.argsort(distances)[:self.k]\n            # Get labels of k nearest neighbors\n            nearest_neighbor_labels = [self.y_train[idx] for idx in nearest_neighbor_indices]\n            # Calculate membership values for each nearest neighbor\n            membership_values = []\n            for d in distances[nearest_neighbor_indices]:\n                if d != 0:\n                    # Avoid division by zero, handle the case when distance is zero\n                    membership_values.append(1 / d)\n                else:\n                    # If distance is zero, assign infinite membership value\n                    membership_values.append(float('inf'))\n            # Calculate membership-weighted counts for each class label\n            label_counts = {}\n            for label, membership in zip(nearest_neighbor_labels, membership_values):\n                if label in label_counts:\n                    label_counts[label] += membership\n                else:\n                    label_counts[label] = membership\n            # Predict the class label with maximum weighted count\n            prediction = max(label_counts, key=label_counts.get)\n            y_pred.append(prediction)  # Store the predicted label\n        return y_pred","metadata":{"papermill":{"duration":0.03393,"end_time":"2024-04-19T13:47:32.059121","exception":false,"start_time":"2024-04-19T13:47:32.025191","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fuzzyModel = FuzzyKNN(k=4)  # Define the number of nearest neighbors (k)\nfuzzyModel.fit(X_train, y_train)  # Train the model\nfuzzy_pred = fuzzyModel.predict(X_test)  # Make predictions","metadata":{"papermill":{"duration":5.888274,"end_time":"2024-04-19T13:47:37.962824","exception":false,"start_time":"2024-04-19T13:47:32.07455","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_metrics(y_test,fuzzy_pred)","metadata":{"papermill":{"duration":0.382309,"end_time":"2024-04-19T13:47:38.360346","exception":false,"start_time":"2024-04-19T13:47:37.978037","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weight Adjusted KNN Algorithm\n\nThe Weight Adjusted KNN algorithm is a variant of the classic K-nearest neighbors (KNN) algorithm that incorporates attribute weighting. In Weight Adjusted KNN, instead of treating all nearest neighbors equally, the algorithm assigns weights to each neighbor based on a kernel function. These weights are used to adjust the influence of each neighbor in the classification process. Common kernel functions include inverse distance weighting, Gaussian kernel, or Epanechnikov kernel.\n\n## 1. Inverse Distance Weighting (IDW) Kernel\n\nThe Inverse Distance Weighting (IDW) kernel assigns weights to data points based on their distance from the query point. The weight of each data point is inversely proportional to its distance from the query point. The IDW kernel is defined as:\n\n$$\nK(x, x_i) = \\frac{1}{d(x, x_i)}\n$$\n\nwhere $d(x, x_i)$ is the distance between the query point $x$ and the data point $x_i$.\n\n## 2. Gaussian Kernel\n\nThe Gaussian kernel is a popular choice due to its smoothness and continuous nature. It assigns weights to data points based on their distance from the query point, following a Gaussian (bell-shaped) distribution. The Gaussian kernel is defined as:\n\n$$\nK(x, x_i) = \\exp\\left(-\\frac{||x - x_i||^2}{2\\sigma^2}\\right)\n$$\n\nwhere $||x - x_i||$ is the Euclidean distance between the query point $x$ and the data point $x_i$, and $\\sigma$ is a bandwidth parameter controlling the width of the Gaussian.\n\n## 3. Epanechnikov Kernel\n\nThe Epanechnikov kernel is a simple and efficient kernel that assigns weights to data points based on their distance from the query point. It has a quadratic profile and is zero beyond a certain distance from the query point. The Epanechnikov kernel is defined as:\n\n$$\nK(x, x_i) = \\begin{cases}\n\\frac{3}{4}(1 - \\frac{||x - x_i||^2}{r^2}), & \\text{if } ||x - x_i|| < r \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\n\nwhere $||x - x_i||$ is the Euclidean distance between the query point $x$ and the data point $x_i$, and $r$ is a bandwidth parameter controlling the range of the kernel.\n\n#### Key Features:\n\n- **Attribute Weighting:** Weight Adjusted KNN assigns weights to neighbors based on a kernel function, allowing it to give more importance to closer neighbors and less importance to farther neighbors.\n- **Flexibility:** The choice of kernel function provides flexibility in adjusting the influence of neighbors based on distance.\n- **Robustness to Noise:** By incorporating attribute weighting, Weight Adjusted KNN can reduce the impact of noisy or irrelevant features on the classification process.\n\n#### Advantages:\n\n- **Improved Performance:** Weight Adjusted KNN can potentially improve classification performance by giving more emphasis to relevant neighbors and reducing the influence of outliers.\n- **Adaptability:** The choice of kernel function allows Weight Adjusted KNN to adapt to different datasets and classification tasks.\n- **Interpretability:** The weights assigned to neighbors provide insights into the contribution of each neighbor to the classification decision.\n\n#### Limitations:\n\n- **Parameter Sensitivity:** Weight Adjusted KNN performance may depend on the choice of the `k` parameter and the kernel function.\n- **Computational Complexity:** Computing weights for each neighbor can be computationally intensive, especially for large datasets and high-dimensional feature spaces.\n- **Difficulty in Interpretation:** Interpreting the impact of different kernel functions and parameter choices on classification results may require experimentation and analysis.","metadata":{"papermill":{"duration":0.016269,"end_time":"2024-04-19T13:47:38.393165","exception":false,"start_time":"2024-04-19T13:47:38.376896","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class WeightAdjustedKNN:\n    def __init__(self, k):\n        \"\"\"\n        Initialize the WeightAdjustedKNN model.\n\n        Parameters:\n        - k (int): Number of nearest neighbors to consider.\n        \"\"\"\n        self.k = k\n    \n    def fit(self, X_train, y_train):\n        \"\"\"\n        Fit the WeightAdjustedKNN model to the training data.\n\n        Parameters:\n        - X_train (array-like): Training data features.\n        - y_train (array-like): Training data labels.\n        \"\"\"\n        self.X_train = X_train  # Training data features\n        self.y_train = y_train  # Training data labels\n    \n    def _inverse_distance(self, distances):\n        \"\"\"\n        Compute the inverse distance weights for the distances.\n\n        Parameters:\n        - distances (array-like): Array of distances from a test sample to training samples.\n\n        Returns:\n        - weights (array-like): Array of weights computed using the inverse distance.\n        \"\"\"\n        return 1 / (distances + 1e-5)  # Avoid division by zero\n    \n    def _gaussian_kernel(self, distances, sigma=1.0):\n        \"\"\"\n        Compute the Gaussian kernel weights for the distances.\n\n        Parameters:\n        - distances (array-like): Array of distances from a test sample to training samples.\n        - sigma (float): Width parameter of the Gaussian kernel.\n\n        Returns:\n        - weights (array-like): Array of weights computed using the Gaussian kernel.\n        \"\"\"\n        return np.exp(-(distances ** 2) / (2 * (sigma ** 2)))\n    \n    def _epanechnikov_kernel(self, distances):\n        \"\"\"\n        Compute the Epanechnikov kernel weights for the distances.\n\n        Parameters:\n        - distances (array-like): Array of distances from a test sample to training samples.\n\n        Returns:\n        - weights (array-like): Array of weights computed using the Epanechnikov kernel.\n        \"\"\"\n        return 3/4 * (1 - (distances ** 2))\n    \n    def predict(self, X_test, kernel='inverse_distance'):\n        \"\"\"\n        Predict the labels for the test data.\n\n        Parameters:\n        - X_test (array-like): Test data features.\n        - kernel (str): Type of kernel function to use ('inverse_distance', 'gaussian', or 'epanechnikov').\n\n        Returns:\n        - y_pred (list): Predicted labels for the test data.\n        \"\"\"\n        y_pred = []  # Initialize list to store predicted labels for test data\n        for x in X_test:  # Iterate over each test sample\n            # Calculate distances from the test sample to all training samples\n            distances = np.linalg.norm(self.X_train - x, axis=1)\n            # Compute weights using the specified kernel function\n            if kernel == 'inverse_distance':\n                weights = self._inverse_distance(distances)\n            elif kernel == 'gaussian':\n                weights = self._gaussian_kernel(distances)\n            elif kernel == 'epanechnikov':\n                weights = self._epanechnikov_kernel(distances)\n            else:\n                raise ValueError(\"Invalid kernel function. Choose from 'inverse_distance', 'gaussian', or 'epanechnikov'.\")\n            # Find indices of k nearest neighbors\n            nearest_neighbor_indices = np.argsort(distances)[:self.k]\n            # Get labels of k nearest neighbors\n            nearest_neighbor_labels = [self.y_train[idx] for idx in nearest_neighbor_indices]\n            # Calculate weighted counts for each class label\n            label_weights = {}\n            for label, weight in zip(nearest_neighbor_labels, weights):\n                if label in label_weights:\n                    label_weights[label] += weight\n                else:\n                    label_weights[label] = weight\n            # Predict the class label with maximum weighted count\n            prediction = max(label_weights, key=label_weights.get)\n            y_pred.append(prediction)  # Store the predicted label\n        return y_pred\n","metadata":{"papermill":{"duration":0.037411,"end_time":"2024-04-19T13:47:38.446495","exception":false,"start_time":"2024-04-19T13:47:38.409084","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"waModel = WeightAdjustedKNN(k=4)  # Define the number of nearest neighbors (k)\nwaModel.fit(X_train, y_train)  # Train the model","metadata":{"papermill":{"duration":0.025392,"end_time":"2024-04-19T13:47:38.487875","exception":false,"start_time":"2024-04-19T13:47:38.462483","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weightAdjusted_pred = waModel.predict(X_test, kernel='gaussian')  # Make predictions","metadata":{"papermill":{"duration":6.452544,"end_time":"2024-04-19T13:47:44.9561","exception":false,"start_time":"2024-04-19T13:47:38.503556","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_metrics(y_test, weightAdjusted_pred)","metadata":{"papermill":{"duration":0.410631,"end_time":"2024-04-19T13:47:45.383028","exception":false,"start_time":"2024-04-19T13:47:44.972397","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hassanat Distance KNN Algorithm\n\nThe Hassanat Distance KNN algorithm is a variant of the classic K-nearest neighbors (KNN) algorithm that proposes an advanced way to measure the distance between two data points. Instead of using the traditional Euclidean distance, Hassanat distance metric is utilized, which involves the usage of maximum and minimum vector points.\n\n### **Hassanat Distance Calculation**\n\nThe Hassanat distance is calculated by finding the maximum and minimum vector points between the two samples and then computing the Euclidean norm of their difference.\n\n#### Calculation Procedure\n\nLet $x$ and $y$ be two samples with $n$ features.\n\n1. **Find Maximum and Minimum Points**:\n   - Compute the element-wise maximum $max\\_vec$ and minimum $min\\_vec$ vectors between the samples $x$ and $y$.\n     $$\n     max\\_vec_i = \\max(x_i, y_i) \\quad \\text{for } i = 1, 2, ..., n\n     $$\n     $$\n     min\\_vec_i = \\min(x_i, y_i) \\quad \\text{for } i = 1, 2, ..., n\n     $$\n\n2. **Compute Euclidean Norm**:\n   - Compute the Euclidean norm of the difference between the maximum and minimum vectors.\n     $$\n     Hassanat\\_distance = ||max\\_vec - min\\_vec|| = \\sqrt{\\sum_{i=1}^{n} (max\\_vec_i - min\\_vec_i)^2}\n     $$\n\nThe Hassanat distance captures the maximum spread between the two samples, providing a measure of their dissimilarity.\n\nThis distance metric is particularly useful in scenarios where the range of feature values varies significantly between samples and when capturing the overall spread is crucial for assessing dissimilarity.\n\n\n#### Key Features:\n\n- **Advanced Distance Metric:** Hassanat Distance KNN uses the Hassanat distance metric, which considers both the maximum and minimum vector points between samples, providing a potentially more informative distance measure compared to Euclidean distance.\n- **Robustness to Outliers:** By using the Hassanat distance metric, which considers extreme points in the feature space, Hassanat Distance KNN may be more robust to outliers compared to traditional KNN.\n- **Simple Implementation:** Despite the advanced distance metric, the Hassanat Distance KNN algorithm follows a simple implementation similar to the classic KNN algorithm.\n\n#### Advantages:\n\n- **Improved Distance Measure:** Hassanat distance metric may provide improved distance measure, particularly for datasets where the traditional Euclidean distance fails to capture the underlying data distribution effectively.\n- **Potential for Better Performance:** By utilizing an advanced distance metric, Hassanat Distance KNN may achieve better performance compared to traditional KNN, especially in scenarios with complex data distributions.\n\n#### Limitations:\n\n- **Computational Complexity:** Computing Hassanat distances for each test sample can be computationally intensive, especially for large datasets and high-dimensional feature spaces.\n- **Parameter Sensitivity:** The performance of Hassanat Distance KNN may depend on the choice of the `k` parameter and the behavior of the Hassanat distance metric on the specific dataset.\n\nOverall, Hassanat Distance KNN offers a potentially more informative distance metric compared to traditional KNN, which may lead to improved classification performance in certain scenarios.\n","metadata":{"papermill":{"duration":0.01592,"end_time":"2024-04-19T13:47:45.41585","exception":false,"start_time":"2024-04-19T13:47:45.39993","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class HassanatDistanceKNN:\n    def __init__(self, k):\n        \"\"\"\n        Initialize the HassanatDistanceKNN model.\n\n        Parameters:\n        - k (int): Number of nearest neighbors to consider.\n        \"\"\"\n        self.k = k\n    \n    def fit(self, X_train, y_train):\n        \"\"\"\n        Fit the HassanatDistanceKNN model to the training data.\n\n        Parameters:\n        - X_train (array-like): Training data features.\n        - y_train (array-like): Training data labels.\n        \"\"\"\n        self.X_train = X_train  # Training data features\n        self.y_train = y_train  # Training data labels\n    \n    def _hassanat_distance(self, x1, x2):\n        \"\"\"\n        Compute the Hassanat distance metric between two samples.\n\n        Parameters:\n        - x1 (array-like): Feature vector of the first sample.\n        - x2 (array-like): Feature vector of the second sample.\n\n        Returns:\n        - distance (float): Hassanat distance between the two samples.\n        \"\"\"\n        # Example implementation of Hassanat distance metric\n        max_vector = np.maximum(x1, x2)  # Compute maximum vector element-wise\n        min_vector = np.minimum(x1, x2)  # Compute minimum vector element-wise\n        return np.linalg.norm(max_vector - min_vector)  # Compute Euclidean norm of the difference\n    \n    def predict(self, X_test):\n        \"\"\"\n        Predict the labels for the test data.\n\n        Parameters:\n        - X_test (array-like): Test data features.\n\n        Returns:\n        - y_pred (list): Predicted labels for the test data.\n        \"\"\"\n        y_pred = []  # Initialize list to store predicted labels for test data\n        for x in X_test:  # Iterate over each test sample\n            # Calculate Hassanat distances from the test sample to all training samples\n            distances = [self._hassanat_distance(x, x_train) for x_train in self.X_train]\n            # Find indices of k nearest neighbors\n            nearest_neighbor_indices = np.argsort(distances)[:self.k]\n            # Get labels of k nearest neighbors\n            nearest_neighbor_labels = [self.y_train[idx] for idx in nearest_neighbor_indices]\n            # Predict the class label based on majority voting among nearest neighbors\n            prediction = max(set(nearest_neighbor_labels), key=nearest_neighbor_labels.count)\n            y_pred.append(prediction)  # Store the predicted label\n        return y_pred","metadata":{"papermill":{"duration":0.031738,"end_time":"2024-04-19T13:47:45.531707","exception":false,"start_time":"2024-04-19T13:47:45.499969","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hassanatModel = HassanatDistanceKNN(k=4)  # Define the number of nearest neighbors (k)\nhassanatModel.fit(X_train, y_train)  # Train the model\nhassanat_pred = hassanatModel.predict(X_test)  # Make predictions","metadata":{"papermill":{"duration":297.06534,"end_time":"2024-04-19T13:52:42.613456","exception":false,"start_time":"2024-04-19T13:47:45.548116","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_metrics(y_test, hassanat_pred)","metadata":{"papermill":{"duration":0.291667,"end_time":"2024-04-19T13:52:42.922248","exception":false,"start_time":"2024-04-19T13:52:42.630581","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble Approach KNN Algorithm\n\nThe Ensemble Approach KNN algorithm is a variant of the classic K-nearest neighbors (KNN) algorithm that utilizes an ensemble approach to address the issue of having a fixed \"k\" parameter for classification. Instead of using a single value of \"k\", this algorithm considers a range of \"k\" values and combines their predictions using a weighted summation approach.\n\n## Algorithm Overview\n\n1. **Initialize**: Choose a range of $k$ values and a weighting scheme.\n2. **Train**: Store all the training data.\n3. **Predict**:\n   - For each new data point:\n     - Compute predictions for each $k$ value in the range.\n     - Apply weights to the predictions based on the weighting scheme.\n     - Combine the weighted predictions to obtain the final prediction.\n\n## Weighting Scheme\n\nThe weighting scheme determines how much influence each $k$ value has on the final prediction. Common weighting schemes include uniform weighting, where each $k$ value is given equal weight, and inverse distance weighting, where closer neighbors are given higher weight.\n\n## Mathematical Notation\n\n- $X$: Training data with features $X_i$ and labels $y_i$, where $i = 1, 2, ..., n$.\n- $x$: New data point to be classified.\n- $k$: Range of nearest neighbors to consider.\n- $w_k$: Weight for each $k$ value in the range.\n- $\\hat{y}_{k}(x)$: Prediction for $k$ nearest neighbors.\n\n### Prediction Combination\n\nThe final prediction $\\hat{y}(x)$ is obtained by combining the predictions for each $k$ value using a weighted summation:\n\n$$\n\\hat{y}(x) = \\sum_{k} w_k \\cdot \\hat{y}_{k}(x)\n$$\n\n## Pros and Cons\n\n**Pros**:\n- More robust performance compared to using a fixed \"k\" parameter.\n- Flexibility in handling different types of datasets and decision boundaries.\n- Can capture both local and global patterns in the data.\n\n**Cons**:\n- Requires careful selection of the range of \"k\" values and weighting scheme.\n- Computationally more expensive compared to traditional KNN due to considering multiple $k$ values.\n- Performance may degrade if the range of \"k\" values is not properly chosen or if the weighting scheme is not appropriate.\n","metadata":{"papermill":{"duration":0.017131,"end_time":"2024-04-19T13:52:42.956903","exception":false,"start_time":"2024-04-19T13:52:42.939772","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class EnsembleApproachKNN:\n    def __init__(self, k_max):\n        \"\"\"\n        Initialize the EnsembleApproachKNN model.\n\n        Parameters:\n        - k_max (int): Maximum value of k to consider for ensemble approach.\n        \"\"\"\n        self.k_max = k_max\n    \n    def fit(self, X_train, y_train):\n        \"\"\"\n        Fit the EnsembleApproachKNN model to the training data.\n\n        Parameters:\n        - X_train (array-like): Training data features.\n        - y_train (array-like): Training data labels.\n        \"\"\"\n        self.X_train = X_train  # Training data features\n        self.y_train = y_train  # Training data labels\n    \n    def _inverse_logarithm(self, k):\n        \"\"\"\n        Compute the inverse logarithm weight for a given k value.\n\n        Parameters:\n        - k (int): Value of k.\n\n        Returns:\n        - weight (float): Weight computed using the inverse logarithm.\n        \"\"\"\n        return 1 / np.log(k + 1)\n    \n    def predict(self, X_test):\n        \"\"\"\n        Predict the labels for the test data.\n\n        Parameters:\n        - X_test (array-like): Test data features.\n\n        Returns:\n        - y_pred (list): Predicted labels for the test data.\n        \"\"\"\n        y_pred = []  # Initialize list to store predicted labels for test data\n        for x in X_test:  # Iterate over each test sample\n            all_distances = np.linalg.norm(self.X_train - x, axis=1)  # Calculate distances to all training samples\n            sorted_indices = np.argsort(all_distances)  # Sort indices based on distances\n            k_values = np.arange(1, self.k_max + 1, 2)  # Generate odd values of k from 1 to k_max\n            weights = [self._inverse_logarithm(k) for k in k_values]  # Compute weights using inverse logarithm\n            label_counts = {}  # Initialize dictionary to store label counts weighted by k\n            for k, weight in zip(k_values, weights):  # Iterate over k values and weights\n                nearest_neighbor_indices = sorted_indices[:k]  # Get indices of k nearest neighbors\n                nearest_neighbor_labels = [self.y_train[idx] for idx in nearest_neighbor_indices]  # Get labels of k nearest neighbors\n                for label in nearest_neighbor_labels:  # Update label counts with weighted values\n                    if label in label_counts:\n                        label_counts[label] += weight\n                    else:\n                        label_counts[label] = weight\n            prediction = max(label_counts, key=label_counts.get)  # Predict the class label with maximum weighted count\n            y_pred.append(prediction)  # Store the predicted label\n        return y_pred","metadata":{"papermill":{"duration":0.035907,"end_time":"2024-04-19T13:52:43.010414","exception":false,"start_time":"2024-04-19T13:52:42.974507","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eaModel = EnsembleApproachKNN(k_max=10)  # Define the maximum value of k\neaModel.fit(X_train, y_train)  # Train the model\nensembleApproach_pred = eaModel.predict(X_test)  # Make predictions","metadata":{"papermill":{"duration":6.267331,"end_time":"2024-04-19T13:52:49.296429","exception":false,"start_time":"2024-04-19T13:52:43.029098","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_metrics(y_test, ensembleApproach_pred)","metadata":{"papermill":{"duration":0.288029,"end_time":"2024-04-19T13:52:49.602017","exception":false,"start_time":"2024-04-19T13:52:49.313988","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing with Scikit Learn (Standard KNN)","metadata":{"papermill":{"duration":0.017961,"end_time":"2024-04-19T13:52:49.637991","exception":false,"start_time":"2024-04-19T13:52:49.62003","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 4, metric = 'minkowski', p = 2)#The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric.\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","metadata":{"papermill":{"duration":0.588867,"end_time":"2024-04-19T13:52:50.244767","exception":false,"start_time":"2024-04-19T13:52:49.6559","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\nPrecision:\", precision_score(y_test, y_pred))\nprint(\"Recall:\", recall_score(y_test, y_pred))\nprint(\"F1 Score:\", f1_score(y_test, y_pred))\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))","metadata":{"papermill":{"duration":0.050218,"end_time":"2024-04-19T13:52:50.313652","exception":false,"start_time":"2024-04-19T13:52:50.263434","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing the metrics of all the above variants","metadata":{"papermill":{"duration":0.018166,"end_time":"2024-04-19T13:52:50.350195","exception":false,"start_time":"2024-04-19T13:52:50.332029","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def compare_metrics(actual, predicted):\n    if not isinstance(predicted, np.ndarray):\n        predicted = np.array(predicted)\n    matrix = np.zeros((2, 2))\n    n_samples = actual.shape[0]\n    for i in range(n_samples):\n        if actual[i] == predicted[i] and actual[i] == 1:\n            matrix[0, 0] += 1  # True Positive\n        elif actual[i] == predicted[i]:\n            matrix[1, 1] += 1  # True Negative\n        elif predicted[i] == 1:\n            matrix[0, 1] += 1  # False Positive\n        else:\n            matrix[1, 0] += 1  # False Negative\n    \n    # Calculate precision, recall, and F1 score\n    precision = matrix[0,0] / (matrix[0,0] + matrix[0,1]) if (matrix[0,0] + matrix[0,1]) > 0 else 0\n    recall = matrix[0,0] / (matrix[0,0] + matrix[1,0]) if (matrix[0,0] + matrix[1,0]) > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    accuracy = np.sum(np.diag(matrix)) / np.sum(matrix) \n\n    return {\n        \"Accuracy\": accuracy,\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"F1 Score\": f1_score\n    }\n\n# Calculate metrics for each model\nclassic_metrics = compare_metrics(y_test,classicKNN_pred)\nstandard_metrics = compare_metrics(y_test,y_pred)\nfuzzy_metrics = compare_metrics(y_test,fuzzy_pred)\nweight_adjusted_metrics = compare_metrics(y_test, weightAdjusted_pred)\nhassanat_metrics = compare_metrics(y_test, hassanat_pred)\nensemble_metrics = compare_metrics(y_test, ensembleApproach_pred)\n\n# Organize the results into a DataFrame\ndata = {\n    \"Model\": [\"Classic KNN\", \"Standard KNN\", \"Fuzzy KNN\", \"Weight Adjusted KNN\", \"Hassanat KNN\", \"Ensemble Approach KNN\"],\n    \"Accuracy\": [classic_metrics[\"Accuracy\"], standard_metrics[\"Accuracy\"], fuzzy_metrics[\"Accuracy\"], \n                 weight_adjusted_metrics[\"Accuracy\"], hassanat_metrics[\"Accuracy\"], ensemble_metrics[\"Accuracy\"]],\n    \"Precision\": [classic_metrics[\"Precision\"], standard_metrics[\"Precision\"], fuzzy_metrics[\"Precision\"], \n                  weight_adjusted_metrics[\"Precision\"], hassanat_metrics[\"Precision\"], ensemble_metrics[\"Precision\"]],\n    \"Recall\": [classic_metrics[\"Recall\"], standard_metrics[\"Recall\"], fuzzy_metrics[\"Recall\"], \n               weight_adjusted_metrics[\"Recall\"], hassanat_metrics[\"Recall\"], ensemble_metrics[\"Recall\"]],\n    \"F1 Score\": [classic_metrics[\"F1 Score\"], standard_metrics[\"F1 Score\"], fuzzy_metrics[\"F1 Score\"], \n                 weight_adjusted_metrics[\"F1 Score\"], hassanat_metrics[\"F1 Score\"], ensemble_metrics[\"F1 Score\"]]\n}\n\ndf = pd.DataFrame(data)\ndf","metadata":{"papermill":{"duration":0.104833,"end_time":"2024-04-19T13:52:50.473344","exception":false,"start_time":"2024-04-19T13:52:50.368511","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization of the observations of all the variants","metadata":{"papermill":{"duration":0.01819,"end_time":"2024-04-19T13:52:50.509941","exception":false,"start_time":"2024-04-19T13:52:50.491751","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming metrics for four models are stored in lists\nmodel_names = ['Classic KNN', 'Fuzzy KNN', 'Weight Adjusted KNN', 'Hassanat KNN', 'Ensemble Approach KNN']\naccuracies = [classic_metrics[\"Accuracy\"], fuzzy_metrics[\"Accuracy\"], \n              weight_adjusted_metrics[\"Accuracy\"], hassanat_metrics[\"Accuracy\"], ensemble_metrics[\"Accuracy\"]]\nf1_scores = [classic_metrics[\"F1 Score\"], fuzzy_metrics[\"F1 Score\"], \n             weight_adjusted_metrics[\"F1 Score\"], hassanat_metrics[\"F1 Score\"], ensemble_metrics[\"F1 Score\"]]\nprecisions = [classic_metrics[\"Precision\"], fuzzy_metrics[\"Precision\"], \n              weight_adjusted_metrics[\"Precision\"], hassanat_metrics[\"Precision\"], ensemble_metrics[\"Precision\"]]\nrecalls = [classic_metrics[\"Recall\"], fuzzy_metrics[\"Recall\"], \n           weight_adjusted_metrics[\"Recall\"], hassanat_metrics[\"Recall\"], ensemble_metrics[\"Recall\"]]\n\n# Set the width of the bars\nbar_width = 0.2\n\n# Set the positions of the bars on the x-axis\nr1 = np.arange(len(model_names))\nr2 = [x + bar_width for x in r1]\nr3 = [x + bar_width for x in r2]\nr4 = [x + bar_width for x in r3]\n\n# Plotting the grouped bar plot\nplt.figure(figsize=(10, 6))\nplt.bar(r1, accuracies, color='blue', width=bar_width, edgecolor='grey', label='Accuracy')\nplt.bar(r2, f1_scores, color='orange', width=bar_width, edgecolor='grey', label='F1 Score')\nplt.bar(r3, precisions, color='green', width=bar_width, edgecolor='grey', label='Precision')\nplt.bar(r4, recalls, color='red', width=bar_width, edgecolor='grey', label='Recall')\n\n# Add xticks on the middle of the group bars\nplt.xlabel('Models', fontweight='bold')\nplt.xticks([r + bar_width*1.5 for r in range(len(model_names))], model_names, rotation=30)\n\n# Add ylabel and title\nplt.ylabel('Scores', fontweight='bold')\nplt.title('Comparison of Metrics for Different KNN Variants')\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":0.387064,"end_time":"2024-04-19T13:52:50.91552","exception":false,"start_time":"2024-04-19T13:52:50.528456","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.019737,"end_time":"2024-04-19T13:52:50.954881","exception":false,"start_time":"2024-04-19T13:52:50.935144","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}